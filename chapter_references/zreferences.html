<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>参考文献 &#8212; 人工智能数据与模型安全 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="11. 未来展望" href="../source/chap11.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">参考文献</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_references/zreferences.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://trust-ml.github.io/">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/OpenTAI">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://haole1683.github.io/Book_html/html/chapter_preface/empty.html">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/book_logo.png" alt="人工智能数据与模型安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_about_the_book/index.html">关于此书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/%E5%89%8D%E8%A8%80.html">前言</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../source/chap1.html">1. 人工智能与安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap2.html">2. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap3.html">3. 人工智能安全基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap4.html">4. 数据安全：攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap5.html">5. 数据安全：防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap6.html">6. 模型安全：对抗攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap7.html">7. 模型安全：对抗防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap8.html">8. 模型安全：后门攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap9.html">9. 模型安全：后门防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap10.html">10. 模型安全：窃取攻防</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap11.html">11. 未来展望</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/book_logo.png" alt="人工智能数据与模型安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_about_the_book/index.html">关于此书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/%E5%89%8D%E8%A8%80.html">前言</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../source/chap1.html">1. 人工智能与安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap2.html">2. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap3.html">3. 人工智能安全基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap4.html">4. 数据安全：攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap5.html">5. 数据安全：防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap6.html">6. 模型安全：对抗攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap7.html">7. 模型安全：对抗防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap8.html">8. 模型安全：后门攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap9.html">9. 模型安全：后门防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap10.html">10. 模型安全：窃取攻防</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/chap11.html">11. 未来展望</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1>参考文献<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
</div>
<div class="docutils container" id="id2">
<dl class="citation">
<dt class="label" id="id94"><span class="brackets">Abadi et al., 2016</span></dt>
<dd><p>Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., &amp; Zhang, L. (2016). Deep learning with differential privacy. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 308–318).</p>
</dd>
<dt class="label" id="id51"><span class="brackets">Adi et al., 2018</span></dt>
<dd><p>Adi, Y., Baum, C., Cisse, M., Pinkas, B., &amp; Keshet, J. (2018). Turning your weakness into a strength: watermarking deep neural networks by backdooring. <em>USENIX Security Symposium</em> (pp. 1615–1631).</p>
</dd>
<dt class="label" id="id569"><span class="brackets">Agarwal et al., 2020</span></dt>
<dd><p>Agarwal, S., Farid, H., Fried, O., &amp; Agrawala, M. (2020). Detecting deep-fake videos from phoneme-viseme mismatches. <em>IEEE/CVF Computer Vision and Pattern Recognition Conference Workshop</em>.</p>
</dd>
<dt class="label" id="id282"><span class="brackets">Alayrac et al., 2019</span></dt>
<dd><p>Alayrac, J.-B., Uesato, J., Huang, P.-S., Fawzi, A., Stanforth, R., &amp; Kohli, P. (2019). Are labels required for improving adversarial robustness? <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id564"><span class="brackets">Amerini et al., 2019</span></dt>
<dd><p>Amerini, I., Galteri, L., Caldelli, R., &amp; Del Bimbo, A. (2019). Deepfake video detection through optical flow based cnn. <em>International Conference on Computer Vision Workshop</em>.</p>
</dd>
<dt class="label" id="id178"><span class="brackets">Amsaleg et al., 2015</span></dt>
<dd><p>Amsaleg, L., Chelly, O., Furon, T., Girard, S., Houle, M. E., Kawarabayashi, K.-i., &amp; Nett, M. (2015). Estimating local intrinsic dimensionality. <em>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 29–38).</p>
</dd>
<dt class="label" id="id81"><span class="brackets">Andriushchenko et al., 2020</span></dt>
<dd><p>Andriushchenko, M., Croce, F., Flammarion, N., &amp; Hein, M. (2020). Square attack: a query-efficient black-box adversarial attack via random search. <em>European Conference on Computer Vision</em> (pp. 484–501).</p>
</dd>
<dt class="label" id="id95"><span class="brackets">Aono et al., 2017</span></dt>
<dd><p>Aono, Y., Hayashi, T., Wang, L., Moriai, S., &amp; others. (2017). Privacy-preserving deep learning via additively homomorphic encryption. <em>IEEE Transactions on Information Forensics and Security</em>, <em>13</em>(5), 1333–1345.</p>
</dd>
<dt class="label" id="id401"><span class="brackets">Ateniese et al., 2015</span></dt>
<dd><p>Ateniese, G., Mancini, L. V., Spognardi, A., Villani, A., Vitali, D., &amp; Felici, G. (2015). Hacking smart machines with smarter ones: how to extract meaningful data from machine learning classifiers. <em>International Journal of Security and Networks</em>, <em>10</em>(3), 137–150.</p>
</dd>
<dt class="label" id="id179"><span class="brackets">Athalye et al., 2018a</span></dt>
<dd><p>Athalye, A., Carlini, N., &amp; Wagner, D. (2018). Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples. <em>International Conference on Machine Learning</em> (pp. 274–283).</p>
</dd>
<dt class="label" id="id391"><span class="brackets">Athalye et al., 2018b</span></dt>
<dd><p>Athalye, A., Engstrom, L., Ilyas, A., &amp; Kwok, K. (2018). Synthesizing robust adversarial examples. <em>International Conference on Machine Learning</em> (pp. 284–293).</p>
</dd>
<dt class="label" id="id254"><span class="brackets">Bagdasaryan et al., 2020</span></dt>
<dd><p>Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., &amp; Shmatikov, V. (2020). How to backdoor federated learning. <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 2938–2948).</p>
</dd>
<dt class="label" id="id280"><span class="brackets">Bai et al., 2020a</span></dt>
<dd><p>Bai, Y., Zeng, Y., Jiang, Y., Xia, S.-T., Ma, X., &amp; Wang, Y. (2020). Improving adversarial robustness via channel-wise activation suppressing. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id575"><span class="brackets">Bai et al., 2020b</span></dt>
<dd><p>Bai, Y., Guo, Y., Wei, J., Lu, L., Wang, R., &amp; Wang, Y. (2020). Fake generated painting detection via frequency analysis. <em>ICIP</em>.</p>
</dd>
<dt class="label" id="id350"><span class="brackets">Bai et al., 2021</span></dt>
<dd><p>Bai, Y., Mei, J., Yuille, A. L., &amp; Xie, C. (2021). Are transformers more robust than cnns? <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 26831–26843.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">Barreno et al., 2006</span></dt>
<dd><p>Barreno, M., Nelson, B., Sears, R., Joseph, A. D., &amp; Tygar, J. D. (2006). Can machine learning be secure? <em>ACM Symposium on Information, Computer and Communications Security</em> (pp. 16–25).</p>
</dd>
<dt class="label" id="id128"><span class="brackets">Basu et al., 2021</span></dt>
<dd><p>Basu, S., Pope, P., &amp; Feizi, S. (2021). Influence functions in deep learning are fragile. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id402"><span class="brackets">Belghazi et al., 2018</span></dt>
<dd><p>Belghazi, M. I., Baratin, A., Rajeswar, S., Ozair, S., Bengio, Y., Courville, A., &amp; Hjelm, R. D. (2018). Mine: mutual information neural estimation. <em>arXiv preprint arXiv:1801.04062</em>.</p>
</dd>
<dt class="label" id="id169"><span class="brackets">Bendale &amp; Boult, 2016</span></dt>
<dd><p>Bendale, A., &amp; Boult, T. E. (2016). Towards open set deep networks. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 1563–1572).</p>
</dd>
<dt class="label" id="id358"><span class="brackets">Bender et al., 2018</span></dt>
<dd><p>Bender, G., Kindermans, P.-J., Zoph, B., Vasudevan, V., &amp; Le, Q. (2018). Understanding and simplifying one-shot architecture search. <em>International Conference on Machine Learning</em> (pp. 550–559).</p>
</dd>
<dt class="label" id="id96"><span class="brackets">Bengio &amp; others, 2009</span></dt>
<dd><p>Bengio, Y., &amp; others. (2009). Learning deep architectures for ai. <em>Foundations and trends® in Machine Learning</em>, <em>2</em>(1), 1–127.</p>
</dd>
<dt class="label" id="id172"><span class="brackets">Bhagoji et al., 2017</span></dt>
<dd><p>Bhagoji, A. N., Cullina, D., &amp; Mittal, P. (2017). Dimensionality reduction as a defense against evasion attacks on machine learning classifiers. <em>arXiv preprint arXiv:1704.02654</em>, <em>2</em>(1).</p>
</dd>
<dt class="label" id="id351"><span class="brackets">Bhojanapalli et al., 2021</span></dt>
<dd><p>Bhojanapalli, S., Chakrabarti, A., Glasner, D., Li, D., Unterthiner, T., &amp; Veit, A. (2021). Understanding robustness of transformers for image classification. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 10231–10241).</p>
</dd>
<dt class="label" id="id71"><span class="brackets">Biggio et al., 2013</span></dt>
<dd><p>Biggio, B., Corona, I., Maiorca, D., Nelson, B., Šrndić, N., Laskov, P., … Roli, F. (2013). Evasion attacks against machine learning at test time. <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em> (pp. 387–402).</p>
</dd>
<dt class="label" id="id37"><span class="brackets">Biggio et al., 2012</span></dt>
<dd><p>Biggio, B., Nelson, B., &amp; Laskov, P. (2012). Poisoning attacks against support vector machines. <em>International Conference on International Conference on Machine Learning</em> (pp. 1467–1474). Madison, WI, USA: Omnipress.</p>
</dd>
<dt class="label" id="id200"><span class="brackets">Blanchard et al., 2017</span></dt>
<dd><p>Blanchard, P., Mhamdi, E., Guerraoui, R., &amp; Stainer, J. (2017). Machine learning with adversaries: byzantine tolerant gradient descent. <em>Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id133"><span class="brackets">Bone et al., 2014</span></dt>
<dd><p>Bone, D., Li, M., Black, M. P., &amp; Narayanan, S. S. (2014). Intoxicated speech detection: a fusion framework with speaker-normalized hierarchical functionals and gmm supervectors. <em>Computer Speech &amp; Language</em>, <em>28</em>(2), 375–391.</p>
</dd>
<dt class="label" id="id159"><span class="brackets">Boneh et al., 2005</span></dt>
<dd><p>Boneh, D., Goh, E.-J., &amp; Nissim, K. (2005). Evaluating 2-dnf formulas on ciphertexts. <em>Theory of Cryptography Conference</em> (pp. 325–341).</p>
</dd>
<dt class="label" id="id419"><span class="brackets">Borgnia et al., 2021</span></dt>
<dd><p>Borgnia, E., Cherepanova, V., Fowl, L., Ghiasi, A., Geiping, J., Goldblum, M., … Gupta, A. (2021). Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em> (pp. 3855–3859).</p>
</dd>
<dt class="label" id="id204"><span class="brackets">Botoeva et al., 2020</span></dt>
<dd><p>Botoeva, E., Kouvaros, P., Kronqvist, J., Lomuscio, A., &amp; Misener, R. (2020). Efficient verification of relu-based neural networks via dependency analysis. <em>AAAI Conference on Artificial Intelligence</em> (pp. 3291–3299).</p>
</dd>
<dt class="label" id="id162"><span class="brackets">Brakerski et al., 2014</span></dt>
<dd><p>Brakerski, Z., Gentry, C., &amp; Vaikuntanathan, V. (2014). (leveled) fully homomorphic encryption without bootstrapping. <em>ACM Transactions on Computation Theory</em>, <em>6</em>(3), 1–36.</p>
</dd>
<dt class="label" id="id86"><span class="brackets">Brendel et al., 2018</span></dt>
<dd><p>Brendel, W., Rauber, J., &amp; Bethge, M. (2018). Decision-based adversarial attacks: reliable attacks against black-box machine learning models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id389"><span class="brackets">Brown et al., 2017</span></dt>
<dd><p>Brown, T. B., Mané, D., Roy, A., Abadi, M., &amp; Gilmer, J. (2017). Adversarial patch. <em>arXiv preprint arXiv:1712.09665</em>.</p>
</dd>
<dt class="label" id="id313"><span class="brackets">Buades et al., 2005</span></dt>
<dd><p>Buades, A., Coll, B., &amp; Morel, J.-M. (2005). A non-local algorithm for image denoising. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 60–65).</p>
</dd>
<dt class="label" id="id445"><span class="brackets">Bunel et al., 2020</span></dt>
<dd><p>Bunel, R., Mudigonda, P., Turkaslan, I., Torr, P., Lu, J., &amp; Kohli, P. (2020). Branch and bound for piecewise linear neural network verification. <em>Journal of Machine Learning Research</em>, <em>21</em>(2020).</p>
</dd>
<dt class="label" id="id264"><span class="brackets">Cai et al., 2018</span></dt>
<dd><p>Cai, Q.-Z., Liu, C., &amp; Song, D. (2018). Curriculum adversarial training. <em>International Joint Conference on Artificial Intelligence</em> (pp. 3740–3747).</p>
</dd>
<dt class="label" id="id583"><span class="brackets">Cao et al., 2022</span></dt>
<dd><p>Cao, J., Ma, C., Yao, T., Chen, S., Ding, S., &amp; Yang, X. (2022). End-to-end reconstruction-classification learning for face forgery detection. <em>IEEE/CVF Computer Vision and Pattern Recognition Conference</em>.</p>
</dd>
<dt class="label" id="id587"><span class="brackets">Cao et al., 2021a</span></dt>
<dd><p>Cao, S., Zou, Q., Mao, X., Ye, D., &amp; Wang, Z. (2021). Metric learning for anti-compression facial forgery detection. <em>ACM MM</em>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets">Cao et al., 2021b</span></dt>
<dd><p>Cao, X., Jia, J., &amp; Gong, N. Z. (2021). Ipguard: protecting intellectual property of deep neural networks via fingerprinting the classification boundary. <em>ACM Asia Conference on Computer and Communications Security</em> (pp. 14–25).</p>
</dd>
<dt class="label" id="id185"><span class="brackets">Cao et al., 2021c</span></dt>
<dd><p>Cao, Y., Wang, N., Xiao, C., Yang, D., Fang, J., Yang, R., … Li, B. (2021). Invisible for both camera and lidar: security of multi-sensor fusion based perception in autonomous driving under physical-world attacks. <em>IEEE Symposium on Security and Privacy</em> (pp. 176–194).</p>
</dd>
<dt class="label" id="id105"><span class="brackets">Carlini et al., 2020</span></dt>
<dd><p>Carlini, N., Jagielski, M., &amp; Mironov, I. (2020). Cryptanalytic extraction of neural network models. <em>Annual International Cryptology Conference</em> (pp. 189–218).</p>
</dd>
<dt class="label" id="id397"><span class="brackets">Carlini et al., 2019</span></dt>
<dd><p>Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., &amp; Song, D. (2019). The secret sharer: evaluating and testing unintended memorization in neural networks. <em>USENIX Security Symposium</em> (pp. 267–284).</p>
</dd>
<dt class="label" id="id398"><span class="brackets">Carlini et al., 2021</span></dt>
<dd><p>Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … others. (2021). Extracting training data from large language models. <em>USENIX Security Symposium</em> (pp. 2633–2650).</p>
</dd>
<dt class="label" id="id322"><span class="brackets">Carlini &amp; Wagner, 2016</span></dt>
<dd><p>Carlini, N., &amp; Wagner, D. (2016). Defensive distillation is not robust to adversarial examples. <em>arXiv preprint arXiv:1607.04311</em>.</p>
</dd>
<dt class="label" id="id165"><span class="brackets">Carlini &amp; Wagner, 2017a</span></dt>
<dd><p>Carlini, N., &amp; Wagner, D. (2017). Adversarial examples are not easily detected: bypassing ten detection methods. <em>ACM Workshop on Artificial Intelligence and Security</em> (pp. 3–14).</p>
</dd>
<dt class="label" id="id215"><span class="brackets">Carlini &amp; Wagner, 2017b</span></dt>
<dd><p>Carlini, N., &amp; Wagner, D. (2017). Magnet and&quot; efficient defenses against adversarial attacks&quot; are not robust to adversarial examples. <em>arXiv preprint arXiv:1711.08478</em>.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">Carlini &amp; Wagner, 2017c</span></dt>
<dd><p>Carlini, N., &amp; Wagner, D. (2017). Towards evaluating the robustness of neural networks. <em>IEEE Symposium on Security and Privacy</em> (pp. 39–57).</p>
</dd>
<dt class="label" id="id283"><span class="brackets">Carmon et al., 2019</span></dt>
<dd><p>Carmon, Y., Raghunathan, A., Schmidt, L., Duchi, J. C., &amp; Liang, P. S. (2019). Unlabeled data improves adversarial robustness. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id286"><span class="brackets">Caron et al., 2018</span></dt>
<dd><p>Caron, M., Bojanowski, P., Joulin, A., &amp; Douze, M. (2018). Deep clustering for unsupervised learning of visual features. <em>European Conference on Computer Vision</em> (pp. 132–149).</p>
</dd>
<dt class="label" id="id341"><span class="brackets">Cazenavette et al., 2021</span></dt>
<dd><p>Cazenavette, G., Murdock, C., &amp; Lucey, S. (2021). Architectural adversarial robustness: the case for deep pursuit. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 7150–7158).</p>
</dd>
<dt class="label" id="id328"><span class="brackets">Chan et al., 2022</span></dt>
<dd><p>Chan, S.-H., Dong, Y., Zhu, J., Zhang, X., &amp; Zhou, J. (2022). Baddet: backdoor attacks on object detection. <em>arXiv preprint arXiv:2205.14497</em>.</p>
</dd>
<dt class="label" id="id386"><span class="brackets">Chang et al., 2000</span></dt>
<dd><p>Chang, S. G., Yu, B., &amp; Vetterli, M. (2000). Adaptive wavelet thresholding for image denoising and compression. <em>IEEE Transactions on Image Processing</em>, <em>9</em>(9), 1532–1546.</p>
</dd>
<dt class="label" id="id291"><span class="brackets">Chaudhuri &amp; Monteleoni, 2008</span></dt>
<dd><p>Chaudhuri, K., &amp; Monteleoni, C. (2008). Privacy-preserving logistic regression. <em>Advances in Neural Information Processing Systems</em>, <em>21</em>.</p>
</dd>
<dt class="label" id="id363"><span class="brackets">Chen et al., 2018a</span></dt>
<dd><p>Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T., … Srivastava, B. (2018). Detecting backdoor attacks on deep neural networks by activation clustering. <em>arXiv preprint arXiv:1811.03728</em>.</p>
</dd>
<dt class="label" id="id111"><span class="brackets">Chen et al., 2018b</span></dt>
<dd><p>Chen, F., Luo, M., Dong, Z., Li, Z., &amp; He, X. (2018). Federated meta-learning with fast convergence and efficient communication. <em>arXiv preprint arXiv:1802.07876</em>.</p>
</dd>
<dt class="label" id="id359"><span class="brackets">Chen et al., 2020a</span></dt>
<dd><p>Chen, H., Zhang, B., Xue, S., Gong, X., Liu, H., Ji, R., &amp; Doermann, D. (2020). Anti-bandit neural architecture search for model defense. <em>European Conference on Computer Vision</em> (pp. 70–85).</p>
</dd>
<dt class="label" id="id409"><span class="brackets">Chen et al., 2019</span></dt>
<dd><p>Chen, H., Fu, C., Zhao, J., &amp; Koushanfar, F. (2019). Deepinspect: a black-box trojan detection and mitigation framework for deep neural networks. <em>International Joint Conference on Artificial Intelligence</em> (pp. 4658–4664).</p>
</dd>
<dt class="label" id="id68"><span class="brackets">Chen et al., 2022</span></dt>
<dd><p>Chen, J., Wang, J., Peng, T., Sun, Y., Cheng, P., Ji, S., … Song, D. (2022). Copy, right? a testing framework for copyright protection of deep learning models. <em>IEEE Symposium on Security and Privacy</em> (pp. 824–841).</p>
</dd>
<dt class="label" id="id25"><span class="brackets">Chen et al., 2015</span></dt>
<dd><p>Chen, J., Kang, X., Liu, Y., &amp; Wang, Z. J. (2015). Median filtering forensics based on convolutional neural networks. <em>IEEE Signal Processing Letters</em>, <em>22</em>(11), 1849–1853.</p>
</dd>
<dt class="label" id="id334"><span class="brackets">Chen et al., 2021a</span></dt>
<dd><p>Chen, K., Meng, Y., Sun, X., Guo, S., Zhang, T., Li, J., &amp; Fan, C. (2021). Badpre: task-agnostic backdoor attacks to pre-trained nlp foundation models. <em>arXiv preprint arXiv:2110.02467</em>.</p>
</dd>
<dt class="label" id="id82"><span class="brackets">Chen et al., 2017a</span></dt>
<dd><p>Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., &amp; Hsieh, C.-J. (2017). Zoo: zeroth order optimization based black-box attacks to deep neural networks without training substitute models. <em>ACM Workshop on Artificial Intelligence and Security</em> (pp. 15–26).</p>
</dd>
<dt class="label" id="id13"><span class="brackets">Chen et al., 2020b</span></dt>
<dd><p>Chen, R., Chen, X., Ni, B., &amp; Ge, Y. (2020). Simswap: an efficient framework for high fidelity face swapping. <em>ACM International Conference on Multimedia</em> (pp. 2003–2011).</p>
</dd>
<dt class="label" id="id581"><span class="brackets">Chen et al., 2021b</span></dt>
<dd><p>Chen, S., Yao, T., Chen, Y., Ding, S., Li, J., &amp; Ji, R. (2021). Local relation learning for face forgery detection. <em>AAAI</em>.</p>
</dd>
<dt class="label" id="id304"><span class="brackets">Chen et al., 2021c</span></dt>
<dd><p>Chen, T., Zhang, Z., Liu, S., Chang, S., &amp; Wang, Z. (2021). Robust overfitting may be mitigated by properly learned smoothening. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id331"><span class="brackets">Chen et al., 2021d</span></dt>
<dd><p>Chen, X., Salem, A., Chen, D., Backes, M., Ma, S., Shen, Q., … Zhang, Y. (2021). Badnl: backdoor attacks against nlp models with semantic-preserving improvements. <em>Annual Computer Security Applications Conference</em> (pp. 554–569).</p>
</dd>
<dt class="label" id="id57"><span class="brackets">Chen et al., 2017b</span></dt>
<dd><p>Chen, X., Liu, C., Li, B., Lu, K., &amp; Song, D. (2017). Targeted backdoor attacks on deep learning systems using data poisoning. <em>arXiv preprint arXiv:1712.05526</em>.</p>
</dd>
<dt class="label" id="id582"><span class="brackets">Chen &amp; Yang, 2021</span></dt>
<dd><p>Chen, Z., &amp; Yang, H. (2021). Attentive semantic exploring for manipulated face detection. <em>ICASSP</em>.</p>
</dd>
<dt class="label" id="id163"><span class="brackets">Cheng et al., 2021</span></dt>
<dd><p>Cheng, K., Fan, T., Jin, Y., Liu, Y., Chen, T., Papadopoulos, D., &amp; Yang, Q. (2021). Secureboost: a lossless federated learning framework. <em>IEEE Intelligent Systems</em>, <em>36</em>(6), 87–98.</p>
</dd>
<dt class="label" id="id87"><span class="brackets">Cheng et al., 2019a</span></dt>
<dd><p>Cheng, M., Le, T., Chen, P.-Y., Zhang, H., Yi, J., &amp; Hsieh, C.-J. (2019). Query-efficient hard-label black-box attack: an optimization-based approach. <em>International Conference on Learning Representation</em>.</p>
</dd>
<dt class="label" id="id74"><span class="brackets">Cheng et al., 2019b</span></dt>
<dd><p>Cheng, S., Dong, Y., Pang, T., Su, H., &amp; Zhu, J. (2019). Improving black-box adversarial attacks with a transfer-based prior. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id525"><span class="brackets">Cho et al., 2014</span></dt>
<dd><p>Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). On the properties of neural machine translation: encoder-decoder approaches. <em>arXiv preprint arXiv:1409.1259</em>.</p>
</dd>
<dt class="label" id="id568"><span class="brackets">Ciftci et al., 2020</span></dt>
<dd><p>Ciftci, U. A., Demir, I., &amp; Yin, L. (2020). Fakecatcher: detection of synthetic portrait videos using biological signals. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</p>
</dd>
<dt class="label" id="id320"><span class="brackets">Clevert et al., 2016</span></dt>
<dd><p>Clevert, D.-A., Unterthiner, T., &amp; Hochreiter, S. (2016). Fast and accurate deep network learning by exponential linear units (elus). <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id147"><span class="brackets">Cohen et al., 2019</span></dt>
<dd><p>Cohen, J., Rosenfeld, E., &amp; Kolter, Z. (2019). Certified adversarial robustness via randomized smoothing. <em>International Conference on Machine Learning</em> (pp. 1310–1320).</p>
</dd>
<dt class="label" id="id403"><span class="brackets">Cortes &amp; Vapnik, 2004</span></dt>
<dd><p>Cortes, C., &amp; Vapnik, V. N. (2004). Support-vector networks. <em>Machine Learning</em>, <em>20</em>, 273-297.</p>
</dd>
<dt class="label" id="id80"><span class="brackets">Croce &amp; Hein, 2020a</span></dt>
<dd><p>Croce, F., &amp; Hein, M. (2020). Minimally distorted adversarial examples with a fast adaptive boundary attack. <em>International Conference on Machine Learning</em> (pp. 2196–2205).</p>
</dd>
<dt class="label" id="id79"><span class="brackets">Croce &amp; Hein, 2020b</span></dt>
<dd><p>Croce, F., &amp; Hein, M. (2020). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. <em>International Conference on Machine Learning</em> (pp. 2206–2216).</p>
</dd>
<dt class="label" id="id194"><span class="brackets">Cubuk et al., 2019</span></dt>
<dd><p>Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., &amp; Le, Q. V. (2019). Autoaugment: learning augmentation strategies from data. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 113–123).</p>
</dd>
<dt class="label" id="id135"><span class="brackets">Cummins et al., 2017</span></dt>
<dd><p>Cummins, N., Schmitt, M., Amiriparian, S., Krajewski, J., &amp; Schuller, B. (2017). “you sound ill, take the day off”: automatic recognition of speech affected by upper respiratory tract infection. <em>IEEE Engineering in Medicine and Biology Society</em> (pp. 3806–3809).</p>
</dd>
<dt class="label" id="id547"><span class="brackets">DAlonzo &amp; Tegmark, 2022</span></dt>
<dd><p>D'Alonzo, S., &amp; Tegmark, M. (2022). Machine-learning media bias. <em>Plos one</em>, <em>17</em>(8), e0271947.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">DarvishRouhani et al., 2019</span></dt>
<dd><p>Darvish Rouhani, B., Chen, H., &amp; Koushanfar, F. (2019). Deepsigns: an end-to-end watermarking framework for ownership protection of deep neural networks. <em>International Conference on Architectural Support for Programming Languages and Operating Systems</em> (pp. 485–497).</p>
</dd>
<dt class="label" id="id383"><span class="brackets">Das et al., 2017</span></dt>
<dd><p>Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Chen, L., Kounavis, M. E., &amp; Chau, D. H. (2017). Keeping the bad guys out: protecting and vaccinating deep learning with jpeg compression. <em>arXiv preprint arXiv:1705.02900</em>.</p>
</dd>
<dt class="label" id="id202"><span class="brackets">Dathathri et al., 2018</span></dt>
<dd><p>Dathathri, S., Zheng, S., Yin, T., Murray, R. M., &amp; Yue, Y. (2018). Detecting adversarial examples via neural fingerprinting. <em>arXiv preprint arXiv:1803.03870</em>.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">Davis, 1976</span></dt>
<dd><p>Davis, R. (1976). <em>Use of meta level knowledge in the construction and maintenance of large knowledge bases.</em> Stanford University.</p>
</dd>
<dt class="label" id="id446"><span class="brackets">DePalma et al., 2021</span></dt>
<dd><p>De Palma, A., Bunel, R., Desmaison, A., Dvijotham, K., Kohli, P., Torr, P. H., &amp; Kumar, M. P. (2021). Improved branch and bound for neural network verification via lagrangian decomposition. <em>arXiv preprint arXiv:2104.06718</em>.</p>
</dd>
<dt class="label" id="id187"><span class="brackets">DeGrave et al., 2021</span></dt>
<dd><p>DeGrave, A. J., Janizek, J. D., &amp; Lee, S.-I. (2021). Ai for radiographic covid-19 detection selects shortcuts over signal. <em>Nature Machine Intelligence</em>, <em>3</em>(7), 610–619.</p>
</dd>
<dt class="label" id="id511"><span class="brackets">Deng et al., 2009</span></dt>
<dd><p>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: a large-scale hierarchical image database. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 248–255).</p>
</dd>
<dt class="label" id="id537"><span class="brackets">Deng et al., 2019</span></dt>
<dd><p>Deng, J., Guo, J., Xue, N., &amp; Zafeiriou, S. (2019). Arcface: additive angular margin loss for deep face recognition. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 4690–4699).</p>
</dd>
<dt class="label" id="id138"><span class="brackets">Deng et al., 2020</span></dt>
<dd><p>Deng, Y., Kamani, M. M., &amp; Mahdavi, M. (2020). Adaptive personalized federated learning. <em>arXiv preprint arXiv:2003.13461</em>.</p>
</dd>
<dt class="label" id="id362"><span class="brackets">Devaguptapu et al., 2021</span></dt>
<dd><p>Devaguptapu, C., Agarwal, D., Mittal, G., Gopalani, P., &amp; Balasubramanian, V. N. (2021). On adversarial robustness: a neural architecture search perspective. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 152–161).</p>
</dd>
<dt class="label" id="id196"><span class="brackets">DeVries &amp; Taylor, 2017</span></dt>
<dd><p>DeVries, T., &amp; Taylor, G. W. (2017). Improved regularization of convolutional neural networks with cutout. <em>arXiv preprint arXiv:1708.04552</em>.</p>
</dd>
<dt class="label" id="id266"><span class="brackets">Ding et al., 2019</span></dt>
<dd><p>Ding, G. W., Sharma, Y., Lui, K. Y. C., &amp; Huang, R. (2019). Mma training: direct input space margin maximization through adversarial training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id573"><span class="brackets">Ding et al., 2021</span></dt>
<dd><p>Ding, Y., Thakur, N., &amp; Li, B. (2021). Does a gan leave distinct model-specific fingerprints? <em>BMVC</em>.</p>
</dd>
<dt class="label" id="id489"><span class="brackets">Dolhansky et al., 2019</span></dt>
<dd><p>Dolhansky, B., Howes, R., Pflaum, B., Baram, N., &amp; Ferrer, C. C. (2019). The deepfake detection challenge (dfdc) preview dataset. <em>arXiv preprint arXiv:1910.08854</em>.</p>
</dd>
<dt class="label" id="id305"><span class="brackets">Dong et al., 2020</span></dt>
<dd><p>Dong, Y., Deng, Z., Pang, T., Zhu, J., &amp; Su, H. (2020). Adversarial distributional training for robust deep learning. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 8270–8283.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">Dong et al., 2018</span></dt>
<dd><p>Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., &amp; Li, J. (2018). Boosting adversarial attacks with momentum. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 9185–9193).</p>
</dd>
<dt class="label" id="id91"><span class="brackets">Dong et al., 2019</span></dt>
<dd><p>Dong, Y., Pang, T., Su, H., &amp; Zhu, J. (2019). Evading defenses to transferable adversarial examples by translation-invariant attacks. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 4312–4321).</p>
</dd>
<dt class="label" id="id348"><span class="brackets">Dosovitskiy et al., 2021</span></dt>
<dd><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … others. (2021). An image is worth 16x16 words: transformers for image recognition at scale. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id392"><span class="brackets">Duan et al., 2020</span></dt>
<dd><p>Duan, R., Ma, X., Wang, Y., Bailey, J., Qin, A. K., &amp; Yang, Y. (2020). Adversarial camouflage: hiding physical-world attacks with natural styles. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 1000–1008).</p>
</dd>
<dt class="label" id="id544"><span class="brackets">Duchi et al., 2011</span></dt>
<dd><p>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. <em>Journal of Machine Learning Research</em>, <em>12</em>(7).</p>
</dd>
<dt class="label" id="id373"><span class="brackets">Duddu et al., 2020</span></dt>
<dd><p>Duddu, V., Boutet, A., &amp; Shejwalkar, V. (2020). Quantifying privacy leakage in graph embedding. <em>EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services</em> (pp. 76–85).</p>
</dd>
<dt class="label" id="id578"><span class="brackets">Durall et al., 2019</span></dt>
<dd><p>Durall, R., Keuper, M., Pfreundt, F.-J., &amp; Keuper, J. (2019). Unmasking deepfakes with simple features. <em>arXiv preprint arXiv:1911.00686</em>.</p>
</dd>
<dt class="label" id="id451"><span class="brackets">Dvijotham et al., 2018a</span></dt>
<dd><p>Dvijotham, K., Gowal, S., Stanforth, R., Arandjelovic, R., O'Donoghue, B., Uesato, J., &amp; Kohli, P. (2018). Training verified learners with learned verifiers. <em>arXiv preprint arXiv:1805.10265</em>.</p>
</dd>
<dt class="label" id="id452"><span class="brackets">Dvijotham et al., 2018b</span></dt>
<dd><p>Dvijotham, K., Stanforth, R., Gowal, S., Mann, T. A., &amp; Kohli, P. (2018). A dual approach to scalable verification of deep networks. <em>UAI</em> (p. 3).</p>
</dd>
<dt class="label" id="id267"><span class="brackets">Dwork, 2006</span></dt>
<dd><p>Dwork, C. (2006). Differential privacy. <em>International Conference on Automata, Languages and Programming</em>.</p>
</dd>
<dt class="label" id="id273"><span class="brackets">Dwork, 2011</span></dt>
<dd><p>Dwork, C. (2011). A firm foundation for private data analysis. <em>Communications of the ACM</em>, <em>54</em>(1), 86–95.</p>
</dd>
<dt class="label" id="id269"><span class="brackets">Dwork et al., 2006a</span></dt>
<dd><p>Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., &amp; Naor, M. (2006). Our data, ourselves: privacy via distributed noise generation. <em>International Conference on the Theory and Applications of Cryptographic Techniques</em> (pp. 486–503).</p>
</dd>
<dt class="label" id="id224"><span class="brackets">Dwork et al., 2006b</span></dt>
<dd><p>Dwork, C., McSherry, F., Nissim, K., &amp; Smith, A. (2006). Calibrating noise to sensitivity in private data analysis. <em>Theory of Cryptography Conference</em> (pp. 265–284).</p>
</dd>
<dt class="label" id="id227"><span class="brackets">Dwork et al., 2014</span></dt>
<dd><p>Dwork, C., Roth, A., &amp; others. (2014). The algorithmic foundations of differential privacy. <em>Foundations and Trends® in Theoretical Computer Science</em>, <em>9</em>(3–4), 211–407.</p>
</dd>
<dt class="label" id="id366"><span class="brackets">Dwork et al., 2010</span></dt>
<dd><p>Dwork, C., Rothblum, G. N., &amp; Vadhan, S. (2010). Boosting and differential privacy. <em>IEEE Annual Symposium on Foundations of Computer Science</em> (pp. 51–60).</p>
</dd>
<dt class="label" id="id276"><span class="brackets">Engstrom et al., 2018a</span></dt>
<dd><p>Engstrom, L., Ilyas, A., &amp; Athalye, A. (2018). Evaluating and understanding the robustness of adversarial logit pairing. <em>arXiv preprint arXiv:1807.10272</em>.</p>
</dd>
<dt class="label" id="id459"><span class="brackets">Engstrom et al., 2018b</span></dt>
<dd><p><strong>missing journal in engstrom2018rotation</strong></p>
</dd>
<dt class="label" id="id9"><span class="brackets">Ester et al., 1996</span></dt>
<dd><p>Ester, M., Kriegel, H.-P., Sander, J., Xu, X., &amp; others. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. <em>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 226–231).</p>
</dd>
<dt class="label" id="id184"><span class="brackets">Eykholt et al., 2018</span></dt>
<dd><p>Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., … Song, D. (2018). Robust physical-world attacks on deep learning visual classification. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 1625–1634).</p>
</dd>
<dt class="label" id="id110"><span class="brackets">Fallah et al., 2020</span></dt>
<dd><p>Fallah, A., Mokhtari, A., &amp; Ozdaglar, A. (2020). Personalized federated learning: a meta-learning approach. <em>arXiv preprint arXiv:2002.07948</em>.</p>
</dd>
<dt class="label" id="id161"><span class="brackets">Fan &amp; Vercauteren, 2012</span></dt>
<dd><p>Fan, J., &amp; Vercauteren, F. (2012). Somewhat practical fully homomorphic encryption. <em>Cryptology ePrint Archive</em>.</p>
</dd>
<dt class="label" id="id126"><span class="brackets">Fang et al., 2020</span></dt>
<dd><p>Fang, M., Gong, N. Z., &amp; Liu, J. (2020). Influence function based data poisoning attacks to top-n recommender systems. <em>The Web Conference 2020</em> (pp. 3019–3025).</p>
</dd>
<dt class="label" id="id146"><span class="brackets">Fawzi et al., 2016</span></dt>
<dd><p>Fawzi, A., Moosavi-Dezfooli, S.-M., &amp; Frossard, P. (2016). Robustness of classifiers: from adversarial to random noise. <em>Advances in Neural Information Processing Systems</em>, <em>29</em>.</p>
</dd>
<dt class="label" id="id177"><span class="brackets">Feinman et al., 2017</span></dt>
<dd><p>Feinman, R., Curtin, R. R., Shintre, S., &amp; Gardner, A. B. (2017). Detecting adversarial samples from artifacts. <em>arXiv preprint arXiv:1703.00410</em>.</p>
</dd>
<dt class="label" id="id125"><span class="brackets">Feng et al., 2019</span></dt>
<dd><p>Feng, J., Cai, Q.-Z., &amp; Zhou, Z.-H. (2019). Learning to confuse: generating training time adversarial data with auto-encoder. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id567"><span class="brackets">Fernandes et al., 2019</span></dt>
<dd><p>Fernandes, S., Raj, S., Ortiz, E., Vintila, I., Salter, M., Urosevic, G., &amp; Jha, S. (2019). Predicting heart rate variations of deepfake videos using neural ode. <em>International Conference on Computer Vision Workshop</em>.</p>
</dd>
<dt class="label" id="id277"><span class="brackets">Fredrikson et al., 2015</span></dt>
<dd><p>Fredrikson, M., Jha, S., &amp; Ristenpart, T. (2015). Model inversion attacks that exploit confidence information and basic countermeasures. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 1322–1333).</p>
</dd>
<dt class="label" id="id371"><span class="brackets">Fredrikson et al., 2014</span></dt>
<dd><p>Fredrikson, M., Lantz, E., Jha, S., Lin, S., Page, D., &amp; Ristenpart, T. (2014). Privacy in pharmacogenetics: an $\$End-to-End$\$ case study of personalized warfarin dosing. <em>USENIX Security Symposium</em> (pp. 17–32).</p>
</dd>
<dt class="label" id="id22"><span class="brackets">Fridrich &amp; Kodovsky, 2012</span></dt>
<dd><p>Fridrich, J., &amp; Kodovsky, J. (2012). Rich models for steganalysis of digital images. <em>IEEE Transactions on Information Forensics and Security</em>, <em>7</em>(3), 868–882.</p>
</dd>
<dt class="label" id="id54"><span class="brackets">Frosst et al., 2019</span></dt>
<dd><p>Frosst, N., Papernot, N., &amp; Hinton, G. (2019). Analyzing and improving representations with the soft nearest neighbor loss. <em>International Conference on Machine Learning</em> (pp. 2012–2020).</p>
</dd>
<dt class="label" id="id553"><span class="brackets">Gal et al., 2022</span></dt>
<dd><p>Gal, R., Patashnik, O., Maron, H., Bermano, A. H., Chechik, G., &amp; Cohen-Or, D. (2022). Stylegan-nada: clip-guided domain adaptation of image generators. <em>ACM Transactions on Graphics</em>, <em>41</em>(4), 1–13.</p>
</dd>
<dt class="label" id="id182"><span class="brackets">Gal &amp; Ghahramani, 2016</span></dt>
<dd><p>Gal, Y., &amp; Ghahramani, Z. (2016). A theoretically grounded application of dropout in recurrent neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>29</em>.</p>
</dd>
<dt class="label" id="id473"><span class="brackets">Garrido et al., 2014</span></dt>
<dd><p>Garrido, P., Valgaerts, L., Rehmsen, O., Thormahlen, T., Perez, P., &amp; Theobalt, C. (2014). Automatic face reenactment. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 4217–4224).</p>
</dd>
<dt class="label" id="id4"><span class="brackets">Gaschnig, 1979</span></dt>
<dd><p>Gaschnig, J. (1979). Preliminary performance analysis of the prospector consultant system for mineral exploration. <em>International Joint Conference on Artificial Intelligence</em> (pp. 308–310).</p>
</dd>
<dt class="label" id="id467"><span class="brackets">Geiping et al., 2020</span></dt>
<dd><p>Geiping, J., Bauermeister, H., Dröge, H., &amp; Moeller, M. (2020). Inverting gradients-how easy is it to break privacy in federated learning? <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 16937–16947.</p>
</dd>
<dt class="label" id="id121"><span class="brackets">Geiping et al., 2021</span></dt>
<dd><p>Geiping, J., Fowl, L. H., Huang, W. R., Czaja, W., Taylor, G., Moeller, M., &amp; Goldstein, T. (2021). Witches' brew: industrial scale data poisoning via gradient matching. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id160"><span class="brackets">Gentry, 2009</span></dt>
<dd><p>Gentry, C. (2009). <em>A fully homomorphic encryption scheme</em>. Stanford university.</p>
</dd>
<dt class="label" id="id514"><span class="brackets">Ghosh et al., 2017</span></dt>
<dd><p>Ghosh, A., Kumar, H., &amp; Sastry, P. S. (2017). Robust loss functions under label noise for deep neural networks. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id145"><span class="brackets">Gilmer et al., 2019</span></dt>
<dd><p>Gilmer, J., Ford, N., Carlini, N., &amp; Cubuk, E. (2019). Adversarial examples are a natural consequence of test error in noise. <em>International Conference on Machine Learning</em> (pp. 2280–2289).</p>
</dd>
<dt class="label" id="id141"><span class="brackets">Glorot et al., 2011</span></dt>
<dd><p>Glorot, X., Bordes, A., &amp; Bengio, Y. (2011). Deep sparse rectifier neural networks. <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 315–323).</p>
</dd>
<dt class="label" id="id323"><span class="brackets">Goldblum et al., 2020</span></dt>
<dd><p>Goldblum, M., Fowl, L., Feizi, S., &amp; Goldstein, T. (2020). Adversarially robust distillation. <em>AAAI Conference on Artificial Intelligence</em> (pp. 3996–4003).</p>
</dd>
<dt class="label" id="id238"><span class="brackets">Golub &amp; Vorst, 2000</span></dt>
<dd><p>Golub, G. H., &amp; Van der Vorst, H. A. (2000). Eigenvalue computation in the 20th century. <em>Journal of Computational and Applied Mathematics</em>, <em>123</em>(1-2), 35–65.</p>
</dd>
<dt class="label" id="id420"><span class="brackets">Gong et al., 2020</span></dt>
<dd><p>Gong, C., Ren, T., Ye, M., &amp; Liu, Q. (2020). Maxup: a simple way to improve generalization of neural network training. <em>arXiv preprint arXiv:2002.09024</em>.</p>
</dd>
<dt class="label" id="id167"><span class="brackets">Gong et al., 2017</span></dt>
<dd><p>Gong, Z., Wang, W., &amp; Ku, W.-S. (2017). Adversarial and clean data are not twins. <em>arXiv preprint arXiv:1704.04960</em>.</p>
</dd>
<dt class="label" id="id507"><span class="brackets">Goodfellow, 2019</span></dt>
<dd><p>Goodfellow, I. (2019). A research agenda: dynamic models to defend against correlated attacks. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id248"><span class="brackets">Goodfellow et al., 2014</span></dt>
<dd><p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative adversarial nets. <em>Advances in Neural Information Processing Systems</em>, <em>27</em>.</p>
</dd>
<dt class="label" id="id142"><span class="brackets">Goodfellow et al., 2013</span></dt>
<dd><p>Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., &amp; Bengio, Y. (2013). Maxout networks. <em>International Conference on Machine Learning</em> (pp. 1319–1327).</p>
</dd>
<dt class="label" id="id98"><span class="brackets">Goodfellow et al., 2015</span></dt>
<dd><p>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. (2015). Explaining and harnessing adversarial examples. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id428"><span class="brackets">Gowal et al., 2019</span></dt>
<dd><p>Gowal, S., Dvijotham, K. D., Stanforth, R., Bunel, R., Qin, C., Uesato, J., … Kohli, P. (2019). Scalable verified training for provably robust image classification. <em>International Conference on Computer Vision</em> (pp. 4842–4851).</p>
</dd>
<dt class="label" id="id301"><span class="brackets">Gowal et al., 2021</span></dt>
<dd><p>Gowal, S., Rebuffi, S.-A., Wiles, O., Stimberg, F., Calian, D. A., &amp; Mann, T. A. (2021). Improving robustness using generated data. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 4218–4233.</p>
</dd>
<dt class="label" id="id175"><span class="brackets">Gretton et al., 2012</span></dt>
<dd><p>Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., &amp; Smola, A. (2012). A kernel two-sample test. <em>The Journal of Machine Learning Research</em>, <em>13</em>(1), 723–773.</p>
</dd>
<dt class="label" id="id166"><span class="brackets">Grosse et al., 2017</span></dt>
<dd><p>Grosse, K., Manoharan, P., Papernot, N., Backes, M., &amp; McDaniel, P. (2017). On the (statistical) detection of adversarial examples. <em>arXiv preprint arXiv:1702.06280</em>.</p>
</dd>
<dt class="label" id="id223"><span class="brackets">Gu et al., 2017</span></dt>
<dd><p>Gu, T., Dolan-Gavitt, B., &amp; Garg, S. (2017). Badnets: identifying vulnerabilities in the machine learning model supply chain. <em>arXiv preprint arXiv:1708.06733</em>.</p>
</dd>
<dt class="label" id="id574"><span class="brackets">Guarnera et al., 2020</span></dt>
<dd><p>Guarnera, L., Giudice, O., &amp; Battiato, S. (2020). Deepfake detection by analyzing convolutional traces. <em>IEEE/CVF Computer Vision and Pattern Recognition Conference Workshop</em>.</p>
</dd>
<dt class="label" id="id343"><span class="brackets">Guo et al., 2020</span></dt>
<dd><p>Guo, M., Yang, Y., Xu, R., Liu, Z., &amp; Lin, D. (2020). When nas meets robustness: in search of robust architectures against adversarial attacks. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 631–640).</p>
</dd>
<dt class="label" id="id408"><span class="brackets">Guo et al., 2019</span></dt>
<dd><p>Guo, W., Wang, L., Xing, X., Du, M., &amp; Song, D. (2019). Tabor: a highly accurate approach to inspecting and restoring trojan backdoors in ai systems. <em>arXiv preprint arXiv:1908.01763</em>.</p>
</dd>
<dt class="label" id="id586"><span class="brackets">Guo et al., 2021</span></dt>
<dd><p>Guo, Z., Yang, G., Chen, J., &amp; Sun, X. (2021). Fake face detection via adaptive manipulation traces extraction network. <em>CVIU</em>.</p>
</dd>
<dt class="label" id="id506"><span class="brackets">Gupta &amp; Rahtu, 2019</span></dt>
<dd><p>Gupta, P., &amp; Rahtu, E. (2019). Ciidefence: defeating adversarial attacks by fusing class-specific image inpainting and image denoising. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 6708–6717).</p>
</dd>
<dt class="label" id="id156"><span class="brackets">Gupta et al., 2021</span></dt>
<dd><p>Gupta, U., Stripelis, D., Lam, P. K., Thompson, P., Ambite, J. L., &amp; Ver Steeg, G. (2021). Membership inference attacks on deep regression models for neuroimaging. <em>Medical Imaging with Deep Learning</em> (pp. 228–251).</p>
</dd>
<dt class="label" id="id442"><span class="brackets">Gurobi, 2020</span></dt>
<dd><p>Gurobi, L. (2020). <em>“Gurobi - the fastest solver - gurobi,” Gurobi Optimization</em>.</p>
</dd>
<dt class="label" id="id225"><span class="brackets">Hampel, 1974</span></dt>
<dd><p>Hampel, F. R. (1974). The influence curve and its role in robust estimation. <em>Journal of the American Statistical Association</em>, <em>69</em>(346), 383–393.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">Hartigan &amp; Wong, 1979</span></dt>
<dd><p>Hartigan, J. A., &amp; Wong, M. A. (1979). Algorithm as 136: a k-means clustering algorithm. <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em>, <em>28</em>(1), 100–108.</p>
</dd>
<dt class="label" id="id155"><span class="brackets">Hayes et al., 2019</span></dt>
<dd><p>Hayes, J., Melis, L., Danezis, G., &amp; De Cristofaro, E. (2019). Logan: membership inference attacks against generative models. <em>Privacy Enhancing Technologies</em>, <em>2019</em>(1), 133–152.</p>
</dd>
<dt class="label" id="id530"><span class="brackets">He et al., 2021a</span></dt>
<dd><p>He, J., Erfani, S., Ma, X., Bailey, J., Chi, Y., &amp; Hua, X.-S. (2021). Alpha-iou: a family of power intersection over union losses for bounding box regression. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 20230–20242.</p>
</dd>
<dt class="label" id="id540"><span class="brackets">He et al., 2022</span></dt>
<dd><p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2022). Masked autoencoders are scalable vision learners. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 16000–16009).</p>
</dd>
<dt class="label" id="id539"><span class="brackets">He et al., 2020</span></dt>
<dd><p>He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 9729–9738).</p>
</dd>
<dt class="label" id="id517"><span class="brackets">He et al., 2015</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Delving deep into rectifiers: surpassing human-level performance on imagenet classification. <em>International Conference on Computer Vision</em> (pp. 1026–1034).</p>
</dd>
<dt class="label" id="id523"><span class="brackets">He et al., 2016</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 770–778).</p>
</dd>
<dt class="label" id="id576"><span class="brackets">He et al., 2019</span></dt>
<dd><p>He, P., Li, H., &amp; Wang, H. (2019). Detection of fake images via the ensemble of deep representations from multi color spaces. <em>ICIP</em>.</p>
</dd>
<dt class="label" id="id374"><span class="brackets">He et al., 2021b</span></dt>
<dd><p>He, X., Jia, J., Backes, M., Gong, N. Z., &amp; Zhang, Y. (2021). Stealing links from graph neural networks. <em>USENIX Security Symposium</em> (pp. 2669–2686).</p>
</dd>
<dt class="label" id="id454"><span class="brackets">Hein &amp; Andriushchenko, 2017</span></dt>
<dd><p>Hein, M., &amp; Andriushchenko, M. (2017). Formal guarantees on the robustness of a classifier against adversarial manipulation. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>.</p>
</dd>
<dt class="label" id="id171"><span class="brackets">Hendrycks &amp; Gimpel, 2016a</span></dt>
<dd><p>Hendrycks, D., &amp; Gimpel, K. (2016). Early methods for detecting adversarial images. <em>arXiv preprint arXiv:1608.00530</em>.</p>
</dd>
<dt class="label" id="id318"><span class="brackets">Hendrycks &amp; Gimpel, 2016b</span></dt>
<dd><p>Hendrycks, D., &amp; Gimpel, K. (2016). Gaussian error linear units (gelus). <em>arXiv preprint arXiv:1606.08415</em>.</p>
</dd>
<dt class="label" id="id299"><span class="brackets">Hinton et al., 2015</span></dt>
<dd><p>Hinton, G., Vinyals, O., Dean, J., &amp; others. (2015). Distilling the knowledge in a neural network. <em>arXiv preprint arXiv:1503.02531</em>, <em>2</em>(7).</p>
</dd>
<dt class="label" id="id10"><span class="brackets">Hinton &amp; Salakhutdinov, 2006</span></dt>
<dd><p>Hinton, G. E., &amp; Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. <em>Science</em>, <em>313</em>(5786), 504–507.</p>
</dd>
<dt class="label" id="id176"><span class="brackets">Hitaj et al., 2017</span></dt>
<dd><p>Hitaj, B., Ateniese, G., &amp; Perez-Cruz, F. (2017). Deep models under the gan: information leakage from collaborative deep learning. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 603–618).</p>
</dd>
<dt class="label" id="id549"><span class="brackets">Ho et al., 2020</span></dt>
<dd><p>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 6840–6851.</p>
</dd>
<dt class="label" id="id139"><span class="brackets">Hochreiter &amp; Schmidhuber, 1997</span></dt>
<dd><p>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, <em>9</em>(8), 1735–1780.</p>
</dd>
<dt class="label" id="id150"><span class="brackets">Homer et al., 2008</span></dt>
<dd><p>Homer, N., Szelinger, S., Redman, M., Duggan, D., Tembe, W., Muehling, J., … Craig, D. W. (2008). Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. <em>PLOS Genetics</em>, <em>4</em>(8), e1000167.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">Hong et al., 2018</span></dt>
<dd><p>Hong, S., Yan, X., Huang, T. S., &amp; Lee, H. (2018). Learning hierarchical semantic image manipulation through structured representations. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id345"><span class="brackets">Hosseini et al., 2021</span></dt>
<dd><p>Hosseini, R., Yang, X., &amp; Xie, P. (2021). Dsrna: differentiable search of robust neural architectures. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 6196–6205).</p>
</dd>
<dt class="label" id="id197"><span class="brackets">Hu et al., 2019</span></dt>
<dd><p>Hu, S., Yu, T., Guo, C., Chao, W.-L., &amp; Weinberger, K. Q. (2019). A new defense against adversarial images: turning a weakness into a strength. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">Hu et al., 2020</span></dt>
<dd><p>Hu, X., Zhang, Z., Jiang, Z., Chaudhuri, S., Yang, Z., &amp; Nevatia, R. (2020). Span: spatial pyramid attention network for image manipulation localization. <em>European Conference on Computer Vision</em> (pp. 312–328).</p>
</dd>
<dt class="label" id="id39"><span class="brackets">Huang et al., 2020a</span></dt>
<dd><p>Huang, H., Ma, X., Erfani, S. M., Bailey, J., &amp; Wang, Y. (2020). Unlearnable examples: making personal data unexploitable. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id347"><span class="brackets">Huang et al., 2021</span></dt>
<dd><p>Huang, H., Wang, Y., Erfani, S., Gu, Q., Bailey, J., &amp; Ma, X. (2021). Exploring architectural ingredients of adversarially robust deep neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 5545–5559.</p>
</dd>
<dt class="label" id="id236"><span class="brackets">Huang et al., 2016</span></dt>
<dd><p>Huang, R., Xu, B., Schuurmans, D., &amp; Szepesvári, C. (2016). Learning with a strong adversary. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id120"><span class="brackets">Huang et al., 2020b</span></dt>
<dd><p>Huang, W. R., Geiping, J., Fowl, L., Taylor, G., &amp; Goldstein, T. (2020). Metapoison: practical general-purpose clean-label data poisoning. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 12080–12091.</p>
</dd>
<dt class="label" id="id84"><span class="brackets">Ilyas et al., 2018</span></dt>
<dd><p>Ilyas, A., Engstrom, L., Athalye, A., &amp; Lin, J. (2018). Black-box adversarial attacks with limited queries and information. <em>International Conference on Machine Learning</em> (pp. 2137–2146).</p>
</dd>
<dt class="label" id="id132"><span class="brackets">Ilyas et al., 2019</span></dt>
<dd><p>Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., &amp; Madry, A. (2019). Adversarial examples are not bugs, they are features. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id300"><span class="brackets">Izmailov et al., 2018</span></dt>
<dd><p>Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., &amp; Wilson, A. G. (2018). Averaging weights leads to wider optima and better generalization. <em>Conference on Uncertainty in Artificial Intelligence</em> (pp. 876–885).</p>
</dd>
<dt class="label" id="id104"><span class="brackets">Jagielski et al., 2020</span></dt>
<dd><p>Jagielski, M., Carlini, N., Berthelot, D., Kurakin, A., &amp; Papernot, N. (2020). High accuracy and high fidelity extraction of neural networks. <em>USENIX Security Symposium</em> (pp. 1345–1362).</p>
</dd>
<dt class="label" id="id470"><span class="brackets">Jagielski et al., 2018</span></dt>
<dd><p>Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., &amp; Li, B. (2018). Manipulating machine learning: poisoning attacks and countermeasures for regression learning. <em>IEEE Symposium on Security and Privacy</em> (pp. 19–35).</p>
</dd>
<dt class="label" id="id140"><span class="brackets">Jarrett et al., 2009</span></dt>
<dd><p>Jarrett, K., Kavukcuoglu, K., Ranzato, Marc'Aurelio, &amp; LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? <em>International Conference on Computer Vision</em> (pp. 2146–2153).</p>
</dd>
<dt class="label" id="id457"><span class="brackets">Jeong &amp; Shin, 2020</span></dt>
<dd><p>Jeong, J., &amp; Shin, J. (2020). Consistency regularization for certified robustness of smoothed classifiers. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 10558–10570.</p>
</dd>
<dt class="label" id="id503"><span class="brackets">Jeong et al., 2022</span></dt>
<dd><p>Jeong, Y., Kim, D., Min, S., Joe, S., Gwon, Y., &amp; Choi, J. (2022). Bihpf: bilateral high-pass filters for robust deepfake detection. <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em> (pp. 48–57).</p>
</dd>
<dt class="label" id="id53"><span class="brackets">Jia et al., 2021</span></dt>
<dd><p>Jia, H., Choquette-Choo, C. A., Chandrasekaran, V., &amp; Papernot, N. (2021). Entangled watermarks as a defense against model extraction. <em>USENIX Security Symposium</em> (pp. 1937–1954).</p>
</dd>
<dt class="label" id="id449"><span class="brackets">Jia &amp; Rinard, 2021</span></dt>
<dd><p>Jia, K., &amp; Rinard, M. (2021). Exploiting verified neural networks via floating point numerical error. <em>International Static Analysis Symposium</em> (pp. 191–205).</p>
</dd>
<dt class="label" id="id460"><span class="brackets">Jia et al., 2019a</span></dt>
<dd><p>Jia, R., Raghunathan, A., Göksel, K., &amp; Liang, P. (2019). Certified robustness to adversarial word substitutions. <em>arXiv preprint arXiv:1909.00986</em>.</p>
</dd>
<dt class="label" id="id384"><span class="brackets">Jia et al., 2019b</span></dt>
<dd><p>Jia, X., Wei, X., Cao, X., &amp; Foroosh, H. (2019). Comdefend: an efficient image compression model to defend adversarial examples. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 6084–6092).</p>
</dd>
<dt class="label" id="id109"><span class="brackets">Jiang et al., 2019</span></dt>
<dd><p>Jiang, Y., Konečn\`y, J., Rush, K., &amp; Kannan, S. (2019). Improving federated learning personalization via model agnostic meta learning. <em>arXiv preprint arXiv:1909.12488</em>.</p>
</dd>
<dt class="label" id="id216"><span class="brackets">Jin et al., 2019</span></dt>
<dd><p>Jin, G., Shen, S., Zhang, D., Dai, F., &amp; Zhang, Y. (2019). Ape-gan: adversarial perturbation elimination with gan. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em> (pp. 3842–3846).</p>
</dd>
<dt class="label" id="id134"><span class="brackets">Jin &amp; Wang, 2018</span></dt>
<dd><p>Jin, H., &amp; Wang, S. (2018 , October 9). <em>Voice-based determination of physical and emotional characteristics of users</em>. US Patent 10,096,319.</p>
</dd>
<dt class="label" id="id416"><span class="brackets">Jin et al., 2021</span></dt>
<dd><p>Jin, X., Chen, P.-Y., Hsu, C.-Y., Yu, C.-M., &amp; Chen, T. (2021). Cafe: catastrophic data leakage in vertical federated learning. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 994–1006.</p>
</dd>
<dt class="label" id="id287"><span class="brackets">Jolliffe, 2002</span></dt>
<dd><p>Jolliffe, I. T. (2002). <em>Principal component analysis for special types of data</em>. Springer.</p>
</dd>
<dt class="label" id="id462"><span class="brackets">Jovanovic et al., 2021</span></dt>
<dd><p>Jovanović, N., Balunović, M., Baader, M., &amp; Vechev, M. (2021). Certified defenses: why tighter relaxations may hurt training. <em>arXiv preprint arXiv:2102.06700</em>.</p>
</dd>
<dt class="label" id="id520"><span class="brackets">Jumper et al., 2021</span></dt>
<dd><p>Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., … others. (2021). Highly accurate protein structure prediction with alphafold. <em>Nature</em>, <em>596</em>(7873), 583–589.</p>
</dd>
<dt class="label" id="id572"><span class="brackets">Jung et al., 2020</span></dt>
<dd><p>Jung, T., Kim, S., &amp; Kim, K. (2020). Deepvision: deepfakes detection using human eye blinking pattern. <em>IEEE Access</em>.</p>
</dd>
<dt class="label" id="id310"><span class="brackets">Juuti et al., 2019</span></dt>
<dd><p>Juuti, M., Szyller, S., Marchal, S., &amp; Asokan, N. (2019). Prada: protecting against dnn model stealing attacks. <em>IEEE European Symposium on Security and Privacy</em> (pp. 512–527).</p>
</dd>
<dt class="label" id="id275"><span class="brackets">Kannan et al., 2018</span></dt>
<dd><p>Kannan, H., Kurakin, A., &amp; Goodfellow, I. (2018). Adversarial logit pairing. <em>arXiv preprint arXiv:1803.06373</em>.</p>
</dd>
<dt class="label" id="id108"><span class="brackets">Karimireddy et al., 2020</span></dt>
<dd><p>Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., &amp; Suresh, A. T. (2020). Scaffold: stochastic controlled averaging for federated learning. <em>International Conference on Machine Learning</em> (pp. 5132–5143).</p>
</dd>
<dt class="label" id="id593"><span class="brackets">Karras et al., 2020</span></dt>
<dd><p>Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., &amp; Aila, T. (2020). Analyzing and improving the image quality of stylegan. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 8110–8119).</p>
</dd>
<dt class="label" id="id137"><span class="brackets">Kearns &amp; Li, 1993</span></dt>
<dd><p>Kearns, M., &amp; Li, M. (1993). Learning in the presence of malicious errors. <em>SIAM Journal on Computing</em>, <em>22</em>(4), 807–837.</p>
</dd>
<dt class="label" id="id311"><span class="brackets">Kesarwani et al., 2018</span></dt>
<dd><p>Kesarwani, M., Mukhoty, B., Arya, V., &amp; Mehta, S. (2018). Model extraction warning in mlaas paradigm. <em>Annual Computer Security Applications Conference</em> (pp. 371–380).</p>
</dd>
<dt class="label" id="id271"><span class="brackets">Kifer &amp; Lin, 2010</span></dt>
<dd><p>Kifer, D., &amp; Lin, B.-R. (2010). Towards an axiomatization of statistical privacy and utility. <em>ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems</em> (pp. 147–158).</p>
</dd>
<dt class="label" id="id548"><span class="brackets">Kim et al., 2022</span></dt>
<dd><p>Kim, G., Kwon, T., &amp; Ye, J. C. (2022). Diffusionclip: text-guided diffusion models for robust image manipulation. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 2426–2435).</p>
</dd>
<dt class="label" id="id226"><span class="brackets">Kingma &amp; Ba, 2015</span></dt>
<dd><p>Kingma, D. P., &amp; Ba, J. (2015). Adam: a method for stochastic optimization. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id249"><span class="brackets">Kingma &amp; Welling, 2013</span></dt>
<dd><p>Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. <em>arXiv preprint arXiv:1312.6114</em>.</p>
</dd>
<dt class="label" id="id335"><span class="brackets">Koffas et al., 2021</span></dt>
<dd><p>Koffas, S., Xu, J., Conti, M., &amp; Picek, S. (2021). Can you hear it? backdoor attacks via ultrasonic triggers. <em>arXiv preprint arXiv:2107.14569</em>.</p>
</dd>
<dt class="label" id="id127"><span class="brackets">Koh &amp; Liang, 2017</span></dt>
<dd><p>Koh, P. W., &amp; Liang, P. (2017). Understanding black-box predictions via influence functions. <em>International Conference on Machine Learning</em> (pp. 1885–1894).</p>
</dd>
<dt class="label" id="id119"><span class="brackets">Koh et al., 2022</span></dt>
<dd><p>Koh, P. W., Steinhardt, J., &amp; Liang, P. (2022). Stronger data poisoning attacks break data sanitization defenses. <em>Machine Learning</em>, <em>111</em>(1), 1–47.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">Korshunova et al., 2017</span></dt>
<dd><p>Korshunova, I., Shi, W., Dambre, J., &amp; Theis, L. (2017). Fast face-swap using convolutional neural networks. <em>International Conference on Computer Vision</em> (pp. 3677–3685).</p>
</dd>
<dt class="label" id="id516"><span class="brackets">Krizhevsky et al., 2017</span></dt>
<dd><p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. <em>Communications of the ACM</em>, <em>60</em>(6), 84–90.</p>
</dd>
<dt class="label" id="id118"><span class="brackets">Kumar et al., 2020</span></dt>
<dd><p>Kumar, R. S. S., Nyström, M., Lambert, J., Marshall, A., Goertzel, M., Comissoneru, A., … Xia, S. (2020). Adversarial machine learning-industry perspectives. <em>IEEE Security and Privacy Workshops</em> (pp. 69–75).</p>
</dd>
<dt class="label" id="id469"><span class="brackets">Kurakin et al., 2016</span></dt>
<dd><p>Kurakin, A., Goodfellow, I., &amp; Bengio, S. (2016). Adversarial machine learning at scale. <em>arXiv preprint arXiv:1611.01236</em>.</p>
</dd>
<dt class="label" id="id72"><span class="brackets">Kurakin et al., 2018</span></dt>
<dd><p>Kurakin, A., Goodfellow, I. J., &amp; Bengio, S. (2018). Adversarial examples in the physical world. <em>Artificial Intelligence Safety and Security</em> (pp. 99–112). Chapman and Hall/CRC.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">LeMerrer et al., 2020</span></dt>
<dd><p>Le Merrer, E., Perez, P., &amp; Trédan, G. (2020). Adversarial frontier stitching for remote neural network watermarking. <em>Neural Computing and Applications</em>, <em>32</em>(13), 9233–9244.</p>
</dd>
<dt class="label" id="id180"><span class="brackets">Lee et al., 2018</span></dt>
<dd><p>Lee, K., Lee, K., Lee, H., &amp; Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id464"><span class="brackets">Lee et al., 2020</span></dt>
<dd><p>Lee, S., Lee, J., &amp; Park, S. (2020). Lipschitz-certifiable training with a tight outer bound. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 16891–16902.</p>
</dd>
<dt class="label" id="id214"><span class="brackets">Leino &amp; Fredrikson, 2020</span></dt>
<dd><p>Leino, K., &amp; Fredrikson, M. (2020). Stolen memories: leveraging model memorization for calibrated $\$White-Box$\$ membership inference. <em>USENIX Security Symposium</em> (pp. 1605–1622).</p>
</dd>
<dt class="label" id="id465"><span class="brackets">Leino et al., 2021</span></dt>
<dd><p>Leino, K., Wang, Z., &amp; Fredrikson, M. (2021). Globally-robust neural networks. <em>International Conference on Machine Learning</em> (pp. 6212–6222).</p>
</dd>
<dt class="label" id="id422"><span class="brackets">Levine &amp; Feizi, 2021</span></dt>
<dd><p>Levine, A., &amp; Feizi, S. (2021). Deep partition aggregation: provable defense against general poisoning attacks. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id590"><span class="brackets">Li et al., 2021a</span></dt>
<dd><p>Li, A., Ke, Q., Ma, X., Weng, H., Zong, Z., Xue, F., &amp; Zhang, R. (2021). Noise doesn't lie: towards universal detection of deep inpainting. <em>International Joint Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id429"><span class="brackets">Li et al., 2019a</span></dt>
<dd><p>Li, B., Chen, C., Wang, W., &amp; Carin, L. (2019). Certified adversarial robustness with additive noise. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id577"><span class="brackets">Li et al., 2020a</span></dt>
<dd><p>Li, H., Li, B., Tan, S., &amp; Huang, J. (2020). Identification of deep network generated images using disparities in color components. <em>Signal Processing</em>.</p>
</dd>
<dt class="label" id="id579"><span class="brackets">Li et al., 2004</span></dt>
<dd><p>Li, J., Wang, Y., Tan, T., &amp; Jain, A. K. (2004). Live face detection based on the analysis of fourier spectra. <em>BTHI</em>.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">Li et al., 2020b</span></dt>
<dd><p>Li, L., Bao, J., Yang, H., Chen, D., &amp; Wen, F. (2020). Advancing high fidelity identity swapping for forgery detection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 5074–5083).</p>
</dd>
<dt class="label" id="id30"><span class="brackets">Li et al., 2020c</span></dt>
<dd><p>Li, L., Bao, J., Zhang, T., Yang, H., Chen, D., Wen, F., &amp; Guo, B. (2020). Face x-ray for more general face forgery detection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 5001–5010).</p>
</dd>
<dt class="label" id="id423"><span class="brackets">Li et al., 2023</span></dt>
<dd><p>Li, L., Qi, X., Xie, T., &amp; Li, B. (2023). Sok: certified robustness for deep neural networks. <em>IEEE Symposium on Security and Privacy</em>.</p>
</dd>
<dt class="label" id="id453"><span class="brackets">Li et al., 2019b</span></dt>
<dd><p>Li, Q., Haque, S., Anil, C., Lucas, J., Grosse, R. B., &amp; Jacobsen, J.-H. (2019). Preventing gradient attenuation in lipschitz constrained convolutional networks. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id99"><span class="brackets">Li et al., 2020d</span></dt>
<dd><p>Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., &amp; Smith, V. (2020). Federated optimization in heterogeneous networks. <em>Proceedings of Machine Learning and Systems</em>, <em>2</em>, 429–450.</p>
</dd>
<dt class="label" id="id592"><span class="brackets">Li et al., 2017</span></dt>
<dd><p>Li, T., Bolkart, T., Black, M. J., Li, H., &amp; Romero, J. (2017). Learning a model of facial shape and expression from 4d scans. <em>ACM Transactions on Graphics</em>, <em>36</em>(6).</p>
</dd>
<dt class="label" id="id174"><span class="brackets">Li &amp; Li, 2017</span></dt>
<dd><p>Li, X., &amp; Li, F. (2017). Adversarial examples detection in deep networks with convolutional filter statistics. <em>International Conference on Computer Vision</em> (pp. 5764–5772).</p>
</dd>
<dt class="label" id="id344"><span class="brackets">Li et al., 2021b</span></dt>
<dd><p>Li, Y., Yang, Z., Wang, Y., &amp; Xu, C. (2021). Neural architecture dilation for adversarial robustness. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 29578–29589.</p>
</dd>
<dt class="label" id="id396"><span class="brackets">Li et al., 2021c</span></dt>
<dd><p>Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., &amp; Ma, X. (2021). Anti-backdoor learning: training clean models on poisoned data. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 14900–14912.</p>
</dd>
<dt class="label" id="id329"><span class="brackets">Li et al., 2021d</span></dt>
<dd><p>Li, Y., Li, Y., Lv, Y., Jiang, Y., &amp; Xia, S.-T. (2021). Hidden backdoor attack against semantic segmentation models. <em>arXiv preprint arXiv:2103.04038</em>.</p>
</dd>
<dt class="label" id="id327"><span class="brackets">Li et al., 2022</span></dt>
<dd><p>Li, Y., Zhong, H., Ma, X., Jiang, Y., &amp; Xia, S.-T. (2022). Few-shot backdoor attacks on visual object tracking. <em>arXiv preprint arXiv:2201.13178</em>.</p>
</dd>
<dt class="label" id="id481"><span class="brackets">Li et al., 2018</span></dt>
<dd><p>Li, Y., Chang, M.-C., &amp; Lyu, S. (2018). In ictu oculi: exposing ai created fake videos by detecting eye blinking. <em>IEEE International Workshop on Information Forensics and Security</em> (pp. 1–7).</p>
</dd>
<dt class="label" id="id232"><span class="brackets">Li et al., 2021e</span></dt>
<dd><p>Li, Y., Li, Y., Wu, B., Li, L., He, R., &amp; Lyu, S. (2021). Invisible backdoor attack with sample-specific triggers. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 16463–16472).</p>
</dd>
<dt class="label" id="id381"><span class="brackets">Liao et al., 2018</span></dt>
<dd><p>Liao, F., Liang, M., Dong, Y., Pang, T., Hu, X., &amp; Zhu, J. (2018). Defense against adversarial attacks using high-level representation guided denoiser. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 1778–1787).</p>
</dd>
<dt class="label" id="id92"><span class="brackets">Lin et al., 2019</span></dt>
<dd><p>Lin, J., Song, C., He, K., Wang, L., &amp; Hopcroft, J. E. (2019). Nesterov accelerated gradient and scale invariance for adversarial attacks. <em>arXiv preprint arXiv:1908.06281</em>.</p>
</dd>
<dt class="label" id="id513"><span class="brackets">Lin et al., 2014</span></dt>
<dd><p>Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., … Zitnick, C. L. (2014). Microsoft coco: common objects in context. <em>European Conference on Computer Vision</em> (pp. 740–755).</p>
</dd>
<dt class="label" id="id206"><span class="brackets">Liu et al., 2019a</span></dt>
<dd><p>Liu, G., Wang, C., Peng, K., Huang, H., Li, Y., &amp; Cheng, W. (2019). Socinf: membership inference attacks on social media health data with machine learning. <em>IEEE Transactions on Computational Social Systems</em>, <em>6</em>(5), 907–921.</p>
</dd>
<dt class="label" id="id357"><span class="brackets">Liu et al., 2019b</span></dt>
<dd><p>Liu, H., Simonyan, K., &amp; Yang, Y. (2019). Darts: differentiable architecture search. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id407"><span class="brackets">Liu et al., 2018a</span></dt>
<dd><p>Liu, K., Dolan-Gavitt, B., &amp; Garg, S. (2018). Fine-pruning: defending against backdooring attacks on deep neural networks. <em>International Symposium on Research in Attacks, Intrusions, and Defenses</em> (pp. 273–294).</p>
</dd>
<dt class="label" id="id535"><span class="brackets">Liu et al., 2017</span></dt>
<dd><p>Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., &amp; Song, L. (2017). Sphereface: deep hypersphere embedding for face recognition. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 212–220).</p>
</dd>
<dt class="label" id="id534"><span class="brackets">Liu et al., 2016a</span></dt>
<dd><p>Liu, W., Wen, Y., Yu, Z., &amp; Yang, M. (2016). Large-margin softmax loss for convolutional neural networks. <em>International Conference on Machine Learning</em> (pp. 507–516).</p>
</dd>
<dt class="label" id="id28"><span class="brackets">Liu et al., 2022</span></dt>
<dd><p>Liu, X., Liu, Y., Chen, J., &amp; Liu, X. (2022). Pscc-net: progressive spatio-channel correlation network for image manipulation detection and localization. <em>IEEE Transactions on Circuits and Systems for Video Technology</em>.</p>
</dd>
<dt class="label" id="id88"><span class="brackets">Liu et al., 2016b</span></dt>
<dd><p>Liu, Y., Chen, X., Liu, C., &amp; Song, D. (2016). Delving into transferable adversarial examples and black-box attacks. <em>arXiv preprint arXiv:1611.02770</em>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">Liu et al., 2018b</span></dt>
<dd><p>Liu, Y., Ma, S., Aafer, Y., Lee, W.-C., Zhai, J., Wang, W., &amp; Zhang, X. (2018). Trojaning attack on neural networks. <em>Network and Distributed Systems Security Symposium</em>.</p>
</dd>
<dt class="label" id="id563"><span class="brackets">Liu et al., 2020</span></dt>
<dd><p>Liu, Y., Ma, X., Bailey, J., &amp; Lu, F. (2020). Reflection backdoor: a natural backdoor attack on deep neural networks. <em>European Conference on Computer Vision</em> (pp. 182–199).</p>
</dd>
<dt class="label" id="id349"><span class="brackets">Liu et al., 2021</span></dt>
<dd><p>Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … Guo, B. (2021). Swin transformer: hierarchical vision transformer using shifted windows. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 10012–10022).</p>
</dd>
<dt class="label" id="id205"><span class="brackets">Long et al., 2020</span></dt>
<dd><p>Long, Y., Wang, L., Bu, D., Bindschaedler, V., Wang, X., Tang, H., … Chen, K. (2020). A pragmatic approach to membership inferences on machine learning models. <em>IEEE European Symposium on Security and Privacy</em> (pp. 521–534).</p>
</dd>
<dt class="label" id="id561"><span class="brackets">Lorenz et al., 2022</span></dt>
<dd><p>Lorenz, P., Keuper, M., &amp; Keuper, J. (2022). Unfolding local growth rate estimates for (almost) perfect adversarial detection. <em>International Conference on Computer Vision Theory and Applications</em>.</p>
</dd>
<dt class="label" id="id67"><span class="brackets">Lukas et al., 2019</span></dt>
<dd><p>Lukas, N., Zhang, Y., &amp; Kerschbaum, F. (2019). Deep neural network fingerprinting by conferrable adversarial examples. <em>arXiv preprint arXiv:1912.00888</em>.</p>
</dd>
<dt class="label" id="id559"><span class="brackets">Lukavs et al., 2006</span></dt>
<dd><p>Lukáš, J., Fridrich, J., &amp; Goljan, M. (2006). Detecting digital image forgeries using sensor pattern noise. <em>SPIE</em>.</p>
</dd>
<dt class="label" id="id242"><span class="brackets">Lyu et al., 2015</span></dt>
<dd><p>Lyu, C., Huang, K., &amp; Liang, H.-N. (2015). A unified gradient regularization family for adversarial examples. <em>IEEE International Conference on Data Mining</em> (pp. 301–309).</p>
</dd>
<dt class="label" id="id164"><span class="brackets">Lyu et al., 2022</span></dt>
<dd><p>Lyu, L., Yu, H., Ma, X., Chen, C., Sun, L., Zhao, J., … Philip, S. Y. (2022). Privacy and robustness in federated learning: attacks and defenses. <em>IEEE Transactions on Neural Networks and Learning Systems</em>.</p>
</dd>
<dt class="label" id="id131"><span class="brackets">Ma et al., 2018</span></dt>
<dd><p>Ma, X., Li, B., Wang, Y., Erfani, S. M., Wijewickrema, S., Schoenebeck, G., … Bailey, J. (2018). Characterizing adversarial subspaces using local intrinsic dimensionality. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id143"><span class="brackets">Madry et al., 2018</span></dt>
<dd><p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2018). Towards deep learning models resistant to adversarial attacks. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id181"><span class="brackets">Mahalanobis, 1936</span></dt>
<dd><p>Mahalanobis, P. C. (1936). On the generalized distance in statistics. <em>Proceedings of the National Institute of Sciences</em>, <em>2</em>, 49–55.</p>
</dd>
<dt class="label" id="id62"><span class="brackets">Mahloujifar &amp; Mahmoody, 2017</span></dt>
<dd><p>Mahloujifar, S., &amp; Mahmoody, M. (2017). Blockwise p-tampering attacks on cryptographic primitives, extractors, and learners. <em>Theory of Cryptography Conference</em> (pp. 245–279).</p>
</dd>
<dt class="label" id="id63"><span class="brackets">Mahloujifar et al., 2019</span></dt>
<dd><p>Mahloujifar, S., Mahmoody, M., &amp; Mohammed, A. (2019). Universal multi-party poisoning attacks. <em>International Conference on Machine Learning</em> (pp. 4274–4283).</p>
</dd>
<dt class="label" id="id353"><span class="brackets">Mahmood et al., 2021</span></dt>
<dd><p>Mahmood, K., Mahmood, R., &amp; Van Dijk, M. (2021). On the robustness of vision transformers to adversarial examples. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 7838–7847).</p>
</dd>
<dt class="label" id="id113"><span class="brackets">Marfoq et al., 2021</span></dt>
<dd><p>Marfoq, O., Neglia, G., Bellet, A., Kameni, L., &amp; Vidal, R. (2021). Federated multi-task learning under a mixture of distributions. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 15434–15447.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">McMahan et al., 2017</span></dt>
<dd><p>McMahan, B., Moore, E., Ramage, D., Hampson, S., &amp; y Arcas, B. A. (2017). Communication-efficient learning of deep networks from decentralized data. <em>Artificial intelligence and statistics</em> (pp. 1273–1282).</p>
</dd>
<dt class="label" id="id42"><span class="brackets">McMahan et al., 2016</span></dt>
<dd><p>McMahan, H. B., Moore, E., Ramage, D., &amp; y Arcas, B. A. (2016). Federated learning of deep networks using model averaging. <em>arXiv preprint arXiv:1602.05629</em>, <em>2</em>.</p>
</dd>
<dt class="label" id="id274"><span class="brackets">McSherry &amp; Talwar, 2007</span></dt>
<dd><p>McSherry, F., &amp; Talwar, K. (2007). Mechanism design via differential privacy. <em>IEEE Annual Symposium on Foundations of Computer Science</em> (pp. 94–103).</p>
</dd>
<dt class="label" id="id270"><span class="brackets">McSherry, 2009</span></dt>
<dd><p>McSherry, F. D. (2009). Privacy integrated queries: an extensible platform for privacy-preserving data analysis. <em>ACM SIGMOD International Conference on Management of Data</em> (pp. 19–30).</p>
</dd>
<dt class="label" id="id117"><span class="brackets">Mei &amp; Zhu, 2015</span></dt>
<dd><p>Mei, S., &amp; Zhu, X. (2015). Using machine teaching to identify optimal training-set attacks on machine learners. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id149"><span class="brackets">Melis et al., 2019</span></dt>
<dd><p>Melis, L., Song, C., De Cristofaro, E., &amp; Shmatikov, V. (2019). Exploiting unintended feature leakage in collaborative learning. <em>IEEE Symposium on Security and Privacy</em> (pp. 691–706).</p>
</dd>
<dt class="label" id="id201"><span class="brackets">Meng &amp; Chen, 2017</span></dt>
<dd><p>Meng, D., &amp; Chen, H. (2017). Magnet: a two-pronged defense against adversarial examples. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 135–147).</p>
</dd>
<dt class="label" id="id168"><span class="brackets">Metzen et al., 2017</span></dt>
<dd><p>Metzen, J. H., Genewein, T., Fischer, V., &amp; Bischoff, B. (2017). On detecting adversarial perturbations. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id308"><span class="brackets">Micikevicius et al., 2018</span></dt>
<dd><p>Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., … others. (2018). Mixed precision training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id524"><span class="brackets">Mikolov et al., 2013</span></dt>
<dd><p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. <em>Advances in Neural Information Processing Systems</em>, <em>26</em>.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">Minsky, 1974</span></dt>
<dd><p>Minsky, M. (1974). <em>A framework for representing knowledge</em>.</p>
</dd>
<dt class="label" id="id484"><span class="brackets">Mittal et al., 2020</span></dt>
<dd><p>Mittal, T., Bhattacharya, U., Chandra, R., Bera, A., &amp; Manocha, D. (2020). Emotions don't lie: an audio-visual deepfake detection method using affective cues. <em>ACM International Conference on Multimedia</em> (pp. 2823–2832).</p>
</dd>
<dt class="label" id="id235"><span class="brackets">Miyato et al., 2016</span></dt>
<dd><p>Miyato, T., Maeda, S.-i., Koyama, M., Nakae, K., &amp; Ishii, S. (2016). Distributional smoothing with virtual adversarial training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">Moosavi-Dezfooli et al., 2016</span></dt>
<dd><p>Moosavi-Dezfooli, S.-M., Fawzi, A., &amp; Frossard, P. (2016). Deepfool: a simple and accurate method to fool deep neural networks. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 2574–2582).</p>
</dd>
<dt class="label" id="id441"><span class="brackets">Moura &amp; Bjorner, 2008</span></dt>
<dd><p>Moura, L. d., &amp; Bjørner, N. (2008). Z3: an efficient smt solver. <em>International Conference on Tools and Algorithms for the Construction and Analysis of Systems</em> (pp. 337–340).</p>
</dd>
<dt class="label" id="id443"><span class="brackets">Munoz-Gonzalez et al., 2017</span></dt>
<dd><p>Muñoz-González, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E. C., &amp; Roli, F. (2017). Towards poisoning of deep learning algorithms with back-gradient optimization. <em>ACM Workshop on Artificial Intelligence and Security</em> (pp. 27–38).</p>
</dd>
<dt class="label" id="id124"><span class="brackets">Munoz-Gonzalez et al., 2019</span></dt>
<dd><p>Muñoz-González, L., Pfitzner, B., Russo, M., Carnerero-Cano, J., &amp; Lupu, E. C. (2019). Poisoning attacks with generative adversarial nets. <em>arXiv preprint arXiv:1906.07773</em>.</p>
</dd>
<dt class="label" id="id317"><span class="brackets">Nair &amp; Hinton, 2010</span></dt>
<dd><p>Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id356"><span class="brackets">Nakkiran, 2019</span></dt>
<dd><p>Nakkiran, P. (2019). Adversarial robustness may be at odds with simplicity. <em>arXiv preprint arXiv:1901.00532</em>.</p>
</dd>
<dt class="label" id="id508"><span class="brackets">Nasr et al., 2019a</span></dt>
<dd><p>Nasr, M., Shokri, R., &amp; Houmansadr, A. (2019). Comprehensive privacy analysis of deep learning: passive and active white-box inference attacks against centralized and federated learning. <em>IEEE Symposium on Security and Privacy (SP)</em>.</p>
</dd>
<dt class="label" id="id213"><span class="brackets">Nasr et al., 2019b</span></dt>
<dd><p>Nasr, M., Shokri, R., &amp; Houmansadr, A. (2019). Comprehensive privacy analysis of deep learning: passive and active white-box inference attacks against centralized and federated learning. <em>IEEE Symposium on Security and Privacy</em> (pp. 739–753).</p>
</dd>
<dt class="label" id="id36"><span class="brackets">Nelson et al., 2008</span></dt>
<dd><p>Nelson, B., Barreno, M., Chi, F. J., Joseph, A. D., Rubinstein, B. I., Saini, U., … Xia, K. (2008). Exploiting machine learning to subvert your spam filter. <em>LEET</em>, <em>8</em>(1), 9.</p>
</dd>
<dt class="label" id="id545"><span class="brackets">Nesterov, 1983</span></dt>
<dd><p>Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o (1/kˆ 2). <em>Doklady ANSSSR</em> (pp. 543–547).</p>
</dd>
<dt class="label" id="id3"><span class="brackets">Newell &amp; Simon, 1956</span></dt>
<dd><p>Newell, A., &amp; Simon, H. (1956). The logic theory machine–a complex information processing system. <em>IRE Transactions on Information Theory</em>, <em>2</em>(3), 61–79.</p>
</dd>
<dt class="label" id="id498"><span class="brackets">Nguyen et al., 2019</span></dt>
<dd><p>Nguyen, H. H., Yamagishi, J., &amp; Echizen, I. (2019). Capsule-forensics: using capsule networks to detect forged images and videos. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em> (pp. 2307–2311).</p>
</dd>
<dt class="label" id="id466"><span class="brackets">Nguyen &amp; Tran, 2020</span></dt>
<dd><p>Nguyen, T. A., &amp; Tran, A. (2020). Input-aware dynamic backdoor attack. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 3454–3464.</p>
</dd>
<dt class="label" id="id281"><span class="brackets">Ning et al., 2020</span></dt>
<dd><p>Ning, X., Zhao, J., Li, W., Zhao, T., Yang, H., &amp; Wang, Y. (2020). Multi-shot nas for discovering adversarially robust convolutional neural architectures at targeted capacities. <em>arXiv preprint arXiv:2012.11835</em>.</p>
</dd>
<dt class="label" id="id476"><span class="brackets">Nirkin et al., 2022</span></dt>
<dd><p><strong>missing booktitle in FSGAN2022</strong></p>
</dd>
<dt class="label" id="id272"><span class="brackets">Nissim et al., 2007</span></dt>
<dd><p>Nissim, K., Raskhodnikova, S., &amp; Smith, A. (2007). Smooth sensitivity and sampling in private data analysis. <em>ACM Symposium on Theory of Computing</em> (pp. 75–84).</p>
</dd>
<dt class="label" id="id268"><span class="brackets">Novac et al., 2017</span></dt>
<dd><p>Novac, O. C., Novac, M., Gordan, C., Berczes, T., &amp; Bujdosó, G. (2017). Comparative study of google android, apple ios and microsoft windows phone mobile operating systems. <em>Engineering of Modern Electric Systems</em> (pp. 154–159).</p>
</dd>
<dt class="label" id="id234"><span class="brackets">Nokland, 2015</span></dt>
<dd><p>Nøkland, A. (2015). Improving back-propagation by adding an adversarial gradient. <em>arXiv preprint arXiv:1510.04189</em>.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">Oh et al., 2019</span></dt>
<dd><p>Oh, S. J., Schiele, B., &amp; Fritz, M. (2019). Towards reverse-engineering black-box neural networks. <em>Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</em> (pp. 121–144). Springer.</p>
</dd>
<dt class="label" id="id538"><span class="brackets">Oord et al., 2018</span></dt>
<dd><p>Oord, A. v. d., Li, Y., &amp; Vinyals, O. (2018). Representation learning with contrastive predictive coding. <em>arXiv preprint arXiv:1807.03748</em>.</p>
</dd>
<dt class="label" id="id102"><span class="brackets">Orekondy et al., 2019</span></dt>
<dd><p>Orekondy, T., Schiele, B., &amp; Fritz, M. (2019). Knockoff nets: stealing functionality of black-box models. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 4954–4963).</p>
</dd>
<dt class="label" id="id372"><span class="brackets">Pan et al., 2020</span></dt>
<dd><p>Pan, X., Zhang, M., Ji, S., &amp; Yang, M. (2020). Privacy risks of general-purpose language models. <em>IEEE Symposium on Security and Privacy</em> (pp. 1314–1331).</p>
</dd>
<dt class="label" id="id24"><span class="brackets">Pan et al., 2012</span></dt>
<dd><p>Pan, X., Zhang, X., &amp; Lyu, S. (2012). Exposing image splicing with inconsistent local noise variances. <em>IEEE International Conference on Computational Photography</em> (pp. 1–10).</p>
</dd>
<dt class="label" id="id471"><span class="brackets">Pang et al., 2018</span></dt>
<dd><p>Pang, T., Du, C., Dong, Y., &amp; Zhu, J. (2018). Towards robust detection of adversarial examples. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">Papernot et al., 2017</span></dt>
<dd><p>Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., &amp; Swami, A. (2017). Practical black-box attacks against machine learning. <em>ACM on Asia Conference on Computer and Communications Security</em> (pp. 506–519).</p>
</dd>
<dt class="label" id="id75"><span class="brackets">Papernot et al., 2016a</span></dt>
<dd><p>Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., &amp; Swami, A. (2016). The limitations of deep learning in adversarial settings. <em>IEEE European Symposium on Security and Privacy</em> (pp. 372–387).</p>
</dd>
<dt class="label" id="id298"><span class="brackets">Papernot et al., 2016b</span></dt>
<dd><p>Papernot, N., McDaniel, P., Wu, X., Jha, S., &amp; Swami, A. (2016). Distillation as a defense to adversarial perturbations against deep neural networks. <em>IEEE Symposium on Security and Privacy</em> (pp. 582–597).</p>
</dd>
<dt class="label" id="id554"><span class="brackets">Patashnik et al., 2021</span></dt>
<dd><p>Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., &amp; Lischinski, D. (2021). Styleclip: text-driven manipulation of stylegan imagery. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 2085–2094).</p>
</dd>
<dt class="label" id="id555"><span class="brackets">Pathak et al., 2016</span></dt>
<dd><p>Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., &amp; Efros, A. A. (2016). Context encoders: feature learning by inpainting. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2536–2544).</p>
</dd>
<dt class="label" id="id369"><span class="brackets">Phan et al., 2016</span></dt>
<dd><p>Phan, N., Wang, Y., Wu, X., &amp; Dou, D. (2016). Differential privacy preservation for deep auto-encoders: an application of human behavior prediction. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id220"><span class="brackets">Pillutla et al., 2019</span></dt>
<dd><p>Pillutla, K., Kakade, S. M., &amp; Harchaoui, Z. (2019). Robust aggregation for federated learning. <em>arXiv preprint arXiv:1912.13445</em>.</p>
</dd>
<dt class="label" id="id385"><span class="brackets">Prakash et al., 2018</span></dt>
<dd><p>Prakash, A., Moran, N., Garber, S., DiLillo, A., &amp; Storer, J. (2018). Deflecting adversarial attacks with pixel deflection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 8571–8580).</p>
</dd>
<dt class="label" id="id151"><span class="brackets">Pyrgelis et al., 2018</span></dt>
<dd><p>Pyrgelis, A., Troncoso, C., &amp; Cristofaro, E. D. (2018). Knock knock, who's there? membership inference on aggregate location data. <em>Network and Distributed System Security Symposium</em>. The Internet Society.</p>
</dd>
<dt class="label" id="id152"><span class="brackets">Pyrgelis et al., 2020</span></dt>
<dd><p>Pyrgelis, A., Troncoso, C., &amp; De Cristofaro, E. (2020). Measuring membership privacy on aggregate location time-series. <em>ACM on Measurement and Analysis of Computing Systems</em>, <em>4</em>(2), 1–28.</p>
</dd>
<dt class="label" id="id332"><span class="brackets">Qi et al., 2021</span></dt>
<dd><p>Qi, F., Li, M., Chen, Y., Zhang, Z., Liu, Z., Wang, Y., &amp; Sun, M. (2021). Hidden killer: invisible textual backdoor attacks with syntactic trigger. <em>arXiv preprint arXiv:2105.12400</em>.</p>
</dd>
<dt class="label" id="id546"><span class="brackets">Qian, 1999</span></dt>
<dd><p>Qian, N. (1999). On the momentum term in gradient descent learning algorithms. <em>Neural Networks</em>, <em>12</em>(1), 145–151.</p>
</dd>
<dt class="label" id="id589"><span class="brackets">Qian et al., 2020</span></dt>
<dd><p>Qian, Y., Yin, G., Sheng, L., Chen, Z., &amp; Shao, J. (2020). Thinking in frequency: face forgery detection by mining frequency-aware clues. <em>ECCV</em>.</p>
</dd>
<dt class="label" id="id505"><span class="brackets">Qiao et al., 2019</span></dt>
<dd><p>Qiao, X., Yang, Y., &amp; Li, H. (2019). Defending neural backdoors via generative distribution modeling. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id321"><span class="brackets">Qin et al., 2019</span></dt>
<dd><p>Qin, C., Martens, J., Gowal, S., Krishnan, D., Dvijotham, K., Fawzi, A., … Kohli, P. (2019). Adversarial robustness through local linearization. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id551"><span class="brackets">Radford et al., 2021</span></dt>
<dd><p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … others. (2021). Learning transferable visual models from natural language supervision. <em>International Conference on Machine Learning</em> (pp. 8748–8763).</p>
</dd>
<dt class="label" id="id527"><span class="brackets">Rahman &amp; Wang, 2016</span></dt>
<dd><p>Rahman, M. A., &amp; Wang, Y. (2016). Optimizing intersection-over-union in deep neural networks for image segmentation. <em>International Symposium on Visual Computing</em> (pp. 234–244).</p>
</dd>
<dt class="label" id="id319"><span class="brackets">Ramachandran et al., 2017</span></dt>
<dd><p>Ramachandran, P., Zoph, B., &amp; Le, Q. V. (2017). Searching for activation functions. <em>arXiv preprint arXiv:1710.05941</em>.</p>
</dd>
<dt class="label" id="id289"><span class="brackets">Rebuffi et al., 2021a</span></dt>
<dd><p>Rebuffi, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., &amp; Mann, T. (2021). Fixing data augmentation to improve adversarial robustness. <em>arXiv preprint arXiv:2103.01946</em>.</p>
</dd>
<dt class="label" id="id284"><span class="brackets">Rebuffi et al., 2021b</span></dt>
<dd><p>Rebuffi, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., &amp; Mann, T. A. (2021). Data augmentation can improve robustness. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 29935–29948.</p>
</dd>
<dt class="label" id="id390"><span class="brackets">Redmon &amp; Farhadi, 2017</span></dt>
<dd><p>Redmon, J., &amp; Farhadi, A. (2017). Yolo9000: better, faster, stronger. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 7263–7271).</p>
</dd>
<dt class="label" id="id526"><span class="brackets">Rezatofighi et al., 2019</span></dt>
<dd><p>Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., &amp; Savarese, S. (2019). Generalized intersection over union: a metric and a loss for bounding box regression. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id303"><span class="brackets">Rice et al., 2020</span></dt>
<dd><p>Rice, L., Wong, E., &amp; Kolter, Z. (2020). Overfitting in adversarially robust deep learning. <em>International Conference on Machine Learning</em> (pp. 8093–8104).</p>
</dd>
<dt class="label" id="id148"><span class="brackets">Rivest et al., 1978</span></dt>
<dd><p>Rivest, R. L., Adleman, L., Dertouzos, M. L., &amp; others. (1978). On data banks and privacy homomorphisms. <em>Foundations of Secure Computation</em>, <em>4</em>(11), 169–180.</p>
</dd>
<dt class="label" id="id382"><span class="brackets">Ronneberger et al., 2015</span></dt>
<dd><p>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-net: convolutional networks for biomedical image segmentation. <em>International Conference on Medical Image Computing and Computer Assisted Intervention</em> (pp. 234–241).</p>
</dd>
<dt class="label" id="id192"><span class="brackets">Roth et al., 2019</span></dt>
<dd><p>Roth, K., Kilcher, Y., &amp; Hofmann, T. (2019). The odds are odd: a statistical test for detecting adversarial examples. <em>International Conference on Machine Learning</em> (pp. 5498–5507).</p>
</dd>
<dt class="label" id="id199"><span class="brackets">Rubinstein et al., 2009</span></dt>
<dd><p>Rubinstein, B., Nelson, B., Ling, H., Joseph, A. D., &amp; Tygar, J. D. (2009). Antidote: understanding and defending against poisoning of anomaly detectors. <em>Acm Sigcomm Conference on Internet Measurement</em>.</p>
</dd>
<dt class="label" id="id368"><span class="brackets">Rudin &amp; others, 1976</span></dt>
<dd><p>Rudin, W., &amp; others. (1976). <em>Principles of mathematical analysis</em>. Vol. 3. McGraw-hill New York.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">Rumelhart et al., 1986</span></dt>
<dd><p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, <em>323</em>(6088), 533–536.</p>
</dd>
<dt class="label" id="id512"><span class="brackets">Russakovsky et al., 2015</span></dt>
<dd><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., … others. (2015). Imagenet large scale visual recognition challenge. <em>International journal of computer vision</em>, <em>115</em>(3), 211–252.</p>
</dd>
<dt class="label" id="id250"><span class="brackets">Saha et al., 2020</span></dt>
<dd><p>Saha, A., Subramanya, A., &amp; Pirsiavash, H. (2020). Hidden trigger backdoor attacks. <em>AAAI Conference on Artificial Intelligence</em> (pp. 11957–11965).</p>
</dd>
<dt class="label" id="id203"><span class="brackets">Salem et al., 2019</span></dt>
<dd><p>Salem, A., Zhang, Y., Humbert, M., Fritz, M., &amp; Backes, M. (2019). Ml-leaks: model and data independent membership inference attacks and defenses on machine learning models. <em>Network and Distributed Systems Security Symposium</em>.</p>
</dd>
<dt class="label" id="id434"><span class="brackets">Salman et al., 2019</span></dt>
<dd><p>Salman, H., Yang, G., Zhang, H., Hsieh, C.-J., &amp; Zhang, P. (2019). A convex relaxation barrier to tight robustness verification of neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id388"><span class="brackets">Samangouei et al., 2018</span></dt>
<dd><p>Samangouei, P., Kabkab, M., &amp; Chellappa, R. (2018). Defense-gan: protecting classifiers against adversarial attacks using generative models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id114"><span class="brackets">Sattler et al., 2020</span></dt>
<dd><p>Sattler, F., Müller, K.-R., &amp; Samek, W. (2020). Clustered federated learning: model-agnostic distributed multitask optimization under privacy constraints. <em>IEEE Transactions on Neural Networks and Learning Systems</em>, <em>32</em>(8), 3710–3722.</p>
</dd>
<dt class="label" id="id144"><span class="brackets">Schmidt et al., 2018</span></dt>
<dd><p>Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., &amp; Madry, A. (2018). Adversarially robust generalization requires more data. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id532"><span class="brackets">Schroff et al., 2015</span></dt>
<dd><p>Schroff, F., Kalenichenko, D., &amp; Philbin, J. (2015). Facenet: a unified embedding for face recognition and clustering. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 815–823).</p>
</dd>
<dt class="label" id="id136"><span class="brackets">Schuller et al., 2015</span></dt>
<dd><p>Schuller, B., Steidl, S., Batliner, A., Nöth, E., Vinciarelli, A., Burkhardt, F., … others. (2015). A survey on perceived speaker traits: personality, likability, pathology, and the first challenge. <em>Computer Speech &amp; Language</em>, <em>29</em>(1), 100–131.</p>
</dd>
<dt class="label" id="id533"><span class="brackets">Schultz &amp; Joachims, 2003</span></dt>
<dd><p>Schultz, M., &amp; Joachims, T. (2003). Learning a distance metric from relative comparisons. <em>Advances in Neural Information Processing Systems</em>, <em>16</em>.</p>
</dd>
<dt class="label" id="id591"><span class="brackets">Segal et al., 2009</span></dt>
<dd><p>Segal, A., Haehnel, D., &amp; Thrun, S. (2009). Generalized-icp. <em>Robotics: science and systems</em> (p. 435).</p>
</dd>
<dt class="label" id="id60"><span class="brackets">Shafahi et al., 2018</span></dt>
<dd><p>Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., &amp; Goldstein, T. (2018). Poison frogs! targeted clean-label poisoning attacks on neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id302"><span class="brackets">Shafahi et al., 2019</span></dt>
<dd><p>Shafahi, A., Najibi, M., Ghiasi, M. A., Xu, Z., Dickerson, J., Studer, C., … Goldstein, T. (2019). Adversarial training for free! <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id158"><span class="brackets">Shafi &amp; Silvio, 1982</span></dt>
<dd><p>Shafi, G., &amp; Silvio, M. (1982). Probabilistic encryption &amp; how to play mental poker keeping secret all partial information. <em>ACM Symposium on Theory of Computing</em> (pp. 365–377).</p>
</dd>
<dt class="label" id="id237"><span class="brackets">Shaham et al., 2015</span></dt>
<dd><p>Shaham, U., Yamada, Y., &amp; Negahban, S. (2015). Understanding adversarial training: increasing local stability of neural nets through robust optimization. <em>arXiv preprint arXiv:1511.05432</em>.</p>
</dd>
<dt class="label" id="id352"><span class="brackets">Shao et al., 2021</span></dt>
<dd><p>Shao, R., Shi, Z., Yi, J., Chen, P.-Y., &amp; Hsieh, C.-J. (2021). On the adversarial robustness of vision transformers. <em>arXiv preprint arXiv:2103.15670</em>.</p>
</dd>
<dt class="label" id="id394"><span class="brackets">Sharif et al., 2016</span></dt>
<dd><p>Sharif, M., Bhagavatula, S., Bauer, L., &amp; Reiter, M. K. (2016). Accessorize to a crime: real and stealthy attacks on state-of-the-art face recognition. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 1528–1540).</p>
</dd>
<dt class="label" id="id40"><span class="brackets">Sharir et al., 2020</span></dt>
<dd><p>Sharir, O., Peleg, B., &amp; Shoham, Y. (2020). The cost of training nlp models: a concise overview. <em>arXiv preprint arXiv:2004.08900</em>.</p>
</dd>
<dt class="label" id="id218"><span class="brackets">Shen et al., 2016</span></dt>
<dd><p>Shen, S., Tople, S., &amp; Saxena, P. (2016). Auror: defending against poisoning attacks in collaborative deep learning systems. <em>Conference on Computer Security Applications</em>.</p>
</dd>
<dt class="label" id="id421"><span class="brackets">Shen &amp; Sanghavi, 2019</span></dt>
<dd><p>Shen, Y., &amp; Sanghavi, S. (2019). Learning with bad training data via iterative trimmed loss minimization. <em>International Conference on Machine Learning</em> (pp. 5739–5748).</p>
</dd>
<dt class="label" id="id153"><span class="brackets">Shokri et al., 2017</span></dt>
<dd><p>Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. <em>IEEE Symposium on Security and Privacy</em> (pp. 3–18).</p>
</dd>
<dt class="label" id="id17"><span class="brackets">Siarohin et al., 2019</span></dt>
<dd><p>Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., &amp; Sebe, N. (2019). First order motion model for image animation. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id519"><span class="brackets">Silver et al., 2016</span></dt>
<dd><p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., … others. (2016). Mastering the game of go with deep neural networks and tree search. <em>Nature</em>, <em>529</em>(7587), 484–489.</p>
</dd>
<dt class="label" id="id518"><span class="brackets">Silver et al., 2017</span></dt>
<dd><p>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., … others. (2017). Mastering the game of go without human knowledge. <em>Nature</em>, <em>550</em>(7676), 354–359.</p>
</dd>
<dt class="label" id="id173"><span class="brackets">Simon-Gabriel et al., 2019</span></dt>
<dd><p>Simon-Gabriel, C.-J., Ollivier, Y., Bottou, L., Schölkopf, B., &amp; Lopez-Paz, D. (2019). First-order adversarial vulnerability of neural networks and input dimension. <em>International Conference on Machine Learning</em> (pp. 5809–5817).</p>
</dd>
<dt class="label" id="id521"><span class="brackets">Simonyan &amp; Zisserman, 2015</span></dt>
<dd><p>Simonyan, K., &amp; Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id450"><span class="brackets">Singh et al., 2019a</span></dt>
<dd><p>Singh, G., Ganvir, R., Püschel, M., &amp; Vechev, M. (2019). Beyond the single neuron convex barrier for neural network certification. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id458"><span class="brackets">Singh et al., 2019b</span></dt>
<dd><p>Singh, G., Gehr, T., Püschel, M., &amp; Vechev, M. (2019). An abstract domain for certifying neural networks. <em>ACM on Programming Languages</em>, <em>3</em>(POPL), 1–30.</p>
</dd>
<dt class="label" id="id307"><span class="brackets">Smith &amp; Topin, 2018</span></dt>
<dd><p><strong>missing journal in smith2018super</strong></p>
</dd>
<dt class="label" id="id129"><span class="brackets">Smith et al., 2017</span></dt>
<dd><p>Smith, V., Chiang, C.-K., Sanjabi, M., &amp; Talwalkar, A. S. (2017). Federated multi-task learning. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>.</p>
</dd>
<dt class="label" id="id154"><span class="brackets">Song &amp; Raghunathan, 2020</span></dt>
<dd><p>Song, C., &amp; Raghunathan, A. (2020). Information leakage in embedding models. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 377–390).</p>
</dd>
<dt class="label" id="id400"><span class="brackets">Song et al., 2017</span></dt>
<dd><p>Song, C., Ristenpart, T., &amp; Shmatikov, V. (2017). Machine learning models that remember too much. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 587–601).</p>
</dd>
<dt class="label" id="id550"><span class="brackets">Song et al., 2021a</span></dt>
<dd><p>Song, J., Meng, C., &amp; Ermon, S. (2021). Denoising diffusion implicit models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id474"><span class="brackets">Song et al., 2021b</span></dt>
<dd><p>Song, L., Wu, W., Fu, C., Qian, C., Loy, C. C., &amp; He, R. (2021). Everything's talkin': pareidolia face reenactment. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id211"><span class="brackets">Song &amp; Mittal, 2021</span></dt>
<dd><p>Song, L., &amp; Mittal, P. (2021). Systematic evaluation of privacy risks of machine learning models. <em>USENIX Security Symposium</em> (pp. 2615–2632).</p>
</dd>
<dt class="label" id="id290"><span class="brackets">Song et al., 2013</span></dt>
<dd><p>Song, S., Chaudhuri, K., &amp; Sarwate, A. D. (2013). Stochastic gradient descent with differentially private updates. <em>IEEE Global Conference on Signal and Information Processing</em> (pp. 245–248).</p>
</dd>
<dt class="label" id="id183"><span class="brackets">Srivastava et al., 2014</span></dt>
<dd><p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. <em>Journal of Machine Learning Research</em>, <em>15</em>(1), 1929–1958.</p>
</dd>
<dt class="label" id="id314"><span class="brackets">Su et al., 2017</span></dt>
<dd><p>Su, D., Cao, J., Li, N., Bertino, E., Lyu, M., &amp; Jin, H. (2017). Differentially private k-means clustering and a hybrid approach to private optimization. <em>ACM Transactions on Privacy and Security</em>, <em>20</em>(4), 1–33.</p>
</dd>
<dt class="label" id="id355"><span class="brackets">Su et al., 2018</span></dt>
<dd><p>Su, D., Zhang, H., Chen, H., Yi, J., Chen, P.-Y., &amp; Gao, Y. (2018). Is robustness the cost of accuracy?–a comprehensive study on the robustness of 18 deep image classification models. <em>European Conference on Computer Vision</em> (pp. 631–648).</p>
</dd>
<dt class="label" id="id477"><span class="brackets">Sun et al., 2022a</span></dt>
<dd><p>Sun, J., Wang, X., Zhang, Y., Li, X., Zhang, Q., Liu, Y., &amp; Wang, J. (2022). Fenerf: face editing in neural radiance fields. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 7672–7682).</p>
</dd>
<dt class="label" id="id588"><span class="brackets">Sun et al., 2022b</span></dt>
<dd><p>Sun, K., Yao, T., Chen, S., Ding, S., Li, J., &amp; Ji, R. (2022). Dual contrastive learning for general face forgery detection. <em>AAAI</em>.</p>
</dd>
<dt class="label" id="id562"><span class="brackets">Sun et al., 2022c</span></dt>
<dd><p>Sun, Y., Zhang, T., Ma, X., Zhou, P., Lou, J., Xu, Z., … Sun, L. (2022). Backdoor attacks on crowd counting. <em>ACM International Conference on Multimedia</em> (pp. 5351–5360).</p>
</dd>
<dt class="label" id="id229"><span class="brackets">Sun et al., 2019</span></dt>
<dd><p>Sun, Z., Kairouz, P., Suresh, A. T., &amp; McMahan, H. B. (2019). Can you really backdoor federated learning? <em>arXiv preprint arXiv:1911.07963</em>.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">Suwajanakorn et al., 2017</span></dt>
<dd><p>Suwajanakorn, S., Seitz, S. M., &amp; Kemelmacher-Shlizerman, I. (2017). Synthesizing obama: learning lip sync from audio. <em>ACM Transactions on Graphics</em>, <em>36</em>(4), 1–13.</p>
</dd>
<dt class="label" id="id522"><span class="brackets">Szegedy et al., 2016</span></dt>
<dd><p>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2818–2826).</p>
</dd>
<dt class="label" id="id97"><span class="brackets">Szegedy et al., 2014</span></dt>
<dd><p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., &amp; Fergus, R. (2014). Intriguing properties of neural networks. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">Szyller et al., 2021</span></dt>
<dd><p>Szyller, S., Atli, B. G., Marchal, S., &amp; Asokan, N. (2021). Dawn: dynamic adversarial watermarking of neural networks. <em>ACM International Conference on Multimedia</em> (pp. 4417–4425).</p>
</dd>
<dt class="label" id="id112"><span class="brackets">TDinh et al., 2020</span></dt>
<dd><p>T Dinh, C., Tran, N., &amp; Nguyen, J. (2020). Personalized federated learning with moreau envelopes. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 21394–21405.</p>
</dd>
<dt class="label" id="id316"><span class="brackets">Tan &amp; Le, 2019</span></dt>
<dd><p>Tan, M., &amp; Le, Q. (2019). Efficientnet: rethinking model scaling for convolutional neural networks. <em>International Conference on Machine Learning</em> (pp. 6105–6114).</p>
</dd>
<dt class="label" id="id107"><span class="brackets">Tanay &amp; Griffin, 2016</span></dt>
<dd><p>Tanay, T., &amp; Griffin, L. (2016). A boundary tilting perspective on the phenomenon of adversarial examples. <em>arXiv preprint arXiv:1608.07690</em>.</p>
</dd>
<dt class="label" id="id59"><span class="brackets">Tang et al., 2020</span></dt>
<dd><p>Tang, R., Du, M., Liu, N., Yang, F., &amp; Hu, X. (2020). An embarrassingly simple approach for trojan attack in deep neural networks. <em>ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (pp. 218–228).</p>
</dd>
<dt class="label" id="id354"><span class="brackets">Tang et al., 2021</span></dt>
<dd><p>Tang, S., Gong, R., Wang, Y., Liu, A., Wang, J., Chen, X., … others. (2021). Robustart: benchmarking robustness on architecture design and training techniques. <em>arXiv preprint arXiv:2109.05211</em>.</p>
</dd>
<dt class="label" id="id456"><span class="brackets">Teng et al., 2020</span></dt>
<dd><p>Teng, J., Lee, G.-H., &amp; Yuan, Y. (2020). <em>\$\ell_1\$ Adversarial Robustness Certificates: a Randomized Smoothing Approach</em>.</p>
</dd>
<dt class="label" id="id190"><span class="brackets">Tian et al., 2018</span></dt>
<dd><p>Tian, S., Yang, G., &amp; Cai, Y. (2018). Detecting adversarial examples through image transformation. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id510"><span class="brackets">Tian et al., 2020</span></dt>
<dd><p>Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., &amp; Isola, P. (2020). What makes for good views for contrastive learning? <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 6827–6839.</p>
</dd>
<dt class="label" id="id479"><span class="brackets">Tian et al., 2021</span></dt>
<dd><p>Tian, Y., Ren, J., Chai, M., Olszewski, K., Peng, X., Metaxas, D. N., &amp; Tulyakov, S. (2021). A good image generator is what you need for high-resolution video synthesis. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id543"><span class="brackets">Tieleman et al., 2012</span></dt>
<dd><p>Tieleman, T., Hinton, G., &amp; others. (2012). Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude. <em>COURSERA: Neural networks for machine learning</em>, <em>4</em>(2), 26–31.</p>
</dd>
<dt class="label" id="id193"><span class="brackets">Tramer et al., 2020</span></dt>
<dd><p>Tramer, F., Carlini, N., Brendel, W., &amp; Madry, A. (2020). On adaptive attacks to adversarial example defenses. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 1633–1645.</p>
</dd>
<dt class="label" id="id262"><span class="brackets">Tramer et al., 2018</span></dt>
<dd><p>Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., &amp; McDaniel, P. (2018). Ensemble adversarial training: attacks and defenses. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id100"><span class="brackets">Tramer et al., 2016</span></dt>
<dd><p>Tramèr, F., Zhang, F., Juels, A., Reiter, M. K., &amp; Ristenpart, T. (2016). Stealing machine learning models via prediction $\$APIs$\$. <em>USENIX Security Symposium</em> (pp. 601–618).</p>
</dd>
<dt class="label" id="id364"><span class="brackets">Tran et al., 2018</span></dt>
<dd><p>Tran, B., Li, J., &amp; Madry, A. (2018). Spectral signatures in backdoor attacks. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id566"><span class="brackets">Trinh et al., 2021</span></dt>
<dd><p>Trinh, L., Tsang, M., Rambhatla, S., &amp; Liu, Y. (2021). Interpretable and trustworthy deepfake detection via dynamic prototypes. <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em>.</p>
</dd>
<dt class="label" id="id375"><span class="brackets">Truex et al., 2019</span></dt>
<dd><p>Truex, S., Liu, L., Gursoy, M. E., Yu, L., &amp; Wei, W. (2019). Demystifying membership inference attacks in machine learning as a service. <em>IEEE Transactions on Services Computing</em>.</p>
</dd>
<dt class="label" id="id463"><span class="brackets">Tsuzuku et al., 2018</span></dt>
<dd><p>Tsuzuku, Y., Sato, I., &amp; Sugiyama, M. (2018). Lipschitz-margin training: scalable certification of perturbation invariance for deep neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id83"><span class="brackets">Tu et al., 2019</span></dt>
<dd><p>Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi, J., … Cheng, S.-M. (2019). Autozoom: autoencoder-based zeroth order optimization method for attacking black-box neural networks. <em>AAAI Conference on Artificial Intelligence</em> (pp. 742–749).</p>
</dd>
<dt class="label" id="id61"><span class="brackets">Turner et al., 2018</span></dt>
<dd><p><strong>missing journal in turner2018clean</strong></p>
</dd>
<dt class="label" id="id48"><span class="brackets">Uchida et al., 2017</span></dt>
<dd><p>Uchida, Y., Nagai, Y., Sakazawa, S., &amp; Satoh, Shin'ichi. (2017). Embedding watermarks into deep neural networks. <em>ACM on International Conference on Multimedia Retrieval</em> (pp. 269–277).</p>
</dd>
<dt class="label" id="id288"><span class="brackets">Maaten &amp; Hinton, 2008</span></dt>
<dd><p>Van der Maaten, L., &amp; Hinton, G. (2008). Visualizing data using t-sne. <em>Journal of Machine Learning Research</em>, <em>9</em>(11).</p>
</dd>
<dt class="label" id="id130"><span class="brackets">Vanhaesebrouck et al., 2017</span></dt>
<dd><p>Vanhaesebrouck, P., Bellet, A., &amp; Tommasi, M. (2017). Decentralized collaborative learning of personalized models over networks. <em>Artificial Intelligence and Statistics</em> (pp. 509–517).</p>
</dd>
<dt class="label" id="id515"><span class="brackets">Vaswani et al., 2017</span></dt>
<dd><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>.</p>
</dd>
<dt class="label" id="id239"><span class="brackets">Wald, 1939</span></dt>
<dd><p>Wald, A. (1939). Contributions to the theory of statistical estimation and testing hypotheses. <em>The Annals of Mathematical Statistics</em>, <em>10</em>(4), 299–326.</p>
</dd>
<dt class="label" id="id240"><span class="brackets">Wald, 1945</span></dt>
<dd><p>Wald, A. (1945). Statistical decision functions which minimize the maximum risk. <em>Annals of Mathematics</em>, pp. 265–280.</p>
</dd>
<dt class="label" id="id241"><span class="brackets">Wald, 1992</span></dt>
<dd><p>Wald, A. (1992). Statistical decision functions. <em>Breakthroughs in Statistics</em> (pp. 342–357). Springer.</p>
</dd>
<dt class="label" id="id101"><span class="brackets">Wang &amp; Gong, 2018</span></dt>
<dd><p>Wang, B., &amp; Gong, N. Z. (2018). Stealing hyperparameters in machine learning. <em>IEEE Symposium on Security and Privacy</em> (pp. 36–52).</p>
</dd>
<dt class="label" id="id365"><span class="brackets">Wang et al., 2019a</span></dt>
<dd><p>Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., &amp; Zhao, B. Y. (2019). Neural cleanse: identifying and mitigating backdoor attacks in neural networks. <em>IEEE Symposium on Security and Privacy</em> (pp. 707–723).</p>
</dd>
<dt class="label" id="id584"><span class="brackets">Wang &amp; Deng, 2021</span></dt>
<dd><p>Wang, C., &amp; Deng, W. (2021). Representative forgery mining for fake face detection. <em>IEEE/CVF Computer Vision and Pattern Recognition Conference</em>.</p>
</dd>
<dt class="label" id="id292"><span class="brackets">Wang et al., 2017</span></dt>
<dd><p>Wang, D., Ye, M., &amp; Xu, J. (2017). Differentially private empirical risk minimization revisited: faster and more general. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>.</p>
</dd>
<dt class="label" id="id536"><span class="brackets">Wang et al., 2018a</span></dt>
<dd><p>Wang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., … Liu, W. (2018). Cosface: large margin cosine loss for deep face recognition. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 5265–5274).</p>
</dd>
<dt class="label" id="id326"><span class="brackets">Wang et al., 2020a</span></dt>
<dd><p>Wang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., Agarwal, S., Sohn, J.-y., … Papailiopoulos, D. (2020). Attack of the tails: yes, you really can backdoor federated learning. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 16070–16084.</p>
</dd>
<dt class="label" id="id410"><span class="brackets">Wang et al., 2020b</span></dt>
<dd><p>Wang, R., Zhang, G., Liu, S., Chen, P.-Y., Xiong, J., &amp; Wang, M. (2020). Practical detection of trojan neural networks: data-limited and data-free cases. <em>European Conference on Computer Vision</em> (pp. 222–238).</p>
</dd>
<dt class="label" id="id447"><span class="brackets">Wang et al., 2021a</span></dt>
<dd><p>Wang, S., Zhang, H., Xu, K., Lin, X., Jana, S., Hsieh, C.-J., &amp; Kolter, J. Z. (2021). Beta-crown: efficient bound propagation with per-neuron split constraints for neural network robustness verification. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 29909–29921.</p>
</dd>
<dt class="label" id="id191"><span class="brackets">Wang et al., 2022</span></dt>
<dd><p>Wang, S., Nepal, S., Abuadbba, A., Rudolph, C., &amp; Grobler, M. (2022). Adversarial detection by latent style transformations. <em>IEEE Transactions on Information Forensics and Security</em>, <em>17</em>, 1099–1114.</p>
</dd>
<dt class="label" id="id252"><span class="brackets">Wang et al., 2020c</span></dt>
<dd><p>Wang, S., Nepal, S., Rudolph, C., Grobler, M., Chen, S., &amp; Chen, T. (2020). Backdoor attacks against transfer learning with pre-trained deep learning models. <em>IEEE Transactions on Services Computing</em>.</p>
</dd>
<dt class="label" id="id556"><span class="brackets">Wang et al., 2018b</span></dt>
<dd><p>Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., Tao, A., Kautz, J., &amp; Catanzaro, B. (2018). High-resolution image synthesis and semantic manipulation with conditional gans. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 8798–8807).</p>
</dd>
<dt class="label" id="id21"><span class="brackets">Wang et al., 2014</span></dt>
<dd><p>Wang, W., Dong, J., &amp; Tan, T. (2014). Exploring dct coefficient quantization effects for local tampering detection. <em>IEEE Transactions on Information Forensics and Security</em>, <em>9</em>(10), 1653–1666.</p>
</dd>
<dt class="label" id="id261"><span class="brackets">Wang et al., 2019b</span></dt>
<dd><p>Wang, Y., Ma, X., Bailey, J., Yi, J., Zhou, B., &amp; Gu, Q. (2019). On the convergence and robustness of adversarial training. <em>International Conference on Machine Learning</em> (pp. 6586–6595).</p>
</dd>
<dt class="label" id="id278"><span class="brackets">Wang et al., 2019c</span></dt>
<dd><p>Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., &amp; Gu, Q. (2019). Improving adversarial robustness requires revisiting misclassified examples. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">Wang et al., 2021b</span></dt>
<dd><p>Wang, Z., Liu, C., &amp; Cui, X. (2021). Evilmodel: hiding malware inside of neural network models. <em>IEEE Symposium on Computers and Communications</em> (pp. 1–7).</p>
</dd>
<dt class="label" id="id531"><span class="brackets">Wen et al., 2016</span></dt>
<dd><p>Wen, Y., Zhang, K., Li, Z., &amp; Qiao, Y. (2016). A discriminative feature learning approach for deep face recognition. <em>European Conference on Computer Vision</em> (pp. 499–515).</p>
</dd>
<dt class="label" id="id424"><span class="brackets">Weng et al., 2018</span></dt>
<dd><p>Weng, L., Zhang, H., Chen, H., Song, Z., Hsieh, C.-J., Daniel, L., … Dhillon, I. (2018). Towards fast computation of certified robustness for relu networks. <em>International Conference on Machine Learning</em> (pp. 5276–5285).</p>
</dd>
<dt class="label" id="id157"><span class="brackets">Wenger et al., 2021</span></dt>
<dd><p>Wenger, E., Passananti, J., Bhagoji, A. N., Yao, Y., Zheng, H., &amp; Zhao, B. Y. (2021). Backdoor attacks against deep learning systems in the physical world. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 6206–6215).</p>
</dd>
<dt class="label" id="id85"><span class="brackets">Wierstra et al., 2014</span></dt>
<dd><p>Wierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., &amp; Schmidhuber, J. (2014). Natural evolution strategies. <em>Journal of Machine Learning Research</em>, <em>15</em>(1), 949–980.</p>
</dd>
<dt class="label" id="id170"><span class="brackets">Wold et al., 1987</span></dt>
<dd><p>Wold, S., Esbensen, K., &amp; Geladi, P. (1987). Principal component analysis. <em>Chemometrics and Intelligent Laboratory Systems</em>, <em>2</em>(1-3), 37–52.</p>
</dd>
<dt class="label" id="id243"><span class="brackets">Wong et al., 2020</span></dt>
<dd><p>Wong, E., Rice, L., &amp; Kolter, J. Z. (2020). Fast is better than free: revisiting adversarial training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id346"><span class="brackets">Wu et al., 2021</span></dt>
<dd><p>Wu, B., Chen, J., Cai, D., He, X., &amp; Gu, Q. (2021). Do wider neural networks really help adversarial robustness? <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 7054–7067.</p>
</dd>
<dt class="label" id="id230"><span class="brackets">Wu et al., 2020a</span></dt>
<dd><p>Wu, C., Yang, X., Zhu, S., &amp; Mitra, P. (2020). Mitigating backdoor attacks in federated learning. <em>arXiv preprint arXiv:2011.01767</em>.</p>
</dd>
<dt class="label" id="id405"><span class="brackets">Wu &amp; Wang, 2021</span></dt>
<dd><p>Wu, D., &amp; Wang, Y. (2021). Adversarial neuron pruning purifies backdoored deep models. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 16913–16925.</p>
</dd>
<dt class="label" id="id93"><span class="brackets">Wu et al., 2020b</span></dt>
<dd><p>Wu, D., Wang, Y., Xia, S.-T., Bailey, J., &amp; Ma, X. (2020). Skip connections matter: on the transferability of adversarial examples generated with resnets. <em>arXiv preprint arXiv:2002.05990</em>.</p>
</dd>
<dt class="label" id="id296"><span class="brackets">Wu et al., 2020c</span></dt>
<dd><p>Wu, D., Xia, S.-T., &amp; Wang, Y. (2020). Adversarial weight perturbation helps robust generalization. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 2958–2969.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">Wu et al., 2019</span></dt>
<dd><p>Wu, Y., AbdAlmageed, W., &amp; Natarajan, P. (2019). Mantra-net: manipulation tracing network for detection and localization of image forgeries with anomalous features. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 9543–9552).</p>
</dd>
<dt class="label" id="id406"><span class="brackets">Wu et al., 2020d</span></dt>
<dd><p>Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., &amp; Philip, S. Y. (2020). A comprehensive survey on graph neural networks. <em>IEEE Transactions on Neural Networks and Learning Systems</em>, <em>32</em>(1), 4–24.</p>
</dd>
<dt class="label" id="id339"><span class="brackets">Xi et al., 2021</span></dt>
<dd><p>Xi, Z., Pang, R., Ji, S., &amp; Wang, T. (2021). Graph backdoor. <em>USENIX Security Symposium</em> (pp. 1523–1540).</p>
</dd>
<dt class="label" id="id552"><span class="brackets">Xia et al., 2021</span></dt>
<dd><p>Xia, W., Yang, Y., Xue, J.-H., &amp; Wu, B. (2021). Tedigan: text-guided diverse face image generation and manipulation. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 2256–2265).</p>
</dd>
<dt class="label" id="id78"><span class="brackets">Xiao et al., 2018a</span></dt>
<dd><p>Xiao, C., Li, B., Zhu, J. Y., He, W., Liu, M., &amp; Song, D. (2018). Generating adversarial examples with adversarial networks. <em>International Joint Conference on Artificial Intelligence</em> (pp. 3905–3911).</p>
</dd>
<dt class="label" id="id444"><span class="brackets">Xiao et al., 2018b</span></dt>
<dd><p>Xiao, K. Y., Tjeng, V., Shafiullah, N. M., &amp; Madry, A. (2018). Training for faster adversarial robustness verification via inducing relu stability. <em>arXiv preprint arXiv:1809.03008</em>.</p>
</dd>
<dt class="label" id="id231"><span class="brackets">Xie et al., 2021</span></dt>
<dd><p>Xie, C., Chen, M., Chen, P.-Y., &amp; Li, B. (2021). Crfl: certifiably robust federated learning against backdoor attacks. <em>International Conference on Machine Learning</em> (pp. 11372–11382).</p>
</dd>
<dt class="label" id="id253"><span class="brackets">Xie et al., 2019a</span></dt>
<dd><p>Xie, C., Huang, K., Chen, P.-Y., &amp; Li, B. (2019). Dba: distributed backdoor attacks against federated learning. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id260"><span class="brackets">Xie et al., 2020</span></dt>
<dd><p>Xie, C., Tan, M., Gong, B., Yuille, A., &amp; Le, Q. V. (2020). Smooth adversarial training. <em>arXiv preprint arXiv:2006.14536</em>.</p>
</dd>
<dt class="label" id="id387"><span class="brackets">Xie et al., 2018</span></dt>
<dd><p>Xie, C., Wang, J., Zhang, Z., Ren, Z., &amp; Yuille, A. (2018). Mitigating adversarial effects through randomization. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id259"><span class="brackets">Xie et al., 2019b</span></dt>
<dd><p>Xie, C., Wu, Y., Maaten, L. v. d., Yuille, A. L., &amp; He, K. (2019). Feature denoising for improving adversarial robustness. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 501–509).</p>
</dd>
<dt class="label" id="id90"><span class="brackets">Xie et al., 2019c</span></dt>
<dd><p>Xie, C., Zhang, Z., Zhou, Y., Bai, S., Wang, J., Ren, Z., &amp; Yuille, A. L. (2019). Improving transferability of adversarial examples with input diversity. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 2730–2739).</p>
</dd>
<dt class="label" id="id340"><span class="brackets">Xu et al., 2021</span></dt>
<dd><p>Xu, J., Xue, M., &amp; Picek, S. (2021). Explainability-based backdoor attacks against graph neural networks. <em>ACM Workshop on Wireless Security and Machine Learning</em> (pp. 31–36).</p>
</dd>
<dt class="label" id="id393"><span class="brackets">Xu et al., 2020</span></dt>
<dd><p>Xu, K., Zhang, G., Liu, S., Fan, Q., Sun, M., Chen, H., … Lin, X. (2020). Adversarial t-shirt! evading person detectors in a physical world. <em>European Conference on Computer Vision</em> (pp. 665–681).</p>
</dd>
<dt class="label" id="id198"><span class="brackets">Xu et al., 2018</span></dt>
<dd><p>Xu, W., Evans, D., &amp; Qi, Y. (2018). Feature squeezing: detecting adversarial examples in deep neural networks. <em>Network and Distributed Systems Security Symposium</em>.</p>
</dd>
<dt class="label" id="id478"><span class="brackets">Xu et al., 2022</span></dt>
<dd><p>Xu, Y., Yin, Y., Jiang, L., Wu, Q., Zheng, C., Loy, C. C., … Wu, W. (2022). Transeditor: transformer-based dual-space gan for highly controllable facial editing. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 7683–7692).</p>
</dd>
<dt class="label" id="id399"><span class="brackets">Yang et al., 2017</span></dt>
<dd><p>Yang, C., Wu, Q., Li, H., &amp; Chen, Y. (2017). Generative poisoning attack method against neural networks. <em>arXiv preprint arXiv:1703.01340</em>.</p>
</dd>
<dt class="label" id="id570"><span class="brackets">Yang et al., 2020a</span></dt>
<dd><p>Yang, C.-Z., Ma, J., Wang, S., &amp; Liew, A. W.-C. (2020). Preventing deepfake attacks on speaker authentication by dynamic lip movement analysis. <em>TIFS</em>.</p>
</dd>
<dt class="label" id="id461"><span class="brackets">Yang et al., 2020b</span></dt>
<dd><p>Yang, G., Duan, T., Hu, J. E., Salman, H., Razenshteyn, I., &amp; Li, J. (2020). Randomized smoothing of all shapes and sizes. <em>International Conference on Machine Learning</em> (pp. 10693–10705).</p>
</dd>
<dt class="label" id="id265"><span class="brackets">Yang et al., 2020c</span></dt>
<dd><p>Yang, H., Zhang, J., Dong, H., Inkawhich, N., Gardner, A., Touchet, A., … Li, H. (2020). Dverge: diversifying vulnerabilities for enhanced robust generation of ensembles. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 5505–5515.</p>
</dd>
<dt class="label" id="id70"><span class="brackets">Yang et al., 2019a</span></dt>
<dd><p>Yang, Q., Liu, Y., Chen, T., &amp; Tong, Y. (2019). Federated machine learning: concept and applications. <em>ACM Transactions on Intelligent Systems and Technology</em>, <em>10</em>(2), 1–19.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">Yang et al., 2019b</span></dt>
<dd><p>Yang, S., Ren, B., Zhou, X., &amp; Liu, L. (2019). Parallel distributed logistic regression for vertical federated learning without third-party coordinator. <em>arXiv preprint arXiv:1911.09824</em>.</p>
</dd>
<dt class="label" id="id571"><span class="brackets">Yang et al., 2019c</span></dt>
<dd><p>Yang, X., Li, Y., &amp; Lyu, S. (2019). Exposing deep fakes using inconsistent head poses. <em>ICASSP</em>.</p>
</dd>
<dt class="label" id="id418"><span class="brackets">Yang et al., 2022</span></dt>
<dd><p>Yang, Y., Liu, T. Y., &amp; Mirzasoleiman, B. (2022). Not all poisons are created equal: robust training against data poisoning. <em>International Conference on Machine Learning</em> (pp. 25154–25165).</p>
</dd>
<dt class="label" id="id186"><span class="brackets">Yao, 1982</span></dt>
<dd><p>Yao, A. C. (1982). Protocols for secure computations. <em>IEEE Annual Symposium on Foundations of Computer Science</em> (pp. 160–164).</p>
</dd>
<dt class="label" id="id251"><span class="brackets">Yao et al., 2019</span></dt>
<dd><p>Yao, Y., Li, H., Zheng, H., &amp; Zhao, B. Y. (2019). Latent backdoor attacks on deep neural networks. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 2041–2055).</p>
</dd>
<dt class="label" id="id337"><span class="brackets">Ye et al., 2022</span></dt>
<dd><p>Ye, J., Liu, X., You, Z., Li, G., &amp; Liu, B. (2022). Drinet: dynamic backdoor attack against automatic speech recognization models. <em>Applied Sciences</em>, <em>12</em>(12), 5786.</p>
</dd>
<dt class="label" id="id210"><span class="brackets">Yeom et al., 2018</span></dt>
<dd><p>Yeom, S., Giacomelli, I., Fredrikson, M., &amp; Jha, S. (2018). Privacy risk in machine learning: analyzing the connection to overfitting. <em>IEEE Computer Security Foundations Symposium</em> (pp. 268–282).</p>
</dd>
<dt class="label" id="id217"><span class="brackets">Yin et al., 2018</span></dt>
<dd><p>Yin, D., Chen, Y., Kannan, R., &amp; Bartlett, P. (2018). Byzantine-robust distributed learning: towards optimal statistical rates. <em>International Conference on Machine Learning</em> (pp. 5650–5659).</p>
</dd>
<dt class="label" id="id415"><span class="brackets">Yin et al., 2021</span></dt>
<dd><p>Yin, H., Mallya, A., Vahdat, A., Alvarez, J. M., Kautz, J., &amp; Molchanov, P. (2021). See through gradients: image batch recovery via gradinversion. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 16337–16346).</p>
</dd>
<dt class="label" id="id312"><span class="brackets">Yu et al., 2020</span></dt>
<dd><p>Yu, H., Yang, K., Zhang, T., Tsai, Y.-Y., Ho, T.-Y., &amp; Jin, Y. (2020). Cloudleak: large-scale deep learning models stealing through adversarial examples. <em>Network and Distributed System Security Symposium</em>.</p>
</dd>
<dt class="label" id="id528"><span class="brackets">Yu et al., 2016</span></dt>
<dd><p>Yu, J., Jiang, Y., Wang, Z., Cao, Z., &amp; Huang, T. (2016). Unitbox: an advanced object detection network. <em>ACM International Conference on Multimedia</em>.</p>
</dd>
<dt class="label" id="id480"><span class="brackets">Yu et al., 2022</span></dt>
<dd><p>Yu, S., Tack, J., Mo, S., Kim, H., Kim, J., Ha, J.-W., &amp; Shin, J. (2022). Generating videos with dynamics-aware implicit generative adversarial networks. <em>arXiv preprint arXiv:2202.10571</em>.</p>
</dd>
<dt class="label" id="id103"><span class="brackets">Yuan et al., 2022</span></dt>
<dd><p>Yuan, X., Ding, L., Zhang, L., Li, X., &amp; Wu, D. O. (2022). Es attack: model stealing against deep neural networks without data hurdles. <em>IEEE Transactions on Emerging Topics in Computational Intelligence</em>.</p>
</dd>
<dt class="label" id="id285"><span class="brackets">Yun et al., 2019</span></dt>
<dd><p>Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., &amp; Yoo, Y. (2019). Cutmix: regularization strategy to train strong classifiers with localizable features. <em>International Conference on Computer Vision</em> (pp. 6023–6032).</p>
</dd>
<dt class="label" id="id542"><span class="brackets">Zeiler, 2012</span></dt>
<dd><p>Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. <em>arXiv preprint arXiv:1212.5701</em>.</p>
</dd>
<dt class="label" id="id336"><span class="brackets">Zhai et al., 2021</span></dt>
<dd><p>Zhai, T., Li, Y., Zhang, Z., Wu, B., Jiang, Y., &amp; Xia, S.-T. (2021). Backdoor attack against speaker verification. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em> (pp. 2560–2564).</p>
</dd>
<dt class="label" id="id455"><span class="brackets">Zhang et al., 2021a</span></dt>
<dd><p>Zhang, B., Lu, Z., Cai, T., He, D., &amp; Wang, L. (2021). <em>Towards certifying \$\ell_\infty\$ robustness using Neural networks with \$\ell_\infty\$-dist Neurons</em>.</p>
</dd>
<dt class="label" id="id306"><span class="brackets">Zhang et al., 2019a</span></dt>
<dd><p>Zhang, D., Zhang, T., Lu, Y., Zhu, Z., &amp; Dong, B. (2019). You only propagate once: accelerating adversarial training via maximal principle. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id233"><span class="brackets">Zhang et al., 2019b</span></dt>
<dd><p>Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., &amp; Jordan, M. (2019). Theoretically principled trade-off between robustness and accuracy. <em>International Conference on Machine Learning</em> (pp. 7472–7482).</p>
</dd>
<dt class="label" id="id195"><span class="brackets">Zhang et al., 2018a</span></dt>
<dd><p>Zhang, H., Cisse, M., Dauphin, Y. N., &amp; Lopez-Paz, D. (2018). Mixup: beyond empirical risk minimization. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">Zhang et al., 2018b</span></dt>
<dd><p>Zhang, J., Gu, Z., Jang, J., Wu, H., Stoecklin, M. P., Huang, H., &amp; Molloy, I. (2018). Protecting intellectual property of deep neural networks with watermarking. <em>ACM Asia Conference on Computer and Communications Security</em> (pp. 159–172).</p>
</dd>
<dt class="label" id="id294"><span class="brackets">Zhang et al., 2017</span></dt>
<dd><p>Zhang, J., Zheng, K., Mou, W., &amp; Wang, L. (2017). Efficient private erm for smooth objectives. <em>arXiv preprint arXiv:1703.09947</em>.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">Zhang et al., 2020a</span></dt>
<dd><p>Zhang, J., Chen, D., Liao, J., Fang, H., Zhang, W., Zhou, W., … Yu, N. (2020). Model watermarking for image processing networks. <em>AAAI Conference on Artificial Intelligence</em> (pp. 12805–12812).</p>
</dd>
<dt class="label" id="id56"><span class="brackets">Zhang et al., 2021b</span></dt>
<dd><p>Zhang, J., Chen, D., Liao, J., Zhang, W., Feng, H., Hua, G., &amp; Yu, N. (2021). Deep model intellectual property protection via deep watermarking. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</p>
</dd>
<dt class="label" id="id263"><span class="brackets">Zhang et al., 2020b</span></dt>
<dd><p>Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M., &amp; Kankanhalli, M. (2020). Attacks which do not kill training make adversarial learning stronger. <em>International Conference on Machine Learning</em> (pp. 11278–11287).</p>
</dd>
<dt class="label" id="id279"><span class="brackets">Zhang et al., 2020c</span></dt>
<dd><p>Zhang, J., Zhu, J., Niu, G., Han, B., Sugiyama, M., &amp; Kankanhalli, M. (2020). Geometry-aware instance-reweighted adversarial training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id367"><span class="brackets">Zhang et al., 2012</span></dt>
<dd><p>Zhang, J., Zhang, Z., Xiao, X., Yang, Y., &amp; Winslett, M. (2012). Functional mechanism: regression analysis under differential privacy. <em>arXiv preprint arXiv:1208.0219</em>.</p>
</dd>
<dt class="label" id="id413"><span class="brackets">Zhang et al., 2022</span></dt>
<dd><p>Zhang, R., Guo, S., Wang, J., Xie, X., &amp; Tao, D. (2022). A survey on gradient inversion: attacks, defenses and future directions. <em>International Joint Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">Zhang &amp; Zhu, 2017</span></dt>
<dd><p>Zhang, R., &amp; Zhu, Q. (2017). A game-theoretic analysis of label flipping attacks on distributed support vector machines. <em>Conference on Information Sciences and Systems</em> (pp. 1–6).</p>
</dd>
<dt class="label" id="id315"><span class="brackets">Zhang et al., 2018c</span></dt>
<dd><p>Zhang, X., Ji, S., &amp; Wang, T. (2018). Differentially private releasing via deep generative model (technical report). <em>arXiv preprint arXiv:1801.01594</em>.</p>
</dd>
<dt class="label" id="id338"><span class="brackets">Zhang et al., 2021c</span></dt>
<dd><p>Zhang, Z., Jia, J., Wang, B., &amp; Gong, N. Z. (2021). Backdoor attacks to graph neural networks. <em>ACM Symposium on Access Control Models and Technologies</em> (pp. 15–26).</p>
</dd>
<dt class="label" id="id417"><span class="brackets">Zhang &amp; Sabuncu, 2018</span></dt>
<dd><p>Zhang, Z., &amp; Sabuncu, M. (2018). Generalized cross entropy loss for training deep neural networks with noisy labels. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p>
</dd>
<dt class="label" id="id414"><span class="brackets">Zhao et al., 2020a</span></dt>
<dd><p>Zhao, B., Mopuri, K. R., &amp; Bilen, H. (2020). Idlg: improved deep leakage from gradients. <em>arXiv preprint arXiv:2001.02610</em>.</p>
</dd>
<dt class="label" id="id585"><span class="brackets">Zhao et al., 2021a</span></dt>
<dd><p>Zhao, H., Zhou, W., Chen, D., Wei, T., Zhang, W., &amp; Yu, N. (2021). Multi-attentional deepfake detection. <em>IEEE/CVF Computer Vision and Pattern Recognition Conference</em>.</p>
</dd>
<dt class="label" id="id404"><span class="brackets">Zhao et al., 2020b</span></dt>
<dd><p><strong>missing booktitle in zhao2020bridging</strong></p>
</dd>
<dt class="label" id="id330"><span class="brackets">Zhao et al., 2020c</span></dt>
<dd><p>Zhao, S., Ma, X., Zheng, X., Bailey, J., Chen, J., &amp; Jiang, Y.-G. (2020). Clean-label backdoor attacks on video recognition models. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 14443–14452).</p>
</dd>
<dt class="label" id="id580"><span class="brackets">Zhao et al., 2021b</span></dt>
<dd><p>Zhao, T., Xu, X., Xu, M., Ding, H., Xiong, Y., &amp; Xia, W. (2021). Learning self-consistency for deepfake detection. <em>International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id309"><span class="brackets">Zheng et al., 2020a</span></dt>
<dd><p>Zheng, H., Zhang, Z., Gu, J., Lee, H., &amp; Prakash, A. (2020). Efficient adversarial training with transferable adversarial examples. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 1181–1190).</p>
</dd>
<dt class="label" id="id565"><span class="brackets">Zheng et al., 2021</span></dt>
<dd><p>Zheng, Y., Bao, J., Chen, D., Zeng, M., &amp; Wen, F. (2021). Exploring temporal coherence for more general video face forgery detection. <em>International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id529"><span class="brackets">Zheng et al., 2020b</span></dt>
<dd><p>Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., &amp; Ren, D. (2020). Distance-iou loss: faster and better learning for bounding box regression. <em>AAAI Conference on Artificial Intelligence</em> (pp. 12993–13000).</p>
</dd>
<dt class="label" id="id395"><span class="brackets">Zhou et al., 2016</span></dt>
<dd><p>Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp; Torralba, A. (2016). Learning deep features for discriminative localization. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 2921–2929).</p>
</dd>
<dt class="label" id="id32"><span class="brackets">Zhou et al., 2017</span></dt>
<dd><p>Zhou, P., Han, X., Morariu, V. I., &amp; Davis, L. S. (2017). Two-stream neural networks for tampered face detection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em> (pp. 1831–1839).</p>
</dd>
<dt class="label" id="id26"><span class="brackets">Zhou et al., 2018a</span></dt>
<dd><p>Zhou, P., Han, X., Morariu, V. I., &amp; Davis, L. S. (2018). Learning rich features for image manipulation detection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 1053–1061).</p>
</dd>
<dt class="label" id="id20"><span class="brackets">Zhou et al., 2018b</span></dt>
<dd><p>Zhou, Y., Song, Y., &amp; Berg, T. L. (2018). Image2gif: generating cinemagraphs using recurrent deep q-networks. <em>IEEE Winter Conference on Applications of Computer Vision</em> (pp. 170–178).</p>
</dd>
<dt class="label" id="id65"><span class="brackets">Zhu et al., 2019a</span></dt>
<dd><p>Zhu, C., Huang, W. R., Li, H., Taylor, G., Studer, C., &amp; Goldstein, T. (2019). Transferable clean-label poisoning attacks on deep neural nets. <em>International Conference on Machine Learning</em> (pp. 7614–7623).</p>
</dd>
<dt class="label" id="id325"><span class="brackets">Zhu et al., 2021</span></dt>
<dd><p>Zhu, J., Yao, J., Han, B., Zhang, J., Liu, T., Niu, G., … Yang, H. (2021). Reliable adversarial distillation with unreliable teachers. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id468"><span class="brackets">Zhu &amp; Blaschko, 2021</span></dt>
<dd><p>Zhu, J., &amp; Blaschko, M. B. (2021). R-gap: recursive gradient attack on privacy. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id412"><span class="brackets">Zhu et al., 2019b</span></dt>
<dd><p>Zhu, L., Liu, Z., &amp; Han, S. (2019). Deep leakage from gradients. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id324"><span class="brackets">Zi et al., 2021</span></dt>
<dd><p>Zi, B., Zhao, S., Ma, X., &amp; Jiang, Y.-G. (2021). Revisiting adversarial robustness distillation: robust soft labels make student better. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 16443–16452).</p>
</dd>
<dt class="label" id="id448"><span class="brackets">Zombori et al., 2021</span></dt>
<dd><p><strong>missing journal in zombori2021fooling</strong></p>
</dd>
<dt class="label" id="id472"><span class="brackets">Zou et al., 2022</span></dt>
<dd><p>Zou, Z., Zhao, R., Shi, T., Qiu, S., &amp; Shi, Z. (2022). Castle in the sky: dynamic sky replacement and harmonization in videos. <em>IEEE Transactions on Image Processing</em>.</p>
</dd>
<dt class="label" id="id188"><span class="brackets">, 2020</span></dt>
<dd><p>方滨兴. (2020). <em>人工智能安全</em>. BEIJING BOOK CO. INC.</p>
</dd>
<dt class="label" id="id558"><span class="brackets"> et al., 2020</span></dt>
<dd><p>梁瑞刚, 吕培卓, 赵月, 陈鹏, 邢豪, 张颖君, … others. (2020). 视听觉深度伪造检测技术研究综述. <em>信息安全学报</em>, <em>5</em>(2), 1–17.</p>
</dd>
<dt class="label" id="id189"><span class="brackets"> et al., 2006</span></dt>
<dd><p>王珏, 周志华, &amp; 周傲英. (2006). <em>机器学习及其应用</em>. Vol. 4. 清华大学出版社有限公司.</p>
</dd>
<dt class="label" id="id47"><span class="brackets"> et al., 2021</span></dt>
<dd><p>谢宸琪, 张保稳, &amp; 易平. (2021). 人工智能模型水印研究综述. <em>计算机科学</em>, <em>48</em>(7), 9–16.</p>
</dd>
</dl>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../source/chap11.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>11. 未来展望</div>
         </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>