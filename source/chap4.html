<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>4. 数据安全：攻击 &#8212; 人工智能：数据与模型安全 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.purple-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/logo_removebg.png?"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. 数据安全：防御" href="chap5.html" />
    <link rel="prev" title="3. 人工智能安全基础" href="chap3.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active"><span class="section-number">4. </span>数据安全：攻击</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/source/chap4.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://trust-ml.github.io/">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/aisecuritybook">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://aisecuritybook.github.io/">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo_with_text.png" alt="人工智能：数据与模型安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="%E5%89%8D%E8%A8%80.html">前言</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chap1.html">1. 人工智能与安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap2.html">2. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap3.html">3. 人工智能安全基础</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. 数据安全：攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap5.html">5. 数据安全：防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap6.html">6. 模型安全：对抗攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap7.html">7. 模型安全：对抗防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap8.html">8. 模型安全：后门攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap9.html">9. 模型安全：后门防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">10. 模型安全：窃取攻防</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">11. 未来展望</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo_with_text.png" alt="人工智能：数据与模型安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="%E5%89%8D%E8%A8%80.html">前言</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chap1.html">1. 人工智能与安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap2.html">2. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap3.html">3. 人工智能安全基础</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. 数据安全：攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap5.html">5. 数据安全：防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap6.html">6. 模型安全：对抗攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap7.html">7. 模型安全：对抗防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap8.html">8. 模型安全：后门攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap9.html">9. 模型安全：后门防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">10. 模型安全：窃取攻防</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">11. 未来展望</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="chap-data-security-attack">
<span id="id1"></span><h1><span class="section-number">4. </span>数据安全：攻击<a class="headerlink" href="#chap-data-security-attack" title="Permalink to this heading">¶</a></h1>
<p>在过去的数十年中，人工智能已经迅速渗透到我们的日常生活中，在包括计算机视觉、自然语言处理、语音识别等多个关键领域取得了巨大的成功。然而，人工智能模型的训练需要大量数据和计算资源。因此，工业界和学术界在训练模型时使用外包数据、第三方机器学习平台或者预训练模型已经成为一种惯例。这种便捷的开发方式可以让研究人员快速的开发一个可用的人工智能模块并迅速投入使用，而不需要了解具体所使用的训练数据，但这也带来了很大的安全隐患。正因数据的完整性和准确性对机器学习算法正确运行的重要性不言而喻，数据也就自然成为攻击者的主要攻击目标之一。本章从数据投毒、隐私攻击、数据窃取和篡改与伪造四个角度介绍现有机器学习范式下数据所面临的攻击。</p>
<div class="section" id="sec-datapoisonattack">
<span id="id2"></span><h2><span class="section-number">4.1. </span>数据投毒<a class="headerlink" href="#sec-datapoisonattack" title="Permalink to this heading">¶</a></h2>
<p>数据投毒（也称投毒攻击）是一种训练阶段的攻击，其通过污染训练数据来干扰模型的训练，从而达到降低模型的推理性能的目的。投毒攻击的一般流程如图
<a class="reference internal" href="#fig-chap4datapoison"><span class="std std-numref">图4.1.1</span></a>
所示。在实际场景中，投毒者可以通过两种方式实施投毒攻击，即被动攻击和主动攻击。被动攻击是指攻击者可通过在线社交媒体上传有毒数据到网上，等待受害者利用网络爬虫下载使用；主动攻击则可以直接将有毒数据发送到数据集收集器中（如聊天机器人、垃圾邮件过滤器或用户信息数据库）。研究机构对28家公司的调查问卷显示数据投毒是工业界最担心的人工智能安全问题
<span id="id3">(<a class="reference internal" href="../chapter_references/zreferences.html#id118" title="Kumar, R. S. S., Nyström, M., Lambert, J., Marshall, A., Goertzel, M., Comissoneru, A., … Xia, S. (2020). Adversarial machine learning-industry perspectives. IEEE Security and Privacy Workshops (pp. 69–75).">Kumar <em>et al.</em>, 2020</a>)</span> 。</p>
<div class="figure align-default" id="id144">
<span id="fig-chap4datapoison"></span><a class="reference internal image-reference" href="../_images/4.1_dataPoison.png"><img alt="../_images/4.1_dataPoison.png" src="../_images/4.1_dataPoison.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.1.1 </span><span class="caption-text">投毒攻击示意图</span><a class="headerlink" href="#id144" title="Permalink to this image">¶</a></p>
</div>
<p>数据投毒工作可以追溯到1993年Kearns和Li <span id="id4">(<a class="reference internal" href="../chapter_references/zreferences.html#id137" title="Kearns, M., &amp; Li, M. (1993). Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4), 807–837.">Kearns and Li, 1993</a>)</span>
，他们在PAC（probably approximately
correct）学习设置下研究了如何在有恶意误差数据存在的情况下进行模型训练。
2006 年，Barreno等人 <span id="id5">(<a class="reference internal" href="../chapter_references/zreferences.html#id35" title="Barreno, M., Nelson, B., Sears, R., Joseph, A. D., &amp; Tygar, J. D. (2006). Can machine learning be secure? ACM Symposium on Information, Computer and Communications Security (pp. 16–25).">Barreno <em>et al.</em>, 2006</a>)</span>
揭示了通过恶意训练人工智能系统可以混淆网络入侵检测系统（intrusion
detection
system，IDS），使其在推理阶段对特定攻击不做拦截。2008年，Nelson等人
<span id="id6">(<a class="reference internal" href="../chapter_references/zreferences.html#id36" title="Nelson, B., Barreno, M., Chi, F. J., Joseph, A. D., Rubinstein, B. I., Saini, U., … Xia, K. (2008). Exploiting machine learning to subvert your spam filter. LEET, 8(1), 9.">Nelson <em>et al.</em>, 2008</a>)</span> 提出了针对垃圾邮件过滤器（spam
filter）的投毒攻击，通过错误标记1%的训练数据成功破坏了朴素贝叶斯（naive
Bayes）分类器的垃圾邮件过滤功能。2012年，Biggio等人
<span id="id7">(<a class="reference internal" href="../chapter_references/zreferences.html#id37" title="Biggio, B., Nelson, B., &amp; Laskov, P. (2012). Poisoning attacks against support vector machines. International Conference on International Conference on Machine Learning (pp. 1467–1474). Madison, WI, USA: Omnipress.">Biggio <em>et al.</em>, 2012</a>)</span>
正式提出了投毒攻击的概念。他们认为投毒攻击指通过将一小部分毒化数据注入训练数据或直接投毒模型参数，进而损害目标系统的功能的攻击。</p>
<p>数据投毒可大致分为六类：标签投毒攻击、在线投毒攻击、特征空间攻击、双层优化攻击、生成式攻击和差别化攻击。</p>
<div class="section" id="id8">
<h3><span class="section-number">4.1.1. </span>标签投毒攻击<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h3>
<p>模型训练是一个对训练样本进行迭代，使其能够一步步靠近标签的过程。所以正确的标签对正确的模型训练至关重要，而对攻击者来说也是如此，攻击训练过程所使用的标签是最直接一种投毒方式。这种攻击方式被称为标签投毒攻击（label
poisoning
attack），其通过混淆样本与标签之间的对应关系来破坏模型的训练。例如，<em>标签翻转攻击</em>（label
flipping，LF）将部分二分类数据的<span class="math notranslate nohighlight">\(0\)</span>/<span class="math notranslate nohighlight">\(1\)</span>标签进行随机翻转，使<span class="math notranslate nohighlight">\(0\)</span>标签对应数据在训练中靠近假标签<span class="math notranslate nohighlight">\(1\)</span>而<span class="math notranslate nohighlight">\(1\)</span>标签对应数据靠近假标签<span class="math notranslate nohighlight">\(0\)</span>。可以看出，此类投毒攻击需要很强的威胁模型，要求投毒者可以操纵训练数据的标注或使用。在二分类问题下，随机标签翻转攻击可形式化表示为：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4lf">
<span class="eqno">(4.1.1)<a class="headerlink" href="#equation-equ-chap4lf" title="Permalink to this equation">¶</a></span>\[\begin{split}LF(y) =\begin{cases}1-y, \;\;  y\in Y =\{0,1\} \\\text{random}( Y/\{y\}), \; \text{ otherwise}\end{cases}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(y\)</span>是原始标签，<span class="math notranslate nohighlight">\(1-y\)</span>可以在0-1分类问题下进行类别的翻转，<span class="math notranslate nohighlight">\(\text{random}(\cdot)\)</span>表示随机选择函数，适用于多分类问题。</p>
<p>除了随机选择样本翻转，我们还可以有选择性地对一部分数据进行翻转以最大化攻击效果。Biggio等人
<span id="id9">(<a class="reference internal" href="../chapter_references/zreferences.html#id37" title="Biggio, B., Nelson, B., &amp; Laskov, P. (2012). Poisoning attacks against support vector machines. International Conference on International Conference on Machine Learning (pp. 1467–1474). Madison, WI, USA: Omnipress.">Biggio <em>et al.</em>, 2012</a>)</span>
在随机标签翻转攻击的基础上，通过优化方法寻找部分易感染样本进行标签翻转，可以成功损害鲁棒训练的目标。Zhang等人
<span id="id10">(<a class="reference internal" href="../chapter_references/zreferences.html#id64" title="Zhang, R., &amp; Zhu, Q. (2017). A game-theoretic analysis of label flipping attacks on distributed support vector machines. Conference on Information Sciences and Systems (pp. 1–6).">Zhang and Zhu, 2017</a>)</span>
从博弈论的角度证明了标签翻转攻击对基于共识（consensus-based）的分布式支持向量机（distributed
support vector
machines，DSVM）同样有效。形象的理解，标签投毒类攻击是一种“<strong>指鹿为马</strong>”攻击，明明是物体A却非要说成是物体B，从而达到混淆视听的目的。</p>
</div>
<div class="section" id="id11">
<h3><span class="section-number">4.1.2. </span>在线投毒攻击<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h3>
<p>在线投毒攻击，也称<span class="math notranslate nohighlight">\(p\)</span>-<em>篡改攻击</em>（<span class="math notranslate nohighlight">\(p\)</span>-tampering
attack）是指在在线学习过程中对训练样本以一定概率<span class="math notranslate nohighlight">\(p\)</span>进行投毒以此削弱模型推理能力的攻击。在线投毒攻击假设攻击者可以对训练样本进行在线的修改、注入等，但对标签不做改动。
最早将<span class="math notranslate nohighlight">\(p\)</span>篡改攻击用于数据投毒的是Mahloujifar和Mahmoody
<span id="id12">(<a class="reference internal" href="../chapter_references/zreferences.html#id62" title="Mahloujifar, S., &amp; Mahmoody, M. (2017). Blockwise p-tampering attacks on cryptographic primitives, extractors, and learners. Theory of Cryptography Conference (pp. 245–279).">Mahloujifar and Mahmoody, 2017</a>)</span>
，他们以在线训练中的一段训练数据为原子，对其中比例为<span class="math notranslate nohighlight">\(p\)</span>的数据施加噪音来进行偏置，进而对模型在推理阶段的功能进行干扰。形象的理解，<span class="math notranslate nohighlight">\(p\)</span>篡改攻击是一种“<strong>暗度陈仓</strong>”攻击，在不改变类标的情况下（高隐蔽性），以一定概率偷偷修改样本，使数据分布产生偏移。</p>
<p>Mahloujifar等人 <span id="id13">(<a class="reference internal" href="../chapter_references/zreferences.html#id63" title="Mahloujifar, S., Mahmoody, M., &amp; Mohammed, A. (2019). Universal multi-party poisoning attacks. International Conference on Machine Learning (pp. 4274–4283).">Mahloujifar <em>et al.</em>, 2019</a>)</span>
后续将单方<span class="math notranslate nohighlight">\(p\)</span>-篡改攻击扩展到了多方学习，并以联邦学习为例进行了研究。
不同于单方学习，多方学习中参与方之间会相互影响，给数据投毒留下了很多空间（可相互传染）也带来一些挑战（避免相互干扰）。在多方学习场景下，<span class="math notranslate nohighlight">\(p\)</span>-篡改攻击可以扩展到<span class="math notranslate nohighlight">\((k,p)\)</span>-篡改攻击，其中<span class="math notranslate nohighlight">\(k\in\{1,2,\ldots,m\}\)</span>表示<span class="math notranslate nohighlight">\(m\)</span>个参与方中被攻击者控制的个数。<span class="math notranslate nohighlight">\((k,p)\)</span>-篡改可以高效的完成攻击，且不需要修改标签，是一种只依赖当前时刻样本的高效在线数据投毒攻击。</p>
</div>
<div class="section" id="id14">
<h3><span class="section-number">4.1.3. </span>特征空间攻击<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h3>
<p>特征空间投毒（feature space
poisoning）攻击通过修改毒化样本的深度特征来完成攻击。通过基于替代模型的深度特征修改，特征空间攻击几乎可以随意修改样本与类别之间的对应关系，即能让一个类别为A的样本跟任意非A类别的深度特征匹配。特征空间攻击有三个隐蔽性优势。首先，在特征空间进行对应关系的修改并不需要修改标签，具有很高的隐蔽性。其次，特征投毒可以基于优化方法通过对输入样本的轻微（微小）扰动完成，并不需要明显的投毒图案，因此可轻易躲过人工审核。第三，特征空间攻击通常只影响模型对特定目标样本的分类，而不影响非特定目标样本，故而很难被检测出来。毒化数据的影响通常在模型部署后才会显现出来。</p>
<p>2018年，Shafahi等人 <span id="id15">(<a class="reference internal" href="../chapter_references/zreferences.html#id60" title="Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., &amp; Goldstein, T. (2018). Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in Neural Information Processing Systems, 31.">Shafahi <em>et al.</em>, 2018</a>)</span>
提出了一种经典的特征空间投毒攻击方法：<em>特征碰撞攻击</em>（feature
collision
attack）。特征碰撞攻击是一种白盒数据毒化方法，其通过扰动部分<em>基类</em>（base
class）训练数据，使其在特征空间下趋于<em>目标类</em>（target
class），从而诱使模型在训练过程中产生误解。特征碰撞攻击最初是为攻击单个目标样本而设计的，所以也被称为“<em>有目标</em>”攻击，攻击多个样本则需要重复多次同样的攻击过程。具体而言，攻击者巧妙的使有毒基类数据点在特征空间中靠近目标类样本，从而诱使目标模型在推理阶段将目标类测试样本误分为基类类别。</p>
<p>特征碰撞攻击的优化目标定义如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4fc">
<span class="eqno">(4.1.2)<a class="headerlink" href="#equation-equ-chap4fc" title="Permalink to this equation">¶</a></span>\[\underset{ x_p}{\min}\Big[\|f( x_p)-f( x_t)\|^2_2+\beta \| x_p- x_b\|^2_2\Big],\]</div>
<p>其中，<span class="math notranslate nohighlight">\(x_p\)</span>为毒化样本，<span class="math notranslate nohighlight">\(x_t\)</span>为目标测试样本，<span class="math notranslate nohighlight">\(x_b\)</span>为训练数据中一个基类样本，<span class="math notranslate nohighlight">\(f\)</span>表示目标模型，<span class="math notranslate nohighlight">\(f(\cdot)\)</span>为模型的输出，<span class="math notranslate nohighlight">\(\beta\)</span>为超参数。上式中，第一项使毒化样本接近攻击目标类别<span class="math notranslate nohighlight">\(t\)</span>，达成攻击目的；第二项<span class="math notranslate nohighlight">\(\| x_p- x_b\|^2_2\)</span>控制毒化数据与基类数据相似，使二者在视觉上无明显差异，起到伪装效果。通俗的理解就是，让<span class="math notranslate nohighlight">\(x_p\)</span>看上去像<span class="math notranslate nohighlight">\(x_b\)</span>而特征和预测类别像<span class="math notranslate nohighlight">\(x_t\)</span>，起到“<strong>声东击西</strong>”的目的。后续很多隐蔽性数据投毒算法都是基于此思想，只是在优化方法上略有不同。</p>
<p>在特征碰撞攻击中，目标模型通常是在干净数据上预训练的模型，而投毒攻击发生在后续的模型微调过程中，主要用于攻击基于公开预训练模型的迁移学习。由于迁移学习冻结特征提取器而只微调最后一层的线性分类器，所以特征碰撞攻击对迁移学习很有效，有时毒化单张图像就可以成功攻击。然而，特征碰撞攻击也存在一定的局限性。首先，特征碰撞攻击需要攻击者掌握目标模型，这是很强的威胁模型假设。其次，一旦目标模型又通过其他干净数据再次微调，那么特征攻击效果会大大降低。因此，<em>端到端训练</em>或<em>逐层微调</em>对特征碰撞攻击具有显著的鲁棒性。</p>
<p>Zhu等人 <span id="id16">(<a class="reference internal" href="../chapter_references/zreferences.html#id65" title="Zhu, C., Huang, W. R., Li, H., Taylor, G., Studer, C., &amp; Goldstein, T. (2019). Transferable clean-label poisoning attacks on deep neural nets. International Conference on Machine Learning (pp. 7614–7623).">Zhu <em>et al.</em>, 2019</a>)</span>
在2019年提出了基于净标签的<em>凸多面体攻击</em>（convex polytope
attack，CPA）以提高特征空间攻击的迁移性。与特征碰撞攻击的单样本混淆策略不同，凸多面体攻击尝试寻找一组毒化样本将目标样本包围在一个凸包内。凸多面体攻击的优化目标如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-0">
<span class="eqno">(4.1.3)<a class="headerlink" href="#equation-source-chap4-0" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}\min\limits_{\{c^{(i)}\},\{ x^{(j)}_p\}}\frac{1}{2m}\sum^m_{i=1}\frac{\left\|f^{(i)}( x_t)-\sum^k_{j=1}c^{(i)}_j f^{(i)}( x^{(j)}_p)\right\|^2}{\left\|f^{(i)}( x_t)\right\|^2} \\s.t. \; \sum^k_{j=1}c^{(i)}_j=1, c^{(i)}_j\geq0, \forall i,j; \; \left\| x^{(j)}_p- x^{(j)}_b\right\|_\infty\leq \epsilon, \forall j,\end{split}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(x_p\)</span>为毒化样本，<span class="math notranslate nohighlight">\(x_t\)</span>为目标测试样本，<span class="math notranslate nohighlight">\(x_b\)</span>为训练数据中一个基类样本；一组预训练模型的集合被定义为<span class="math notranslate nohighlight">\(\{f^{(i)}\}^m_{i=1}\)</span>，<span class="math notranslate nohighlight">\(m\)</span>是集合中模型的数量；<span class="math notranslate nohighlight">\(\{ x^{(j)}_p\}^k_{j=1}\)</span>是针对<span class="math notranslate nohighlight">\(x_t\)</span>设计的<span class="math notranslate nohighlight">\(k\)</span>个“包围”样本，约束<span class="math notranslate nohighlight">\(\sum^k_{j=1}c^{(i)}_j=1, c^{(i)}_j\geq0\)</span>指“包围”投毒样本的权重都大于0且加和为1；添加扰动的上界被定义为<span class="math notranslate nohighlight">\(\epsilon\)</span>。凸多面体攻击在特征空间中构建了更大的“<em>攻击区域</em>”，从而增加了迁移攻击成功的可能性。当在多个中间层中实施凸多面体攻击时，迁移性会更大。形象的理解，凸多面体攻击是一种“<strong>四面楚歌</strong>”攻击，从不同角度对特征子空间进行围攻，从而使投毒数据在从头训练设置下也能起作用。</p>
</div>
<div class="section" id="id17">
<h3><span class="section-number">4.1.4. </span>双层优化攻击<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h3>
<p>最新的研究往往通过<em>双层优化</em>的方式去实现数据投毒，其也可以与其他攻击方式结合产生更大效益。实际上，优化的思想在数据投毒中早已存在，比如通过优化方法找出最适合标签投毒的数据集或者找到最有效的数据投毒方案。早在2008年，Nelson等人
<span id="id18">(<a class="reference internal" href="../chapter_references/zreferences.html#id36" title="Nelson, B., Barreno, M., Chi, F. J., Joseph, A. D., Rubinstein, B. I., Saini, U., … Xia, K. (2008). Exploiting machine learning to subvert your spam filter. LEET, 8(1), 9.">Nelson <em>et al.</em>, 2008</a>)</span>
便在其工作中通过优化产生能最大化合法邮件有害得分的电子邮件，并用来毒化训练数据。2012年，Biggio等人
<span id="id19">(<a class="reference internal" href="../chapter_references/zreferences.html#id37" title="Biggio, B., Nelson, B., &amp; Laskov, P. (2012). Poisoning attacks against support vector machines. International Conference on International Conference on Machine Learning (pp. 1467–1474). Madison, WI, USA: Omnipress.">Biggio <em>et al.</em>, 2012</a>)</span>
同样也使用了优化方法来找到可以最大化分类误差的样本。上述两种方法都是利用<em>梯度上升</em>的迭代算法来一步步计算出最优解决方案，并更新迭代出最终目标模型。为了统一概括数据投毒的优化问题，Mei和Zhu
<span id="id20">(<a class="reference internal" href="../chapter_references/zreferences.html#id117" title="Mei, S., &amp; Zhu, X. (2015). Using machine teaching to identify optimal training-set attacks on machine learners. AAAI Conference on Artificial Intelligence.">Mei and Zhu, 2015</a>)</span>
在2015年正式提出了“<em>有毒数据构建</em>+<em>目标模型更新</em>”的双层优化（bi-level
optimization）问题，并证明利用梯度可以有效地解决此双层优化问题。此后，数据投毒攻击便进入了基于双层优化问题求解的时代，研究者提出了很多更有效、更快速、亦或更便捷的数据投毒攻击方法。</p>
<p>双层优化攻击的核心是一个<strong>最大最小化问题</strong>。双层优化攻击的流程大体上是这样的：（1）攻击者首先将投毒攻击问题转化为一个优化问题以便找到全局最优值；（2）攻击者使用常见优化算法（如随机梯度下降）在相应的约束下检索解决方案。
数据投毒对应的双层优化问题可形式化如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4bilevelopt">
<span class="eqno">(4.1.4)<a class="headerlink" href="#equation-equ-chap4bilevelopt" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}{D_p}' = \underset{D_p}{\mathop{\mathrm{arg\,max}}} \mathcal{F}(D_p, { \theta}' ) = \mathcal{L}_{\text{out}}( D_{\text{val}},\theta' ) \\ s.t. \;\; \theta' = \underset{\theta}{\mathop{\mathrm{arg\,min}}} \mathcal{L}_{\text{in}}(D\cup D_{p}, \theta),\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(D\)</span>、<span class="math notranslate nohighlight">\(D_{\text{val}}\)</span>及<span class="math notranslate nohighlight">\(D_{p}\)</span>分别表示原始训练数据、验证数据已及有毒数据，<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{in}}\)</span>和<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{out}}\)</span>分别代表内层和外层的损失函数。外层优化的目的是生成可以最大化（未受污染的）验证数据集<span class="math notranslate nohighlight">\(D_{\text{val}}\)</span>在目标模型<span class="math notranslate nohighlight">\(\theta'\)</span>上的分类错误的有毒数据。内层优化的目的是通过迭代更新得到数据投毒后的目标模型，即目标模型会在有毒数据集<span class="math notranslate nohighlight">\(D\cup D_p\)</span>上迭代更新。由于目标模型参数<span class="math notranslate nohighlight">\(\theta'\)</span>是由有毒数据集<span class="math notranslate nohighlight">\(D_{p}\)</span>来隐性决定的，所以在外层优化中，我们用函数<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>来表述<span class="math notranslate nohighlight">\(\theta'\)</span>和<span class="math notranslate nohighlight">\(D_{p}\)</span>之间的联系。整个双层优化的过程可以描述为，每当内层优化达到局部最小值，外层优化会用更新后的目标模型<span class="math notranslate nohighlight">\(\theta'\)</span>来更新有毒数据集<span class="math notranslate nohighlight">\(D_{p}\)</span>，直到外层优化的损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{out}}( D_{\text{val}},\theta' )\)</span>收敛。</p>
<p>双层优化攻击是一种<em>破坏型攻击</em>，因为其攻击目标是让目标模型发生错误分类。当然，双层优化攻击也可以是<em>操纵型的</em>，即让目标模型将目标数据误分类为特定类别（需要预先指定）。在这种情况下，双层优化就变成了一个<strong>最小最小化问题</strong>，定义如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4bilevelopttg">
<span class="eqno">(4.1.5)<a class="headerlink" href="#equation-equ-chap4bilevelopttg" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}{D_p}' = \underset{D_p}{\mathop{\mathrm{arg\,min}}} \mathcal{F}( D_p, { \theta}' ) = \mathcal{L}_{\text{out}}( \left\{ x_t, y_{\text{adv}}\right\} , \theta' ) \\ s.t. \;\; \theta' = \underset{\theta}{\mathop{\mathrm{arg\,min}}} \mathcal{L}_{\text{in}}(D\cup D_{p}, \theta ),\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(y_{\text{adv}}\)</span>是攻击者预设的错误类别。而此时外层优化的目的是生成可以最小化目标模型<span class="math notranslate nohighlight">\(\theta'\)</span>在目标数据上的分类错误的有毒数据。上面的双层优化框架很好地概括了数据投毒与目标模型更新之间的关系，通过代入不同的目标函数、攻击目标及训练数据集，几乎所有数据投毒攻击场景都可以用这个框架实现。</p>
<p>解决双层优化问题的一种思路是通过<em>迭代算法</em>来步步逼近全局最优解。而基于梯度的攻击又可以将训练数据往目标梯度方向扰动进行数据毒化，直至毒化数据达到最优效果。在<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{out}}\)</span>可微的情况下，梯度可以通过<em>链式法则</em>（chain
rule）计算如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4gradient">
<span class="eqno">(4.1.6)<a class="headerlink" href="#equation-equ-chap4gradient" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}\nabla_{D_p} \mathcal{F} = \nabla_{D_p} \mathcal{L}_{\text{out}} + \frac{\partial \theta}{\partial D_p}^{\top}\nabla_{\theta}\mathcal{L}_{\text{out}} \\s.t. \;\; \frac{\partial \theta}{\partial D_p}^{\top} = ( \nabla_{D_p} \nabla_{\theta} \mathcal{L}_{\text{in}} ) ( \nabla_{\theta}^2 \mathcal{L}_2 )^{-1},\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\nabla_{D_p}\mathcal{F}\)</span>表示<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>关于<span class="math notranslate nohighlight">\(D_p\)</span>的偏导数。第<span class="math notranslate nohighlight">\(i\)</span>次迭代的毒化数据<span class="math notranslate nohighlight">\(D_p^{(i)}\)</span>可以通过梯度上升更新至<span class="math notranslate nohighlight">\(D_p^{(i+1)}\)</span>，形式化定义如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4poisondataite">
<span class="eqno">(4.1.7)<a class="headerlink" href="#equation-equ-chap4poisondataite" title="Permalink to this equation">¶</a></span>\[D_p^{(i+1)} = D_p^{(i)} + \alpha\nabla_{D_p^{(i+1)}}\mathcal{F}( D_p^{(i)} ),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\alpha\)</span>是由攻击者控制的学习率。</p>
<p>上面提到的例子多是针对传统机器学习（或者简单神经网络）的数据投毒攻击，他们隐性地假设基于梯度的内层优化可以被完美解决。然而，对于深度神经网络来说，其梯度有可能爆炸或消失，因此，针对深度学习的双层优化攻击需要特殊的梯度计算方式。
Mu{~n}oz-Gonz{‘a}lez等人 <span id="id21">(<a class="reference internal" href="../chapter_references/zreferences.html#id443" title="Muñoz-González, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E. C., &amp; Roli, F. (2017). Towards poisoning of deep learning algorithms with back-gradient optimization. ACM Workshop on Artificial Intelligence and Security (pp. 27–38).">Muñoz-González <em>et al.</em>, 2017</a>)</span>
利用“<em>反向梯度法</em>”来更高效稳定地计算内部优化的梯度。作者使用逐步梯度下降法近似求解内层优化问题，而每步梯度下降都会反向传播至外层优化及目标函数进行求解，经过数次迭代最终得到满足双层优化目标的毒化数据。然而基于随机梯度下降法的微分对内存来说是一个非常大的负担，所以该毒化数据制造方法是单个进行而不是分批次进行的，且只能作用于单层神经网络。</p>
<p>2018年，Jagielski等人 <span id="id22">(<a class="reference internal" href="../chapter_references/zreferences.html#id470" title="Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., &amp; Li, B. (2018). Manipulating machine learning: poisoning attacks and countermeasures for regression learning. IEEE Symposium on Security and Privacy (pp. 19–35).">Jagielski <em>et al.</em>, 2018</a>)</span> 将Muñoz-González等人
<span id="id23">(<a class="reference internal" href="../chapter_references/zreferences.html#id443" title="Muñoz-González, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E. C., &amp; Roli, F. (2017). Towards poisoning of deep learning algorithms with back-gradient optimization. ACM Workshop on Artificial Intelligence and Security (pp. 27–38).">Muñoz-González <em>et al.</em>, 2017</a>)</span>
的工作进行了扩展，提出了一个专用于回归模型的数据投毒及防御的理论优化框架。Huang等人
<span id="id24">(<a class="reference internal" href="../chapter_references/zreferences.html#id120" title="Huang, W. R., Geiping, J., Fowl, L., Taylor, G., &amp; Goldstein, T. (2020). Metapoison: practical general-purpose clean-label data poisoning. Advances in Neural Information Processing Systems, 33, 12080–12091.">Huang <em>et al.</em>, 2020</a>)</span>
提出的MetaPoison采用集成的方式，利用<span class="math notranslate nohighlight">\(m\)</span>个模型和<span class="math notranslate nohighlight">\(K\)</span>-step内部最小化来求解公式
<a class="reference internal" href="#equation-equ-chap4bilevelopttg">(4.1.5)</a>
。具体来说，对每个模型在毒化数据上进行<span class="math notranslate nohighlight">\(K\)</span>步基于SGD的梯度下降，然后计算并存储外部最小化对应的梯度（称为<em>对抗梯度</em>），在<span class="math notranslate nohighlight">\(m\)</span>个模型上计算完毕后累积得到平均对抗梯度，之后使用平均对抗梯度更新毒化样本。当达到一定的训练周期后，需要重新初始化<span class="math notranslate nohighlight">\(m\)</span>个模型的参数，以增加探索（防止因模型收敛过快而导致毒化样本探索不够）。</p>
<p>相比之前的方法，MetaPoison是<em>净标签数据投毒</em>（clean-label data
poisoning）领域的一个重要改进。其数据毒化过程更加通用，无论是破坏型还是操纵型攻击目标都可达成，生成的毒化数据可以在整个训练过程中都对目标模型有影响，而且不会对某个代理模型过拟合。此外，MetaPoison还做到了毒化数据的跨模型和训练设置迁移，即MetaPoison生成的毒化数据可投毒其他训练设置、网络架构未知的模型。MetaPoison甚至成功毒化了工业级服务，即Google
Cloud AutoML
API。而MetaPoison也是第一个在人眼不可察觉的前提下，可同时攻击微调模型及端对端模型的数据投毒方法。</p>
<p>2020年，Geiping等人 <span id="id25">(<a class="reference internal" href="../chapter_references/zreferences.html#id121" title="Geiping, J., Fowl, L. H., Huang, W. R., Czaja, W., Taylor, G., Moeller, M., &amp; Goldstein, T. (2021). Witches' brew: industrial scale data poisoning via gradient matching. International Conference on Learning Representations.">Geiping <em>et al.</em>, 2021</a>)</span>
对MetaPoison攻击做了进一步改进，提出了Witches’
Brew攻击方法，使得此类投毒攻击达到工业规模。Witches’
Brew攻击引入“<em>梯度对齐</em>”的概念，使毒化目标函数与对抗目标函数具有一致的梯度，也就是说，使模型在毒化样本和其目标样本上的梯度一致。当这个目标达成时，毒化样本和目标样将对模型产生同样的梯度激活，也就是在训练过程中对模型参数更新起到一模一样的作用，因此训练毒化数据过程中进行的标准梯度下降也会强制使对应目标图像上的对抗性损失降低，进而完成攻击目标（让模型将毒化样本完全当做目标样本来学习）。</p>
</div>
<div class="section" id="id26">
<h3><span class="section-number">4.1.5. </span>生成式攻击<a class="headerlink" href="#id26" title="Permalink to this heading">¶</a></h3>
<p>上述数据投毒攻击都在毒化数据生成和使用效率上有所受限，而基于生成模型的生成式攻击（generative
attack）则可避免基于优化攻击的高昂计算代价，大大提高毒化数据的生成和使用效率。生成式攻击的核心是生成模型的训练（如生成对抗网络和自动编码器）。生成模型可以通过学习毒化噪声分布进而大规模生成毒化数据。生成式攻击一般需要攻击者知晓目标模型的相关知识，对应灰盒或白盒威胁模型。</p>
<p>Yang等人 <span id="id27">(<a class="reference internal" href="../chapter_references/zreferences.html#id399" title="Yang, C., Wu, Q., Li, H., &amp; Chen, Y. (2017). Generative poisoning attack method against neural networks. arXiv preprint arXiv:1703.01340.">Yang <em>et al.</em>, 2017</a>)</span>
提出基于编码器解码器（encoder-decoder）的毒化数据生成攻击。此攻击框架有两个重要的组成模块：生成模型<span class="math notranslate nohighlight">\(G\)</span>以及目标模型<span class="math notranslate nohighlight">\(f\)</span>。其毒化数据生成过程可以描述为：在第<span class="math notranslate nohighlight">\(i\)</span>次迭代中，生成模型产生毒化数据<span class="math notranslate nohighlight">\(x_p^i\)</span>；攻击者将此时的毒化数据注入到训练数据中，令目标模型的参数从<span class="math notranslate nohighlight">\(\theta^{( i-1 )}\)</span>更新为<span class="math notranslate nohighlight">\(\theta^{( i )}\)</span>；攻击者进一步评估目标模型在验证集<span class="math notranslate nohighlight">\(D_{\text{val}}\)</span>上的表现，并以此为依据引导生成模型的进一步优化；攻击者更新生成模型并进入下一迭代。此迭代过程可形式化表示如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4generative">
<span class="eqno">(4.1.8)<a class="headerlink" href="#equation-equ-chap4generative" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} G' = \underset{G}{\mathop{\mathrm{arg\,max}}} \sum_{( x,y)\sim D_{\text{val}}} \mathcal{L}( f_{\theta'}( G( x )), y)\\ s.t. \; \theta' = \underset{\theta}{\mathop{\mathrm{arg\,min}}}  \sum_{( x,y)\sim D_{p}} \mathcal{L}( f_{\theta}( G'(  x ) ), y ),\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\theta\)</span>表示目标模型<span class="math notranslate nohighlight">\(f\)</span>的原始参数，<span class="math notranslate nohighlight">\(\theta'\)</span>表示目标模型被数据投毒攻击后的参数。生成式攻击的最终目标是使生成模型<span class="math notranslate nohighlight">\(G\)</span>能够无限生成能降低目标模型<span class="math notranslate nohighlight">\(f\)</span>性能的毒化数据。在此基础上，Feng等人
<span id="id28">(<a class="reference internal" href="../chapter_references/zreferences.html#id125" title="Feng, J., Cai, Q.-Z., &amp; Zhou, Z.-H. (2019). Learning to confuse: generating training time adversarial data with auto-encoder. Advances in Neural Information Processing Systems, 32.">Feng <em>et al.</em>, 2019</a>)</span>
提出了一个类似的生成模型训练方式，引入<em>伪更新</em>（pseudo-update）步骤来更新生成模型<span class="math notranslate nohighlight">\(G\)</span>。这种新的更新方式克服了交替更新（<span class="math notranslate nohighlight">\(f\)</span>和<span class="math notranslate nohighlight">\(G\)</span>）所导致的生成模型训练不稳定的问题。</p>
<p>除了自动编码器（autoencoder），生成对抗网络（GAN）也可以用来生成毒化数据。例如Muñoz-González等人
<span id="id29">(<a class="reference internal" href="../chapter_references/zreferences.html#id124" title="Muñoz-González, L., Pfitzner, B., Russo, M., Carnerero-Cano, J., &amp; Lupu, E. C. (2019). Poisoning attacks with generative adversarial nets. arXiv preprint arXiv:1906.07773.">Muñoz-González <em>et al.</em>, 2019</a>)</span>
提出的pGAN攻击，其由生成器<span class="math notranslate nohighlight">\(G\)</span>，鉴别器<span class="math notranslate nohighlight">\(D\)</span>及分类器（即目标模型<span class="math notranslate nohighlight">\(f\)</span>）三个子模型组成。鉴别器<span class="math notranslate nohighlight">\(D\)</span>用于区分毒化样本与干净样本，而生成器<span class="math notranslate nohighlight">\(G\)</span>旨在生成高效的毒化样本以最大化目标模型<span class="math notranslate nohighlight">\(f\)</span>的分类误差，同时让鉴别器<span class="math notranslate nohighlight">\(D\)</span>无法区别毒化样本与干净样本。这种对抗博弈使得生成式攻击可以在攻击强度和隐蔽性之间做出权衡，更灵活地应对不同风险级别的人工智能模型。</p>
</div>
<div class="section" id="id30">
<h3><span class="section-number">4.1.6. </span>差别化攻击<a class="headerlink" href="#id30" title="Permalink to this heading">¶</a></h3>
<p>上述几类攻击方法在投毒的过程中随机选取少量训练样本进行毒化，可以被认为是一种<em>无差别化攻击</em>。然而，研究发现毒化样本的选择会大大影响攻击效果。由此引出了基于<em>样本影响力</em>的<em>差别化攻击</em>（influence-based
attack）。基于样本影响力的投毒攻击通过选择影响力大的样本来投毒，以此来提高攻击的强度。单个样本对模型性能的影响可以定义为：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4influence">
<span class="eqno">(4.1.9)<a class="headerlink" href="#equation-equ-chap4influence" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}\mathcal{I}(  z ) = -\mathcal{H}_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}( f_{\hat{\theta}} (  x ), y)\\s.t. \;\; \hat{\theta} = \underset{\theta}{\mathop{\mathrm{arg\,min}}} \sum_{( x,y) \sim D_{\text{val}}}\mathcal{L}( f_{\theta}(  x ),y )\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(x\)</span>表示目标样本，<span class="math notranslate nohighlight">\(y\)</span>表示样本标签，<span class="math notranslate nohighlight">\(\hat{\theta}\)</span>表示移除样本<span class="math notranslate nohighlight">\(x\)</span>后所得到的目标模型的参数，<span class="math notranslate nohighlight">\(D_{\text{val}}\)</span>表示验证数据集，<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>为经验风险的海森矩阵（Hessian
matrix），即<span class="math notranslate nohighlight">\(\mathcal{H}_{\hat{\theta}}= \frac{1}{n}\sum_{i=1}^{n}\nabla^2_{\hat{\theta}}L( f_{\hat{\theta}}( x_i ),y_i )\)</span>。</p>
<p>Koh等人 <span id="id31">(<a class="reference internal" href="../chapter_references/zreferences.html#id127" title="Koh, P. W., &amp; Liang, P. (2017). Understanding black-box predictions via influence functions. International Conference on Machine Learning (pp. 1885–1894).">Koh and Liang, 2017</a>, <a class="reference internal" href="../chapter_references/zreferences.html#id119" title="Koh, P. W., Steinhardt, J., &amp; Liang, P. (2022). Stronger data poisoning attacks break data sanitization defenses. Machine Learning, 111(1), 1–47.">Koh <em>et al.</em>, 2022</a>)</span>
首次将<em>影响函数</em>（influence
function）应用于梯度计算，并提出了三种有效的<em>近似双层优化方法</em>。首先，他们通过公式
<a class="reference internal" href="#equation-equ-chap4influence">(4.1.9)</a>
来计算删除特定样本对测试损失的影响，进而确定对模型训练影响最大的样本。接下来，以较大影响样本作为毒化目标，继续利用影响函数来产生毒化数据。最后，将毒化数据注入到原始训练数据集中。最终，该方法成功使目标模型将一个特定测试图片错误分类。Fang等人
<span id="id32">(<a class="reference internal" href="../chapter_references/zreferences.html#id126" title="Fang, M., Gong, N. Z., &amp; Liu, J. (2020). Influence function based data poisoning attacks to top-n recommender systems. The Web Conference 2020 (pp. 3019–3025).">Fang <em>et al.</em>, 2020</a>)</span>
将基于影响的投毒攻击用在了推荐系统上，攻击者可以通过精心伪造的用户及交互数据投毒推荐系统，使其做出错误推荐。值得注意的是，Basu等人
<span id="id33">(<a class="reference internal" href="../chapter_references/zreferences.html#id128" title="Basu, S., Pope, P., &amp; Feizi, S. (2021). Influence functions in deep learning are fragile. International Conference on Learning Representations.">Basu <em>et al.</em>, 2021</a>)</span>
近来提出，由于深度神经网络的非凸（non-convex）损失表面，影响函数并不能有效捕捉深度神经网络中的数据依赖。因此，差别化攻击在深度神经网络中的应用还有待进一步研究。</p>
</div>
</div>
<div class="section" id="sec-privacyinferenceattack">
<span id="id34"></span><h2><span class="section-number">4.2. </span>隐私攻击<a class="headerlink" href="#sec-privacyinferenceattack" title="Permalink to this heading">¶</a></h2>
<p>目前针对数据的隐私攻击主要是针对深度神经网络的<em>推理</em>攻击，攻击者在白盒或黑盒威胁模型下试图从模型中推理或逆向出有关训练数据的信息或者训练数据本身。图
<a class="reference internal" href="#fig-chap4privacyattack"><span class="std std-numref">图4.2.1</span></a>
以医学图像分析场景为例，展示了在白盒和黑盒两种不同威胁模型下的隐私攻击。
我们的个人数据，如自拍照、健康数据、医疗数据、消费习惯、移动轨迹、个人爱好、电话号码、家庭住址等，有可能会在某个地方被用于训练人工智能模型。而通过隐私攻击，攻击者可以获知个人隐私信息，如是否患有某种疾病、是否到过某个地方等。在人工智能技术被广泛应用的今天，隐私攻击无疑是对个人隐私的巨大威胁。
本章将围绕<em>推理类隐私攻击</em>介绍成员推理攻击、属性推理攻击和其他推理类攻击。
专门的数据窃取类攻击会在章节 <a class="reference internal" href="#sec-dataextractionattack"><span class="std std-numref">4.3节</span></a>
中介绍。</p>
<div class="figure align-default" id="id145">
<span id="fig-chap4privacyattack"></span><a class="reference internal image-reference" href="../_images/4.2_privacy_attack.png"><img alt="../_images/4.2_privacy_attack.png" src="../_images/4.2_privacy_attack.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.2.1 </span><span class="caption-text">白盒和黑盒隐私攻击示例</span><a class="headerlink" href="#id145" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="sec-membership-inference-attack">
<span id="id35"></span><h3><span class="section-number">4.2.1. </span>成员推理攻击<a class="headerlink" href="#sec-membership-inference-attack" title="Permalink to this heading">¶</a></h3>
<p>成员推理攻击（membership inference
attack，MIA）的主要思想是利用目标模型在训练数据和测试数据上的<em>不一致性</em>来推理某一样本是否在目标模型的训练数据中。通过判定某个样本是否存在于训练数据集中，攻击者可以进一步猜测样本所属的类别以及其他一些隐私信息，如推断某人是否去过某个地方、是否患有某种疾病等。图
<a class="reference internal" href="#fig-chap4mia"><span class="std std-numref">图4.2.2</span></a>
展示了黑盒威胁模型下的成员推理攻击，此时攻击者只能根据目标模型的输出（比如概率向量）和样本的真是类别信息来判断样本是否存在于训练集合中，而不知道目标模型的参数和中间层结果。</p>
<div class="figure align-default" id="id146">
<span id="sec-shadow-model-attack"></span><span id="fig-chap4mia"></span><a class="reference internal image-reference" href="../_images/4.3_MIA.png"><img alt="../_images/4.3_MIA.png" src="../_images/4.3_MIA.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.2.2 </span><span class="caption-text">黑盒威胁模型下的成员推理攻击 <span id="id36">(<a class="reference internal" href="../chapter_references/zreferences.html#id153" title="Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. IEEE Symposium on Security and Privacy (pp. 3–18).">Shokri <em>et al.</em>, 2017</a>)</span></span><a class="headerlink" href="#id146" title="Permalink to this image">¶</a></p>
</div>
<p>当前成员推理攻击大都针对深度学习模型提出，关于为什么深度学习模型易受成员推理攻击有三个方面的解释。首先，<em>人工智能模型易过拟合训练数据</em>。深度学习模型的高复杂度以及训练数据的有限性导致模型很容易过拟合到训练数据。过拟合的模型一般在训练数据上表现明显优于测试数据，而这种差异往往被攻击者用来进行成员推理攻击。其次，<em>单个样本的变化会影响最终的模型</em>。研究发现，成员推理攻击对决策边界易受单样本影响的模型更容易成功。第三，<em>数据的多样性及取样的局限性</em>。当训练数据不足以充分代表真实数据分布时，模型会对训练过的数据（即训练数据）和未训练过的数据（即测试数据）产生不同的体现。比如，对于分类模型来说，训练样本的平均置信度在训练样本上往往更高。总结来说，模型在训练和测试数据上的泛化差异是导致成员推理攻击存在的主要原因。</p>
<p>成员推理攻击的雏形来自于2008年Homer等人的工作
<span id="id37">(<a class="reference internal" href="../chapter_references/zreferences.html#id150" title="Homer, N., Szelinger, S., Redman, M., Duggan, D., Tembe, W., Muehling, J., … Craig, D. W. (2008). Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. PLOS Genetics, 4(8), e1000167.">Homer <em>et al.</em>, 2008</a>)</span>
，其基于基因组数据的公开统计数据集推断某特定基因组是否存于在训练数据集中。
后来，Pyrgelis等人 <span id="id38">(<a class="reference internal" href="../chapter_references/zreferences.html#id151" title="Pyrgelis, A., Troncoso, C., &amp; Cristofaro, E. D. (2018). Knock knock, who's there? membership inference on aggregate location data. Network and Distributed System Security Symposium. The Internet Society.">Pyrgelis <em>et al.</em>, 2018</a>, <a class="reference internal" href="../chapter_references/zreferences.html#id152" title="Pyrgelis, A., Troncoso, C., &amp; De Cristofaro, E. (2020). Measuring membership privacy on aggregate location time-series. ACM on Measurement and Analysis of Computing Systems, 4(2), 1–28.">Pyrgelis <em>et al.</em>, 2020</a>)</span>
将相关研究拓展至地理位置数据集。但真正让成员推理攻击取得大众广泛关注的Shokri等人
<span id="id39">(<a class="reference internal" href="../chapter_references/zreferences.html#id153" title="Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. IEEE Symposium on Security and Privacy (pp. 3–18).">Shokri <em>et al.</em>, 2017</a>)</span>
在2017年提出的工作，他们首次提出对深度学习分类模型的成员推理攻击。在此基础上，后续的工作将成员推理攻击拓展到各类型机器学习模型上，包括分类模型
<span id="id40">(<a class="reference internal" href="../chapter_references/zreferences.html#id153" title="Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. IEEE Symposium on Security and Privacy (pp. 3–18).">Shokri <em>et al.</em>, 2017</a>)</span> 、生成模型 <span id="id41">(<a class="reference internal" href="../chapter_references/zreferences.html#id155" title="Hayes, J., Melis, L., Danezis, G., &amp; De Cristofaro, E. (2019). Logan: membership inference attacks against generative models. Privacy Enhancing Technologies, 2019(1), 133–152.">Hayes <em>et al.</em>, 2019</a>)</span>
、回归模型 <span id="id42">(<a class="reference internal" href="../chapter_references/zreferences.html#id156" title="Gupta, U., Stripelis, D., Lam, P. K., Thompson, P., Ambite, J. L., &amp; Ver Steeg, G. (2021). Membership inference attacks on deep regression models for neuroimaging. Medical Imaging with Deep Learning (pp. 228–251).">Gupta <em>et al.</em>, 2021</a>)</span> 、嵌入模型
<span id="id43">(<a class="reference internal" href="../chapter_references/zreferences.html#id154" title="Song, C., &amp; Raghunathan, A. (2020). Information leakage in embedding models. ACM SIGSAC Conference on Computer and Communications Security (pp. 377–390).">Song and Raghunathan, 2020</a>)</span> 等。值得一提的是，Melis等人
<span id="id44">(<a class="reference internal" href="../chapter_references/zreferences.html#id149" title="Melis, L., Song, C., De Cristofaro, E., &amp; Shmatikov, V. (2019). Exploiting unintended feature leakage in collaborative learning. IEEE Symposium on Security and Privacy (pp. 691–706).">Melis <em>et al.</em>, 2019</a>)</span>
在2019年提出针对联邦学习的成员推理攻击，引发了系列针对联邦学习“隐私性”的激烈讨论，因为联邦学习理应是一种保护隐私的学习方式。下面三个小节将详细介绍三类经典的成员推理攻击方法。</p>
<div class="section" id="id45">
<h4><span class="section-number">4.2.1.1. </span>影子模型攻击<a class="headerlink" href="#id45" title="Permalink to this heading">¶</a></h4>
<p>影子模型攻击（shadow model-based attack）是2017年由Shokri等人
<span id="id46">(<a class="reference internal" href="../chapter_references/zreferences.html#id153" title="Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. IEEE Symposium on Security and Privacy (pp. 3–18).">Shokri <em>et al.</em>, 2017</a>)</span>
提出的成员推理攻击方法，也是首个针对深度学习模型的成员推理攻击方法，其打开了深度学习领域成员推理研究的序幕。
影子模型攻击的思想是将成员推理看做一个二分类（‘成员’或‘非成员’）问题。假设攻击者对训练数据的来源分布是有一些先验知识的，即可以从同一个数据分布总池中采样（但与原训练数据不相交）并构建<em>仿数据集</em>的能力。
影子模型攻击大致分为以下三步：</p>
<ul class="simple">
<li><p>攻击者采样多个<em>影子训练集</em>，并在影子训练集上训练多个可以模仿目标模型表现的<em>影子模型</em>；</p></li>
<li><p>根据影子训练集、影子测试集以及影子模型，构建以模型的预测向量输出为样本，以0或1为标签的<em>攻击训练数据集</em>；</p></li>
<li><p>在攻击训练数据集上训练得到一个二分类器（称为攻击模型）来进行成员推理攻击。</p></li>
</ul>
<div class="figure align-default" id="id147">
<span id="fig-chap4shadowmodel"></span><a class="reference internal image-reference" href="../_images/4.4_shadowModel.png"><img alt="../_images/4.4_shadowModel.png" src="../_images/4.4_shadowModel.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.2.3 </span><span class="caption-text">影子训练集与影子模型示意图 <span id="id47">(<a class="reference internal" href="../chapter_references/zreferences.html#id153" title="Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. IEEE Symposium on Security and Privacy (pp. 3–18).">Shokri <em>et al.</em>, 2017</a>)</span></span><a class="headerlink" href="#id147" title="Permalink to this image">¶</a></p>
</div>
<p>图 <a class="reference internal" href="#fig-chap4shadowmodel"><span class="std std-numref">图4.2.3</span></a>
展示了影子训练集和影子模型的概念，影子训练集与隐私训练集来自于相同分布但不重叠，影子模型和目标模型在同一个机器学习平台上相互独立训练。
图 <a class="reference internal" href="#fig-chap4shadowmodelmia"><span class="std std-numref">图4.2.4</span></a>
展示了影子模型攻击中攻击模型（attack
model）的构建过程。假设目标数据集为<span class="math notranslate nohighlight">\(D_t\)</span>、目标模型为<span class="math notranslate nohighlight">\(f_t\)</span>，影子模型攻击通过独立采样获得与目标数据集独立同分布的<span class="math notranslate nohighlight">\(k\)</span>个影子训练集<span class="math notranslate nohighlight">\(\{D^1_s,\ldots,D^k_s\}\)</span>（<span class="math notranslate nohighlight">\(\forall i\)</span>，<span class="math notranslate nohighlight">\(D^i_s \cap D_t=\emptyset\)</span>），并在影子训练集上独立训练得到<span class="math notranslate nohighlight">\(k\)</span>个影子模型<span class="math notranslate nohighlight">\(\{f^1_s,\ldots,f^k_s\}\)</span>。在采样影子训练集的同时，攻击者还采样了<span class="math notranslate nohighlight">\(k\)</span>个影子测试集<span class="math notranslate nohighlight">\(\{D'^1_s,\ldots,D'^k_s\}\)</span>（<span class="math notranslate nohighlight">\(\forall i\)</span>，<span class="math notranslate nohighlight">\(D'^i_s \cap D_t=\emptyset\)</span>），通过影子训练集和影子测试集中的样本，我们就可以构建攻击模型的<em>攻击训练集</em>了。将两类样本输入各自对应的影子模型中，得到模型的预测概率向量，如果样本来自影子训练集则将预测概率向量标记为类别“在”（即此样本在影子模型的训练集中），否则将预测概率向量标记为类别“不在”（即此样本不在影子模型的训练集中）。对<span class="math notranslate nohighlight">\(k\)</span>个影子训练集、影子测试集以及影子模型重复上述操作，可以得到最终的攻击训练集<span class="math notranslate nohighlight">\(D_{\text{attack}}\)</span>，如图
<a class="reference internal" href="#fig-chap4shadowmodelmia"><span class="std std-numref">图4.2.4</span></a> 所示。
在攻击训练集上训练即可得到攻击者做成员推理需要的攻击模型，这也就意味着，攻击训练集是一个二分类数据集，攻击模型是一个二分类器，成员推理攻击是一个二分类问题。</p>
<div class="figure align-default" id="id148">
<span id="fig-chap4shadowmodelmia"></span><a class="reference internal image-reference" href="../_images/4.5_shadowModelMIA.png"><img alt="../_images/4.5_shadowModelMIA.png" src="../_images/4.5_shadowModelMIA.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.2.4 </span><span class="caption-text">影子模型攻击 <span id="id48">(<a class="reference internal" href="../chapter_references/zreferences.html#id153" title="Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. IEEE Symposium on Security and Privacy (pp. 3–18).">Shokri <em>et al.</em>, 2017</a>)</span></span><a class="headerlink" href="#id148" title="Permalink to this image">¶</a></p>
</div>
<p>影子模型攻击通用于白盒和黑盒威胁模型，虽然根据攻击者所掌握目标模型的信息不同，构建的攻击训练数据集会有一定的差别。在黑盒威胁模型下，攻击者通过API对目标模型进行查询，只能得到对应的预测概率向量；而在白盒威胁模型下，攻击者可以获得中间层的激活信息。下面将以黑盒为主介绍影子模型攻击。</p>
<p><strong>黑盒威胁模型。</strong>在黑盒模型下得到的攻击数据集包含以下两个部分:</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4blcmemberdts">
<span class="eqno">(4.2.1)<a class="headerlink" href="#equation-equ-chap4blcmemberdts" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}D^{m}_{\text{attack}} = \{ p( x ), 1\}, \;\; \forall  x \in D^i_s \\D^{n}_{\text{attack}} = \{ p( x ), 0\}, \;\; \forall  x \in D'^i_s,\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(p( x )\)</span>表示目标模型的预测概率输出，<span class="math notranslate nohighlight">\(D^{m}_{\text{attack}}\)</span>和<span class="math notranslate nohighlight">\(D^{n}_{\text{attack}}\)</span>分别表示“成员”（member）和“非成员”（non-member）类别的数据。下一步是训练攻击模型。假设目标模型<span class="math notranslate nohighlight">\(f_t\)</span>为深度神经网络分类器（模型参数为<span class="math notranslate nohighlight">\(\theta_t\)</span>），那么攻击者可以通过最小化以下经典二分类目标函数，训练得到攻击模型<span class="math notranslate nohighlight">\(f_{\text{attack}}\)</span>（参数为<span class="math notranslate nohighlight">\(\theta_{\text{attack}}\)</span>）：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4blcshadowobj">
<span class="eqno">(4.2.2)<a class="headerlink" href="#equation-equ-chap4blcshadowobj" title="Permalink to this equation">¶</a></span>\[\theta_{\text{attack}} = \underset{\theta'}{\mathop{\mathrm{arg\,min}}} \frac{1}{n}\sum ^n_{i=1}\mathcal{L}_{\text{BCE}}( f_\text{attack}( x_i), y'_i),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(n\)</span>表示攻击训练数据集中的样本总数，<span class="math notranslate nohighlight">\(y'_i\)</span>为成员标签（“成员”或“非成员”），<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{BCE}}\)</span>表示二元交叉熵（BCE）损失函数。
通过访问目标模型<span class="math notranslate nohighlight">\(f_t\)</span>的查询API得到任意样本的返回概率向量，再将此向量输入攻击模型<span class="math notranslate nohighlight">\(f_{\text{attack}}\)</span>就可以推理此样本是否属于目标模型的原始训练数据集<span class="math notranslate nohighlight">\(D_t\)</span>。</p>
<p><strong>白盒威胁模型。</strong> Nasr等人 <span id="id49">(<a class="reference internal" href="../chapter_references/zreferences.html#id213" title="Nasr, M., Shokri, R., &amp; Houmansadr, A. (2019). Comprehensive privacy analysis of deep learning: passive and active white-box inference attacks against centralized and federated learning. IEEE Symposium on Security and Privacy (pp. 739–753).">Nasr <em>et al.</em>, 2019</a>)</span>
首先提出了基于白盒威胁模型的成员推理攻击，攻击者对目标模型具有内部访问权限，可以使用更多信息构建攻击数据集。这包括除预测向量及成员类别以外的：任意中间层的激活输出<span class="math notranslate nohighlight">\(f^{(l)}_t( x)\)</span>、损失对于中间层参数的梯度<span class="math notranslate nohighlight">\(\frac{\delta \mathcal{L}_{\text{BCE}}}{\delta \theta^{(l)}_t }\)</span>、输入样本的损失<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{BCE}}\)</span>等。攻击者可以将这些特征拼接为一个大的向量<span class="math notranslate nohighlight">\(v\)</span>，表示为:</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4whtshadowv">
<span class="eqno">(4.2.3)<a class="headerlink" href="#equation-equ-chap4whtshadowv" title="Permalink to this equation">¶</a></span>\[v = \Big( p( x), f^{(1)}_t( x),  \frac{\delta \mathcal{L}_{\text{BCE}}}{\delta \theta^{(1)}_t  }, \ldots, f^{(l)}_t( x),  \frac{\delta \mathcal{L}_{\text{BCE}}}{\delta \theta^{(l)}_t }, \mathcal{L}_{\text{BCE}}( x, y)\Big)\]</div>
<p>而得到的攻击数据集则表示为：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4whtmemberdts">
<span class="eqno">(4.2.4)<a class="headerlink" href="#equation-equ-chap4whtmemberdts" title="Permalink to this equation">¶</a></span>\[D^{m}_{\text{attack}} = \{ v, 1\}, \;\;D^{n}_{\text{attack}} = \{ v, 0\}.\]</div>
<p>类似地，目标函数则表达为：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4whtshadowobj">
<span class="eqno">(4.2.5)<a class="headerlink" href="#equation-equ-chap4whtshadowobj" title="Permalink to this equation">¶</a></span>\[\theta_{\text{attack}} = \underset{\theta'}{\mathop{\mathrm{arg\,min}}} \frac{1}{n}\sum ^n_{i=1}\mathcal{L}_{\text{BCE}}( f_\text{attack}( v_i), y'_i).\]</div>
<p>Shokri等人 <span id="id50">(<a class="reference internal" href="../chapter_references/zreferences.html#id153" title="Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. IEEE Symposium on Security and Privacy (pp. 3–18).">Shokri <em>et al.</em>, 2017</a>)</span>
提出的影子模型攻击需要两个很强的假设：1）攻击者有多个与目标模型结构相同的影子模型；2）攻击者知道训练数据分布。Salem等人
<span id="id51">(<a class="reference internal" href="../chapter_references/zreferences.html#id203" title="Salem, A., Zhang, Y., Humbert, M., Fritz, M., &amp; Backes, M. (2019). Ml-leaks: model and data independent membership inference attacks and defenses on machine learning models. Network and Distributed Systems Security Symposium.">Salem <em>et al.</em>, 2019</a>)</span>
放宽了这两个假设。首先，他们将影子模型的数量缩减至一个，并使用不同于目标模型的网络结构或训练方法来训练这个影子模型。在多个图片数据集上的实验结果表明，宽松假设下的成员推理攻击的成功率有所下降，但在可接受范围之内。紧接着，他们进一步放宽了攻击者所需要掌握的关于目标模型结构和数据分布的先验知识，提出了一种数据迁移攻击。在这种攻击模式下，攻击者用于训练影子模型的数据与目标模型的训练数据来自两个不同的分布。研究发现在基于两个不同分布构建的攻击训练数据集中，成员和非成员数据各自有聚类现象。</p>
<p>Long等人 <span id="id52">(<a class="reference internal" href="../chapter_references/zreferences.html#id205" title="Long, Y., Wang, L., Bu, D., Bindschaedler, V., Wang, X., Tang, H., … Chen, K. (2020). A pragmatic approach to membership inferences on machine learning models. IEEE European Symposium on Security and Privacy (pp. 521–534).">Long <em>et al.</em>, 2020</a>)</span>
研究了未过拟合人工智能模型的成员推理攻击，发现了除过拟合外成员推理攻击的另一诱因。具体来讲，某些样本在训练阶段会对机器学习模型有着独特的影响，导致模型对这些样本产生了独特的记忆。实验表明，即使在目标模型良好泛化（训练和测试准确率差距小于1%）的情况下，攻击者也可以推断出易感数据样本是否为“成员”。</p>
<p>此外，Nasr等人 <span id="id53">(<a class="reference internal" href="../chapter_references/zreferences.html#id213" title="Nasr, M., Shokri, R., &amp; Houmansadr, A. (2019). Comprehensive privacy analysis of deep learning: passive and active white-box inference attacks against centralized and federated learning. IEEE Symposium on Security and Privacy (pp. 739–753).">Nasr <em>et al.</em>, 2019</a>)</span>
提出的白盒影子模型攻击实际上利用了目标模型的中间层结果以及预测损失作为附加特征来提升攻击成功率。对此，Leino和Fredrikson
<span id="id54">(<a class="reference internal" href="../chapter_references/zreferences.html#id214" title="Leino, K., &amp; Fredrikson, M. (2020). Stolen memories: leveraging model memorization for calibrated $\$White-Box$\$ membership inference. USENIX Security Symposium (pp. 1605–1622).">Leino and Fredrikson, 2020</a>)</span>
指出白盒影子模型攻击中的模型及数据太过透明，攻击者对目标模型有完全访问权限并掌握很大一部分目标模型的训练数据集，这在实际应用中并不多见，偏离了成员推理攻击的初始假设。对此，作者提出了一种新的白盒假设：攻击者对目标模型有完全访问权限但不知道目标模型的训练数据集。他们认为目标模型的训练数据与数据池之间的特征分布差异可以被用来进行成员推理攻击。</p>
<p>除了分类模型，影子模型攻同样适用于<em>生成模型</em>。生成模型多被用于在无监督学习下生成与训练数据分布尽可能相近的数据。Hayes等人
<span id="id55">(<a class="reference internal" href="../chapter_references/zreferences.html#id155" title="Hayes, J., Melis, L., Danezis, G., &amp; De Cristofaro, E. (2019). Logan: membership inference attacks against generative models. Privacy Enhancing Technologies, 2019(1), 133–152.">Hayes <em>et al.</em>, 2019</a>)</span>
提出了第一个针对生成模型的成员推理攻击，其原理是生成对抗网络（GAN）的判别器（discriminator）被训练来学习训练数据与生成数据之间的统计差异，因此会以更高置信度输出训练数据（即“成员”）而以相对降低的置信度输出生成数据（即“非成员”）。作者分别研究了白盒威胁模型和黑盒威胁模型下的攻击方法。在白盒威胁模型下，攻击者对目标模型具有完全访问权限，故可以先将所有目标数据（要攻击的样本）输入目标鉴别器，以得到能够反映对应样本属于原始训练数据的置信度。对鉴别器的输出概率向量进行降序排序，排序靠前的数据则极有可能为“成员”。值得注意的是，在白盒模型下，此攻击并不需要训练影子模型，因为鉴别器所提供的的信息已足够。而在黑盒威胁模型下，攻击者仍然需要训练<em>影子生成模型</em>，并使其模仿目标生成模型的表现。其中，影子生成模型的训练数据靠目标模型生成器（generator）来生成。影子生成模型训练完毕后，剩余攻击步骤与白盒威胁模型类似。此外，Hayes等人
<span id="id56">(<a class="reference internal" href="../chapter_references/zreferences.html#id155" title="Hayes, J., Melis, L., Danezis, G., &amp; De Cristofaro, E. (2019). Logan: membership inference attacks against generative models. Privacy Enhancing Technologies, 2019(1), 133–152.">Hayes <em>et al.</em>, 2019</a>)</span>
将要攻击的目标数据限定为与训练数据（欧式距离）相近的数据，因为当目标数据为训练数据或生成数据时才能达到较高攻击准确率。而Liu等人
<span id="id57">(<a class="reference internal" href="../chapter_references/zreferences.html#id206" title="Liu, G., Wang, C., Peng, K., Huang, H., Li, Y., &amp; Cheng, W. (2019). Socinf: membership inference attacks on social media health data with machine learning. IEEE Transactions on Computational Social Systems, 6(5), 907–921.">Liu <em>et al.</em>, 2019</a>)</span>
则不在目标数据上做限制，他们同样在黑盒威胁模型下构建影子生成模型，但是利用<em>影子鉴别器</em>（shadow
discriminator）来进行攻击。</p>
</div>
<div class="section" id="id58">
<h4><span class="section-number">4.2.1.2. </span>指标指导攻击<a class="headerlink" href="#id58" title="Permalink to this heading">¶</a></h4>
<p>与影子模型攻击不同，指标指导的成员推理攻击（metric-guided membership
inference
attack）通过预先指定的统计指标来检测“成员”样本，跟异常检测（anomaly
detection）类似。
其中，成员检测指标大都根据目标模型的概率输出计算得到，然后将指标与预设的阈值做比较，以此作为判断某样本是否为成员的依据。已有成员检测指标大致可分为四类：<em>预测正确性</em>（prediction
correctness）、<em>预测损失</em>（prediction
loss）、<em>预测置信度</em>（prediction
confidence）和<em>预测熵</em>（prediction
entropy）。我们将攻击定义为推理函数<span class="math notranslate nohighlight">\(\mathcal{M}(\cdot)\)</span>，其输出<span class="math notranslate nohighlight">\(1\)</span>或<span class="math notranslate nohighlight">\(0\)</span>分别代表“成员”和“非成员”。</p>
<p><strong>预测正确性。</strong>
一种最简单的攻击方式就是认为“只要预测成功了就是成员样本”，这里假设模型无法向测试集泛化，所有预测成功的都是训练样本。对应的判定函数为：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4metriccorr">
<span class="eqno">(4.2.6)<a class="headerlink" href="#equation-equ-chap4metriccorr" title="Permalink to this equation">¶</a></span>\[\mathcal{M}(  p( x ), y  ) = \mathbb{1}[y = \mathop{\mathrm{arg\,max}}_{i}  p_i( x)],\]</div>
<p>其中，<span class="math notranslate nohighlight">\(p_i( x)\)</span>为模型输出概率响亮的第<span class="math notranslate nohighlight">\(i\)</span>维（对应第<span class="math notranslate nohighlight">\(i\)</span>个类别），<span class="math notranslate nohighlight">\(\mathbb{1}[\cdot]\)</span>为指示器函数（indicator
function）。
这种推理攻击的攻击效果跟目标模型的基本性能相关，在某些很少见的情况下效果其实并不差，比如测试准确率很低的困难问题或者问题本身不难但是测试集里包含很难的样本等。这里举个简单的例子，使用机器学习模型预测股票价格，如果能连续预测正确则大概率是训练过的数据。
但是对于相对简单的任务，目标模型本身就具有不错的泛化性能，那么这种攻击方式将会失效，只能大概率确定分不对的样本是“非成员”而无法说明分对的样本就是“成员”。</p>
<p><strong>预测损失。</strong> Yeom等人 <span id="id59">(<a class="reference internal" href="../chapter_references/zreferences.html#id210" title="Yeom, S., Giacomelli, I., Fredrikson, M., &amp; Jha, S. (2018). Privacy risk in machine learning: analyzing the connection to overfitting. IEEE Computer Security Foundations Symposium (pp. 268–282).">Yeom <em>et al.</em>, 2018</a>)</span>
在其工作中建立了成员推理攻击、属性推理攻击以及过拟合之间的理论关联，并提出借助模型在训练数据和测试数据上的<em>平均损失大小差异</em>进行成员推理攻击。此攻击思想对应机器学习训练与测试数据点之间的分布偏差（由采样引起）。
Yeom等人将成员<em>判定阈值</em>设为所有训练样本的<em>平均损失</em>，对应的推理函数如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4metricloss">
<span class="eqno">(4.2.7)<a class="headerlink" href="#equation-equ-chap4metricloss" title="Permalink to this equation">¶</a></span>\[\mathcal{M}(  p( x ), y  )  =  \mathbb{1}[\mathcal{L}( p( x), y )  \leq \tau ],\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\mathcal{L}( \cdot, \cdot )\)</span>为损失函数（如交叉熵损失），<span class="math notranslate nohighlight">\(\tau\)</span>为所有训练样本的平均损失。</p>
<p><strong>预测置信度。</strong> Salem等人在其工作 <span id="id60">(<a class="reference internal" href="../chapter_references/zreferences.html#id203" title="Salem, A., Zhang, Y., Humbert, M., Fritz, M., &amp; Backes, M. (2019). Ml-leaks: model and data independent membership inference attacks and defenses on machine learning models. Network and Distributed Systems Security Symposium.">Salem <em>et al.</em>, 2019</a>)</span>
中除了提出放松假设的白盒影子模型攻击外，还提出了基于预测置信度以及预测熵的指标指导攻击。这里，高预测置信度指的是在预测向量中，其中一个维度的概率远大于其他概率。目标模型的训练会让训练样本的最大概率无限接近于1，因此可以用<em>最大置信度</em>作为指标来检测“成员”样本。推理函数可定义如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4metricconf">
<span class="eqno">(4.2.8)<a class="headerlink" href="#equation-equ-chap4metricconf" title="Permalink to this equation">¶</a></span>\[\mathcal{M}(  p( x ), y  )  =  \mathbb{1}[\max p( x)  \geq \tau ]\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\tau\)</span>为一个预设的接近1的阈值。</p>
<p><strong>预测熵。</strong> Salem等人 <span id="id61">(<a class="reference internal" href="../chapter_references/zreferences.html#id203" title="Salem, A., Zhang, Y., Humbert, M., Fritz, M., &amp; Backes, M. (2019). Ml-leaks: model and data independent membership inference attacks and defenses on machine learning models. Network and Distributed Systems Security Symposium.">Salem <em>et al.</em>, 2019</a>)</span>
还提出了以预测熵为指标的成员推理攻击。
模型输出概率向量的熵衡量了模型在不同类别上的确信程度，低熵说明模型确信的指向某个类别，高熵说明模型在不同类别上犹豫不决。基于预测熵的推理函数可定义为：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4metricentro">
<span class="eqno">(4.2.9)<a class="headerlink" href="#equation-equ-chap4metricentro" title="Permalink to this equation">¶</a></span>\[\mathcal{M}(  p( x), y)  =  \mathbb{1}[\mathrm{H}( p( x)) \leq \tau ] =   \mathbb{1}[-\sum _{i} p_i\log p_i \leq \tau ].\]</div>
<p>Song和Mittal <span id="id62">(<a class="reference internal" href="../chapter_references/zreferences.html#id211" title="Song, L., &amp; Mittal, P. (2021). Systematic evaluation of privacy risks of machine learning models. USENIX Security Symposium (pp. 2615–2632).">Song and Mittal, 2021</a>)</span>
认为预测熵应该和样本标签结合起来使用，并提出<em>修正预测熵</em>（modified
prediction
entropy）来提高上述基于标准预测熵的推理攻击。举例来说，如果目标模型以极高置信度错误分类某个样本，则它的预测熵将无限接近<span class="math notranslate nohighlight">\(0\)</span>，会被攻击者检测为“成员”，而实际上可能是目标模型无法分类的“非成员”样本。考虑了此问题的修正预测熵定义如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4mentropy">
<span class="eqno">(4.2.10)<a class="headerlink" href="#equation-equ-chap4mentropy" title="Permalink to this equation">¶</a></span>\[\mathrm{MH}( p( x), y) = -( 1- p_y )\log p_y -\sum _{i\neq y} p_i\log(1-  p_i),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(i \neq y\)</span>在概率向量中排除了样本<span class="math notranslate nohighlight">\(x\)</span>本身的类别<span class="math notranslate nohighlight">\(y\)</span>。将式
<a class="reference internal" href="#equation-equ-chap4metricentro">(4.2.9)</a>
中的预测熵替换为修正预测熵便得到基于修正预测熵的推理函数。</p>
</div>
<div class="section" id="id63">
<h4><span class="section-number">4.2.1.3. </span>联邦推理攻击<a class="headerlink" href="#id63" title="Permalink to this heading">¶</a></h4>
<p>近年来，联邦学习（FL）打开了多方协作训练的新范式，参与方在不共享数据只共享梯度（或参数）的情况下，综合各方优势共同训练一个强大的全局模型。
前面介绍的成员推理攻击都是在传统集中式学习范式下的攻击，然而联邦学习的多方协作难免会泄露一些特殊的隐私信息，因为毕竟信息（虽然不是数据本身）还是从参与者汇聚到了全局模型。</p>
<p>Melis等人 <span id="id64">(<a class="reference internal" href="../chapter_references/zreferences.html#id149" title="Melis, L., Song, C., De Cristofaro, E., &amp; Shmatikov, V. (2019). Exploiting unintended feature leakage in collaborative learning. IEEE Symposium on Security and Privacy (pp. 691–706).">Melis <em>et al.</em>, 2019</a>)</span>
提出了第一个针对联邦学习的成员推理攻击。此工作以文本分类为例，目标模型是一个包含词嵌入（word
embedding）层的递归神经网络。词嵌入层通过一个嵌入矩阵（embedding
matrix）将输入投影为低维向量表达，而这个嵌入矩阵会被视作模型的全局变量，在联邦学习的各方共同参与下进行优化。
词嵌入层的梯度是稀疏且与输入词有对应关系的。也就是说，对一批输入文本来说，词嵌入只会根据文本中存在的词进行更新，而其余词（文本中不存在的词）的梯度则为零。攻击者可以利用词嵌入更新的这种特性，通过观察<em>非零梯度</em>来推断一个文本样本是否为“成员”。</p>
<p>Truex等人 <span id="id65">(<a class="reference internal" href="../chapter_references/zreferences.html#id375" title="Truex, S., Liu, L., Gursoy, M. E., Yu, L., &amp; Wei, W. (2019). Demystifying membership inference attacks in machine learning as a service. IEEE Transactions on Services Computing.">Truex <em>et al.</em>, 2019</a>)</span>
则提出了一种针对<em>异构联邦学习</em>的成员推理攻击。在异构联邦学习框架下，参与者利用本地私有数据来训练本地模型，在新样本到来时，参与者与他参与方共享本地模型对新样本的预测向量。异构联邦学习的特点是各方数据不互通且重叠度（overlap）较小，因而不同本地模型之间的决策边界差异很大。
对联邦中的恶意参与者来说，这种<em>决策边界差异</em>就可以用来推理某样本是否存在与其他参与方的私有数据中。</p>
</div>
</div>
<div class="section" id="id66">
<h3><span class="section-number">4.2.2. </span>属性推理攻击<a class="headerlink" href="#id66" title="Permalink to this heading">¶</a></h3>
<p>属性推理攻击（attribute inference attack，AIA）来源于模型逆向攻击（model
inversion
attack），是针对<em>个体属性</em>的隐私攻击。攻击者基于已发布的目标模型，从给定样本的非敏感属性中推断其敏感属性。
最初，Fredrikson等人 <span id="id67">(<a class="reference internal" href="../chapter_references/zreferences.html#id371" title="Fredrikson, M., Lantz, E., Jha, S., Lin, S., Page, D., &amp; Ristenpart, T. (2014). Privacy in pharmacogenetics: an $\$End-to-End$\$ case study of personalized warfarin dosing. USENIX Security Symposium (pp. 17–32).">Fredrikson <em>et al.</em>, 2014</a>)</span>
在2014年通过逆向药物剂量预测模型来推断关于患者的敏感属性。他们根据目标模型输出的华法林预测剂量和患者的非敏感属性（如身高、年龄、体重等）推理得到该患者的基因组信息。具体的，作者将这种攻击形式化为最大化敏感属性的后验概率估计（posteriori
probability
estimate）。攻击者假设每个患者（数据样本）的特征向量中的某个特征为<em>敏感属性</em>，而其他特征为<em>非敏感属性</em>。给定非敏感属性和模型输出，攻击者通过<em>后验概率最大化</em>（maximum
a posteriori，MAP）来最大化敏感属性的后验概率。</p>
<p>上述工作研究的目标模型为简单的线性回归模型（linear
regression），在后续的研究中，Fredrikson等人
<span id="id68">(<a class="reference internal" href="../chapter_references/zreferences.html#id277" title="Fredrikson, M., Jha, S., &amp; Ristenpart, T. (2015). Model inversion attacks that exploit confidence information and basic countermeasures. ACM SIGSAC Conference on Computer and Communications Security (pp. 1322–1333).">Fredrikson <em>et al.</em>, 2015</a>)</span> 又将目光投向了深度学习模型。
给定人名，攻击者利用目标模型的输出（预测概率向量）来攻击基于深度神经网络的人脸识别模型，推理出该人物的人脸敏感特征。在这个工作中，Fredrikson等人将属性推理攻击转化为一个优化问题：找出一个输入人脸图像，其能让目标模型以最大概率预测给定人名。
需要注意的是，虽然通过此攻击可以得到一个能迷惑神经网络的（合成）人脸图像，但是这个图像并不属于目标模型的训练数据集，而更像是一个特征聚合的结果。</p>
<p>在Pan等人 <span id="id69">(<a class="reference internal" href="../chapter_references/zreferences.html#id372" title="Pan, X., Zhang, M., Ji, S., &amp; Yang, M. (2020). Privacy risks of general-purpose language models. IEEE Symposium on Security and Privacy (pp. 1314–1331).">Pan <em>et al.</em>, 2020</a>)</span>
的工作中，研究者将个体属性推理攻击扩展到自然语言处理模型。通用语言模型（如谷歌的BERT，OpenAI的GPT-3）将文本转化为嵌入向量，在自然语言处理中起到了至关重要的作用。然而，Pan等人发现攻击者可以训练一个攻击模型，在只知道通用语言模型输出向量的情况下（对原文本一无所知）找到原文本的关键词（也就是敏感属性）。
举例来说，现在很多医疗机构都在构建<em>全自动预诊断</em>机制，以此来提高患者的就诊效率，其中往往会使用通用语言模型来生成病历的词嵌入向量。攻击者在得到这些嵌入向量之后，便可以推理患者的疾病类型甚至是病灶，泄露病人的隐私。</p>
<p>另外，属性推理攻击在图神经网络（graph neural
network，GNN）中也具有一定的威胁性。2021年，He等人
<span id="id70">(<a class="reference internal" href="../chapter_references/zreferences.html#id374" title="He, X., Jia, J., Backes, M., Gong, N. Z., &amp; Zhang, Y. (2021). Stealing links from graph neural networks. USENIX Security Symposium (pp. 2669–2686).">He <em>et al.</em>, 2021</a>)</span>
最先提出针对边的推理攻击。他们在黑盒威胁模型下对图神经网络进行攻击，推理两个节点之间是否存在边。推理的大致思想是，图神经网络通过聚合邻居节点信息来计算该节点的嵌入向量，那么嵌入向量距离近的节点间大概率会存在连接。实际上，Duddu等人
<span id="id71">(<a class="reference internal" href="../chapter_references/zreferences.html#id373" title="Duddu, V., Boutet, A., &amp; Shejwalkar, V. (2020). Quantifying privacy leakage in graph embedding. EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services (pp. 76–85).">Duddu <em>et al.</em>, 2020</a>)</span>
在2020年就系统研究了三种对图神经网络的攻击：1）成员推理攻击，攻击者利用图嵌入（白盒攻击）或者模型输出（黑盒攻击）推理某个用户节点是否存在于目标模型的训练数据集中；2）图重构和边推理攻击，通过图嵌入和自动编码器逆向图的结构信息（邻接矩阵），并基于此推理攻击节点间的连接情况；3）属性推理攻击，攻击者推理有关用户敏感信息的节点属性。作者假设攻击者拥有从同一数据池采样的（但与目标数据不相交）图，这样就可以利用影子模型（章节
<a class="reference internal" href="#sec-shadow-model-attack"><span class="std std-numref">图4.2.2</span></a>
）来进行攻击。虽然这几种推理攻击的成功率方差很大，很多时候成功率也不高，但是这些工作揭示了图神经网络切实存在的隐私泄露风险，是很重要的一个开始。</p>
</div>
<div class="section" id="id72">
<h3><span class="section-number">4.2.3. </span>其他推理攻击<a class="headerlink" href="#id72" title="Permalink to this heading">¶</a></h3>
<p>以智能手机、智能音箱、智能冰箱等为代表的智能家居设备、物联网（Internet
of
things，IoT）都是隐私攻击的重点对象。例如，无处不在的麦克风可能随时都在“监听”用户的一举一动，说话者的声音、音量及表达方式都有可能暴露他/她们的姓名、性别、年龄、位置、健康状况、醉酒程度、疲劳情况等个人隐私信息。早在2014年，Bone等人
<span id="id73">(<a class="reference internal" href="../chapter_references/zreferences.html#id133" title="Bone, D., Li, M., Black, M. P., &amp; Narayanan, S. S. (2014). Intoxicated speech detection: a fusion framework with speaker-normalized hierarchical functionals and gmm supervectors. Computer Speech &amp; Language, 28(2), 375–391.">Bone <em>et al.</em>, 2014</a>)</span>
便利用语言错误（如难以理解的单词数量、中断、犹豫等特征）和节奏特征来推断说话人是否醉酒。2015年，Schuller等人
<span id="id74">(<a class="reference internal" href="../chapter_references/zreferences.html#id136" title="Schuller, B., Steidl, S., Batliner, A., Nöth, E., Vinciarelli, A., Burkhardt, F., … others. (2015). A survey on perceived speaker traits: personality, likability, pathology, and the first challenge. Computer Speech &amp; Language, 29(1), 100–131.">Schuller <em>et al.</em>, 2015</a>)</span>
通过综合调研，分析说明语速、音量、表达特征等语音数据可以被用来推断人格特征。2017年，Cummins等人
<span id="id75">(<a class="reference internal" href="../chapter_references/zreferences.html#id135" title="Cummins, N., Schmitt, M., Amiriparian, S., Krajewski, J., &amp; Schuller, B. (2017). “you sound ill, take the day off”: automatic recognition of speech affected by upper respiratory tract infection. IEEE Engineering in Medicine and Biology Society (pp. 3806–3809).">Cummins <em>et al.</em>, 2017</a>)</span>
利用声音沙哑程度及咳嗽、吸鼻子的频率来判断说话人是否感冒或喉咙痛。2018年，Jin和Wang
<span id="id76">(<a class="reference internal" href="../chapter_references/zreferences.html#id134" title="Jin, H., &amp; Wang, S. (2018 , October 9). Voice-based determination of physical and emotional characteristics of users. US Patent 10,096,319.">Jin and Wang, 2018</a>)</span>
利用音量、音调变化来识别说话人的情绪，包括愤怒、同情、厌恶、快乐、惊讶等。</p>
<p>虽然单个样本所包含的信息有限，但当数百万条样本汇集起来，攻击者就可以从中分析得到很多重要的知识。因此，具有上百万甚至千万用户的在线社交网络平台（以及未来的元宇宙）都可能会成为个人信息泄露的主要场所。社交网络中的用户档案、交互记录、帖子内容等公开数据可能会被用来推断用户的性别、种族、年龄、位置信息、经济状况、政治兴趣等等。
近来，国外已经出现了专门针对社交网络的个人隐私攻击，一些非法公司通过分析社交网络获取用户不愿透漏的隐私信息，并将这些信息打包出售获得高利。
除社交网络外，开放数据集也有可能泄露个人隐私。随着人工智能的快速发展，大规模数据集的构建和开源越来越频繁，政府、公司、组织间共享数据、整合数据优势的行为也越来越普遍，给隐私攻击留下大量的探索空间。</p>
</div>
</div>
<div class="section" id="sec-dataextractionattack">
<span id="id77"></span><h2><span class="section-number">4.3. </span>数据窃取<a class="headerlink" href="#sec-dataextractionattack" title="Permalink to this heading">¶</a></h2>
<p><em>数据窃取攻击</em>（data stealing
attack）从已训练模型中逆向得到模型的原始训练数据，所以也称为<em>数据抽取攻击</em>（data
extraction attack）或<em>模型逆向攻击</em>（model inversion attack）。图
<a class="reference internal" href="#fig-chap4datastealing"><span class="std std-numref">图4.3.1</span></a>
展示了数据窃取攻击的目的，通常情况下我们利用训练好的模型进行正常推理任务，但是数据窃取攻击者会尝试从模型中逆向出原始训练数据。当前数据窃取攻击针对的主要是深度学习模型研究，利用的是模型在训练过程中记忆的训练数据。数据窃取攻击所带来的威胁是多方面的，会导致私有数据的泄露、知识产权的侵犯等，比上一章节介绍的隐私攻击威胁更大。对数据持有者来说，他们通常花费巨大的代价来收集和标注私有数据，一旦泄露则会导致财产损失，严重时甚至会威胁国家安全。此外，数据的泄露也会破坏保密协定，而当被泄露的数据被用作其他非法目的时，更是会带来一系列附加危害。</p>
<div class="figure align-default" id="id149">
<span id="fig-chap4datastealing"></span><a class="reference internal image-reference" href="../_images/4.6_data_stealing.png"><img alt="../_images/4.6_data_stealing.png" src="../_images/4.6_data_stealing.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.3.1 </span><span class="caption-text">数据窃取攻击示意图</span><a class="headerlink" href="#id149" title="Permalink to this image">¶</a></p>
</div>
<p>2015年，Ateniese等人 <span id="id78">(<a class="reference internal" href="../chapter_references/zreferences.html#id401" title="Ateniese, G., Mancini, L. V., Spognardi, A., Villani, A., Vitali, D., &amp; Felici, G. (2015). Hacking smart machines with smarter ones: how to extract meaningful data from machine learning classifiers. International Journal of Security and Networks, 10(3), 137–150.">Ateniese <em>et al.</em>, 2015</a>)</span>
指出，如果攻击者可以访问比他们自己的模型表现更好的机器学习模型（如支持向量机、隐马尔科夫模型、神经网络等），那么就可以通过对目标模型的访问来推理有关训练数据的信息，进而用于改进自己的模型。在这项工作中，Ateniese等人关注的是能帮助攻击者提高自己模型性能的信息，而不是原始训练数据的泄露。</p>
<p>以此为启发，Song等人 <span id="id79">(<a class="reference internal" href="../chapter_references/zreferences.html#id400" title="Song, C., Ristenpart, T., &amp; Shmatikov, V. (2017). Machine learning models that remember too much. ACM SIGSAC Conference on Computer and Communications Security (pp. 587–601).">Song <em>et al.</em>, 2017</a>)</span>
首次提出了<em>模型记忆攻击</em>（model memorization
attack）的概念，通过（伪）正则化或者数据增强技术将训练数据记忆在深度神经网络里，从而可以在后续的步骤中以白盒或者黑盒的方式访问目标模型提取记忆的数据。
该工作假定攻击者为恶意服务提供商，在机器学习即服务（machine learning as
a
service，MLaaS）的设定下，在向用户提供训练数据增强技术、模型架构及训练编码等服务的同时将训练数据藏在模型参数中进行窃取。在白盒威胁模型下，攻击者可以访问目标模型参数，提取参数中记忆的训练数据。在这种情形下，攻击者可以直接修改训练算法编码，采用编码（encoding）技术将数据藏在模型<em>最不明显的比特</em>（least
significant
bits，LSB）中，亦或使用正则化技术将敏感数据与模型参数（或者参数的正负号）建立关联和记忆。在黑盒威胁模型下，攻击者只有模型的使用权（即输入输出访问权），所以可以使用预埋的恶意数据增广技术，在增广得到的新样本（称为<em>恶意增广样本</em>）的类别标签里隐藏原始训练数据的信息，一个恶意增广样本负责一个比特，从而在推理阶段可以通过恶意增广样本逐一提取这些比特，组合成泄露数据。此类攻击需要很强的假设，即攻击者能够自由执行修改后的训练算法和数据增广技术，而且能窃取的数据量相对较小，比如最核心的敏感信息。</p>
<p>上述攻击是一种恶意的“有意”记忆导致的信息泄露。相比“有意”记忆，更令人担忧的是深度学习模型“无意”或“意外”的记忆。Carlin等人
<span id="id80">(<a class="reference internal" href="../chapter_references/zreferences.html#id397" title="Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., &amp; Song, D. (2019). The secret sharer: evaluating and testing unintended memorization in neural networks. USENIX Security Symposium (pp. 267–284).">Carlini <em>et al.</em>, 2019</a>)</span> 将<em>意外记忆</em>（unintended
memorization）定义为训练有素的神经网络所暴露的分布外（out-of-distribution）训练数据，这些分布外训练数据与学习任务无关却被神经网络（意外地）记住了。</p>
<p><em>数据窃取攻击与上一章节介绍的属性推理攻击有什么不同？</em>
首先，数据窃取攻击与属性推理攻击都是重构还原（全部或者部分）训练数据的过程。区别在于，属性推理攻击的目标是得到使目标模型以最大概率输出特定类别的数据，这意味着其学习的是训练数据的聚合统计属性。如在攻击人脸识别模型的工作
<span id="id81">(<a class="reference internal" href="../chapter_references/zreferences.html#id277" title="Fredrikson, M., Jha, S., &amp; Ristenpart, T. (2015). Model inversion attacks that exploit confidence information and basic countermeasures. ACM SIGSAC Conference on Computer and Communications Security (pp. 1322–1333).">Fredrikson <em>et al.</em>, 2015</a>)</span>
中，攻击者最终得到的是<em>合成人脸图片</em>（被目标模型预测为预定类别）而非实际训练数据。而对于数据窃取攻击来说，它的目的是最大程度的<em>还原训练数据</em>。虽然属性推理攻击也在一定程度上泄露隐私信息，但是其所涉及的对象往往是单个隐私属性，而数据窃取往往泄露的是整个数据集，所造成的危害更大、影响范围更广。当然数据窃取攻击也比属性推理攻击更难实现。</p>
<p>根据攻击者所掌握的信息，数据窃取也可分为黑盒窃取和白盒窃取。概括来说，在黑盒威胁模型下，攻击者只可以与目标模型交互，得到攻击者给定输入的对应输出；而在白盒威胁模型下，攻击者可以访问目标模型的内部结构和模型参数。下面将从黑盒与白盒两个类别进一步介绍数据窃取攻击方法。</p>
<div class="section" id="id82">
<h3><span class="section-number">4.3.1. </span>黑盒数据窃取<a class="headerlink" href="#id82" title="Permalink to this heading">¶</a></h3>
<p>在黑盒数据窃取设定下，攻击者所掌握的信息并不多，只有模型的输出结果，所以只能攻击特定类型的模型，且只能窃取部分数据信息。一般来说，模型的输出维度越高就越容易受到黑盒数据窃取，比如生成模型、序列到序列模型等，这主要是因为输出维度越高暴漏的信息也就会越多。相反的，如果是对输入信息压缩很厉害的分类模型，则难以仅根据输出概率去窃取输入信息。此外，黑盒窃取能获得的信息也非常有限，目前的攻击只能获得与输入样本相关的一些信息。</p>
<p>现有黑盒数据窃取攻击主要针对<em>大语言模型</em>（large language
models）——为单词序列分配概率的统计模型。这类（开放的）语言模型是许多自然语言处理任务的基础，其往往使用非常大的模型架构，并在海量文本数据上进行训练。这种大规模学习的方式赋予了语言模型生成通畅自然语言的能力，被广泛应用于各种下游任务。训练大语言模型的数据集往往包含大量公开在互联网上的文本数据，这些数据经常（意外地）包含个人隐私信息（如身份证号码、手机号码、邮箱、家庭住址等），在面临窃取攻击时容易发生关联泄露（比如出现人名的时候也往往连带着电话号码或家庭住址）。</p>
<div class="figure align-default" id="id150">
<span id="fig-chap4dataextraction"></span><a class="reference internal image-reference" href="../_images/4.7_data_extraction.png"><img alt="../_images/4.7_data_extraction.png" src="../_images/4.7_data_extraction.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.3.2 </span><span class="caption-text">黑盒数据窃取示意图 <span id="id83">(<a class="reference internal" href="../chapter_references/zreferences.html#id398" title="Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … others. (2021). Extracting training data from large language models. USENIX Security Symposium (pp. 2633–2650).">Carlini <em>et al.</em>, 2021</a>)</span></span><a class="headerlink" href="#id150" title="Permalink to this image">¶</a></p>
</div>
<p>图 <a class="reference internal" href="#fig-chap4dataextraction"><span class="std std-numref">图4.3.2</span></a>
展示了一个黑盒数据窃取攻击自然语言处理模型的例子，当输入特定前缀文字后，语言模型自动返回了模型记忆的与前缀文字关联的其他隐私信息。这是由Carlin等人
<span id="id84">(<a class="reference internal" href="../chapter_references/zreferences.html#id397" title="Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., &amp; Song, D. (2019). The secret sharer: evaluating and testing unintended memorization in neural networks. USENIX Security Symposium (pp. 267–284).">Carlini <em>et al.</em>, 2019</a>)</span>
发现的模型的<em>意外记忆</em>。神经网络的设计本意是专注于学习与任务高度相关的数据而忽略任务无关的信息，然而Carlin等人的工作证明了意外记忆的存在和敏感信息泄露的可能性。</p>
<p>为了以一种可控的方式研究并量化模型的记忆，Carlin等人手动构建了与目标学习任务无关的“<em>先兆数据</em>”（canaries <a class="footnote-reference brackets" href="#id143" id="id85">1</a>）。通过定义并计算这些<em>先兆数据</em>在语言模型中的“曝光度”（exposure）可以定量的考核模型是否存在意外记忆以及记忆程度。先兆数据是从输入域中抽取的<em>独立随机序列</em>，随后以不同次数注入训练数据中。举例来说，给定一个格式“我的社会保障号码为<span class="math notranslate nohighlight">\(\ast \ast \ast \ast \ast \ast \ast \ast \ast\)</span>”（美国社会保障号码长度）和数据域<span class="math notranslate nohighlight">\(0-9\)</span>，每一个<span class="math notranslate nohighlight">\(\ast\)</span>都是从数据域<span class="math notranslate nohighlight">\(0-9\)</span>中随机选取的，一个简单的例子为“我的社会保障号码为281265017”。实验同时需要设计对照数据，如与先兆数据只相差一位数的“我的社会保障号码为281265018”。Carlin等人将先兆数据注入一个<em>神经网络机器翻译</em>（neural
machine
translation，NMT）模型中发现，有时候先兆数据只需注入一次就会让模型记住它，导致信息被推理出的概率大大升高。</p>
<div class="figure align-default" id="id151">
<span id="fig-chap4nlpdataextraction"></span><a class="reference internal image-reference" href="../_images/4.8_trainingDataExtraction.png"><img alt="../_images/4.8_trainingDataExtraction.png" src="../_images/4.8_trainingDataExtraction.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.3.3 </span><span class="caption-text">黑盒数据窃取流程图 <span id="id86">(<a class="reference internal" href="../chapter_references/zreferences.html#id398" title="Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … others. (2021). Extracting training data from large language models. USENIX Security Symposium (pp. 2633–2650).">Carlini <em>et al.</em>, 2021</a>)</span></span><a class="headerlink" href="#id151" title="Permalink to this image">¶</a></p>
</div>
<p>在的后续工作中，Carlini等人 <span id="id87">(<a class="reference internal" href="../chapter_references/zreferences.html#id398" title="Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … others. (2021). Extracting training data from large language models. USENIX Security Symposium (pp. 2633–2650).">Carlini <em>et al.</em>, 2021</a>)</span>
将数据窃取的设定进行了一定的泛化。之前的工作假定攻击者知道他们想要“窃取”的目标数据（如社会保障号码），是一种<em>定向攻击</em>。在新的工作中，他们假设攻击者的攻击目的是尽可能多地从模型中窃取训练数据，是一种<em>普适攻击</em>。
在普适攻击设定下，攻击者可以使用成员推理攻击来辅助黑盒数据窃取。
此攻击改进了文本的生成方式，使生成的文本更多样化，同时以混淆值（perplexity）作为成员推理的基准指标，并结合超参数调整和额外互联网数据（与GPT-2类似的前缀数据），力求从模型中生成出更多样化的样本。混淆值的定义如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4perplexity">
<span class="eqno">(4.3.1)<a class="headerlink" href="#equation-equ-chap4perplexity" title="Permalink to this equation">¶</a></span>\[\mathcal{P} = \exp\Big( -\frac{1}{n}\sum _{i=1}^n \log f_\theta (  x_i| x_1,\ldots, x_{i-1} )\Big),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(f_\theta\)</span>为目标语言模型，是概率生成模型的一种。对一个序列<span class="math notranslate nohighlight">\(\{ x_1,\ldots, x_i\}\)</span>计算混淆值，混淆值高表示目标模型对给定序列的出现是“惊讶的”，而混淆值低意味着给定序列对目标模型来说是一个普通的（意料之中的）序列。</p>
<p>如图 <a class="reference internal" href="#fig-chap4nlpdataextraction"><span class="std std-numref">图4.3.3</span></a>
所示，研究者在大语言模型GPT-2及其变体上进行了实验验证。给定一个模型，先生成20万条文本，然后通过6个混淆值指标对文本进行排序（各自独立排序），去除重复并（以网上搜索确认的方式）人工检查前100条记录。最终，通过与数据拥有方OpenAI进行确认，研究者成功逆向了604条训练文本，其中包括100条新闻报道、79条日志或者错误报告记录、32条私人通讯信息（如地址、电话、邮箱和Twitter账户）等。更让人担忧的是，即使包含上述信息的文档在训练数据中只出现一次，攻击也能有可能成功。</p>
</div>
<div class="section" id="id88">
<h3><span class="section-number">4.3.2. </span>白盒窃取<a class="headerlink" href="#id88" title="Permalink to this heading">¶</a></h3>
<p>白盒窃取攻击对目标模型具有完全访问权限，可以获得模型结构和参数，并基于此从目标模型中窃取训练数据。在这种情况下，攻击者往往利用梯度信息进行数据窃取，因此此类攻击也被称为<em>梯度逆向攻击</em>（gradient
inversion attack）。
由于联邦学习在设计上需要各方共享模型参数或梯度信息，这使其更容易遭受白盒数据窃取攻击。实际上，白盒数据窃取攻击也大都以联邦学习范式为主要攻击目标。根据优化目标的不同,梯度逆向攻击可以分为两类：<em>迭代梯度逆向</em>（iterative
gradient inversion）和<em>递归梯度逆向</em>（recursive gradient
inversion）。迭代梯度逆向攻击通过迭代来步步缩小生成梯度与真实梯度（各方共享梯度）之间的差异；而递归梯度逆向攻击则对神经网络从后往前逐层递归优化，得到每层的最优输入。</p>
<div class="figure align-default" id="id152">
<span id="fig-chap4iterativegradientinversionattack"></span><a class="reference internal image-reference" href="../_images/4.9_iterative_grad_attack.png"><img alt="../_images/4.9_iterative_grad_attack.png" src="../_images/4.9_iterative_grad_attack.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.3.4 </span><span class="caption-text">迭代梯度逆向攻击流程图 <span id="id89">(<a class="reference internal" href="../chapter_references/zreferences.html#id413" title="Zhang, R., Guo, S., Wang, J., Xie, X., &amp; Tao, D. (2022). A survey on gradient inversion: attacks, defenses and future directions. International Joint Conference on Artificial Intelligence.">Zhang <em>et al.</em>, 2022</a>)</span></span><a class="headerlink" href="#id152" title="Permalink to this image">¶</a></p>
</div>
<p><strong>迭代梯度逆向攻击</strong>的思想是通过生成。
一种经典的<em>迭代梯度逆向攻击</em>方法是Zhu等人 <span id="id90">(<a class="reference internal" href="../chapter_references/zreferences.html#id412" title="Zhu, L., Liu, Z., &amp; Han, S. (2019). Deep leakage from gradients. Advances in Neural Information Processing Systems, 32.">Zhu <em>et al.</em>, 2019</a>)</span>
提出的<em>深度梯度泄露</em>（deep leakage from gradients，DLG）攻击。如图
<a class="reference internal" href="#fig-chap4iterativegradientinversionattack"><span class="std std-numref">图4.3.4</span></a>
所示，迭代梯度逆向攻击主要包括三个部分：伪数据初始化、梯度求导和梯度匹配。攻击者首先初始化一个随机样本<span class="math notranslate nohighlight">\(x'\)</span>和随机标签<span class="math notranslate nohighlight">\(y'\)</span>，然后通过对伪数据<span class="math notranslate nohighlight">\(x'\)</span>进行优化来还原初始训练数据。具体的，将<span class="math notranslate nohighlight">\(x'\)</span>输入模型，经过正向及反向传播得到模型每一层参数的<em>伪梯度</em><span class="math notranslate nohighlight">\(\{\nabla \theta'_i=(\nabla W'_i, \nabla b'_i)\}_{i=1}^{L}\)</span>（<span class="math notranslate nohighlight">\(L\)</span>为神经网络层数）。然后，迭代最小化伪梯度<span class="math notranslate nohighlight">\(\nabla \theta'_i\)</span>与真实梯度<span class="math notranslate nohighlight">\(\nabla \theta_i=(\nabla W_i, \nabla b_i)\)</span>之间的距离，使生成数据趋近目标模型的训练数据。将上述步骤迭代数次并收敛后即可得到无限近似原始训练数据的合成数据，优化过程定义如下：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4dlg">
<span class="eqno">(4.3.2)<a class="headerlink" href="#equation-equ-chap4dlg" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} x^{*},y^{*} = \min_{ x',y'} \| \nabla \theta' - \nabla \theta \|_2\\ = \mathop{\mathrm{arg\,min}}\limits_{ x',y'} \| \frac{\partial \mathcal{L(\mathcal{F}(  x',\theta ), y')}}{\partial\theta} - \nabla \theta \|_2,\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\| \cdot\|_2\)</span>表示<span class="math notranslate nohighlight">\(L_2\)</span>范数，<span class="math notranslate nohighlight">\(x^{*}\)</span>、<span class="math notranslate nohighlight">\(y^{*}\)</span>分别表示优化得到的窃取样本和标签。
大部分伪数据的初始化选用高斯噪声 <span id="id91">(<a class="reference internal" href="../chapter_references/zreferences.html#id415" title="Yin, H., Mallya, A., Vahdat, A., Alvarez, J. M., Kautz, J., &amp; Molchanov, P. (2021). See through gradients: image batch recovery via gradinversion. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16337–16346).">Yin <em>et al.</em>, 2021</a>, <a class="reference internal" href="../chapter_references/zreferences.html#id412" title="Zhu, L., Liu, Z., &amp; Han, S. (2019). Deep leakage from gradients. Advances in Neural Information Processing Systems, 32.">Zhu <em>et al.</em>, 2019</a>)</span>
，也有部分工作 <span id="id92">(<a class="reference internal" href="../chapter_references/zreferences.html#id416" title="Jin, X., Chen, P.-Y., Hsu, C.-Y., Yu, C.-M., &amp; Chen, T. (2021). Cafe: catastrophic data leakage in vertical federated learning. Advances in Neural Information Processing Systems, 34, 994–1006.">Jin <em>et al.</em>, 2021</a>, <a class="reference internal" href="../chapter_references/zreferences.html#id414" title="Zhao, B., Mopuri, K. R., &amp; Bilen, H. (2020). Idlg: improved deep leakage from gradients. arXiv preprint arXiv:2001.02610.">Zhao <em>et al.</em>, 2020</a>)</span>
选择均匀分布噪音。在迭代梯度逆向的过程中，伪样本和伪标签会在模型训练过程中一起迭代升级。但Zhao等人
<span id="id93">(<a class="reference internal" href="../chapter_references/zreferences.html#id414" title="Zhao, B., Mopuri, K. R., &amp; Bilen, H. (2020). Idlg: improved deep leakage from gradients. arXiv preprint arXiv:2001.02610.">Zhao <em>et al.</em>, 2020</a>)</span>
发现，深度梯度泄露攻击无法保证同时完成<em>模型收敛</em>和<em>正确标签窃取</em>两个任务，因此对深度梯度泄露攻击提出了改进，预先从真实梯度中窃取正确标签，然后只对伪样本进行优化。这一做法有效降低了计算复杂度，提高了窃取效率。</p>
<p>对普通模型训练来说，基于批的训练有助于减少迭代次数，减缓累积误差造成的波动。但是对于数据窃取来说，基于批的训练会造成梯度混杂，给窃取增加了难度。实际上，从混杂梯度中提取梯度和目标数据等同于做<em>平均和分解</em>（decomposition
of averaged
summation）。深度梯度泄露攻击使用的普通优化策略可以计算批大小最大为8的混杂梯度。而Jin等人
<span id="id94">(<a class="reference internal" href="../chapter_references/zreferences.html#id416" title="Jin, X., Chen, P.-Y., Hsu, C.-Y., Yu, C.-M., &amp; Chen, T. (2021). Cafe: catastrophic data leakage in vertical federated learning. Advances in Neural Information Processing Systems, 34, 994–1006.">Jin <em>et al.</em>, 2021</a>)</span>
借助正则化项，成功的从批大小为100的混杂梯度中窃取到了数据。此外，如公式
<a class="reference internal" href="#equation-equ-chap4dlg">(4.3.2)</a>
所示，深度梯度泄露攻击在计算伪梯度与真实梯度之间的距离时使用了<span class="math notranslate nohighlight">\(L_2\)</span>范数。然而，Geiping等人
<span id="id95">(<a class="reference internal" href="../chapter_references/zreferences.html#id467" title="Geiping, J., Bauermeister, H., Dröge, H., &amp; Moeller, M. (2020). Inverting gradients-how easy is it to break privacy in federated learning? Advances in Neural Information Processing Systems, 33, 16937–16947.">Geiping <em>et al.</em>, 2020</a>)</span>
研究发现，对数据窃取攻击来说，梯度的方向比大小更重要，进而使用余弦相似度（cosine
similarity）来代替<span class="math notranslate nohighlight">\(L_2\)</span>距离，取得了更好的窃取效果。</p>
<div class="figure align-default" id="id153">
<span id="fig-chap4recursivegradientinversionattack"></span><a class="reference internal image-reference" href="../_images/4.10_recursive_grad_attack.png"><img alt="../_images/4.10_recursive_grad_attack.png" src="../_images/4.10_recursive_grad_attack.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.3.5 </span><span class="caption-text">递归梯度逆向攻击流程图 <span id="id96">(<a class="reference internal" href="../chapter_references/zreferences.html#id413" title="Zhang, R., Guo, S., Wang, J., Xie, X., &amp; Tao, D. (2022). A survey on gradient inversion: attacks, defenses and future directions. International Joint Conference on Artificial Intelligence.">Zhang <em>et al.</em>, 2022</a>)</span></span><a class="headerlink" href="#id153" title="Permalink to this image">¶</a></p>
</div>
<p><strong>递归梯度逆向攻击</strong>的思想是通过真实梯度来反向推断神经网络每一层的输入，一直到输入层。最简单的情况是每一层都是一个感知器（perceptron）。Phong等人
<span id="id97">(<a class="reference internal" href="../chapter_references/zreferences.html#id95" title="Aono, Y., Hayashi, T., Wang, L., Moriai, S., &amp; others. (2017). Privacy-preserving deep learning via additively homomorphic encryption. IEEE Transactions on Information Forensics and Security, 13(5), 1333–1345.">Aono <em>et al.</em>, 2017</a>)</span>
发现当损失是均方差损失函数时，感知器的某一维的输入可以直接通过当前感知器的梯度逆向得到：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4inputperceptron">
<span class="eqno">(4.3.3)<a class="headerlink" href="#equation-equ-chap4inputperceptron" title="Permalink to this equation">¶</a></span>\[x^{(k)} = \nabla W^{(k)} / \nabla b,\]</div>
<p>其中，<span class="math notranslate nohighlight">\(x^{(k)}\)</span>和<span class="math notranslate nohighlight">\(W^{(k)}\)</span>是输入和模型参数的第<span class="math notranslate nohighlight">\(k = 1,\ldots,d\)</span>维，<span class="math notranslate nohighlight">\(b\)</span>为偏置变量。
而这个结论也被进一步扩展至全连接层（fully connected
layer）和多层感知器（multilayer
perceptron，MLP）。只要偏置项存在，那么就可以通过梯度来逆向推导多层感知器的输入。随后，Zhu和Blaschko
<span id="id98">(<a class="reference internal" href="../chapter_references/zreferences.html#id468" title="Zhu, J., &amp; Blaschko, M. B. (2021). R-gap: recursive gradient attack on privacy. International Conference on Learning Representations.">Zhu and Blaschko, 2021</a>)</span> 将这个结论进一步泛化至卷积层（convolutional
layer）。为了成功窃取第一层卷积的输入，也就是训练数据，Zhu和Blaschko将迭代梯度逆向攻击表述为求解线性方程组的过程：</p>
<div class="math notranslate nohighlight" id="equation-equ-chap4recursiveattack">
<span class="eqno">(4.3.4)<a class="headerlink" href="#equation-equ-chap4recursiveattack" title="Permalink to this equation">¶</a></span>\[\begin{split}\left\{\begin{aligned}W^{(i)}\cdot  x^{(i)}  =  z^{(i)},  \\\nabla  z^{(i)}\cdot  x^{(i)}  = \nabla W^{(i)},\end{aligned}\right.\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-equ-chap4recursivecondition">
<span class="eqno">(4.3.5)<a class="headerlink" href="#equation-equ-chap4recursivecondition" title="Permalink to this equation">¶</a></span>\[\begin{split}s.t. \;\left\{\begin{aligned}z  = \sigma(a) \\\nabla  z  = \nabla a {\sigma}'(a),\end{aligned}\right.\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(z^{(i)}\)</span>、<span class="math notranslate nohighlight">\(\nabla z^{(i)}\)</span>表示输入数据的特征及其对应梯度（损失<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>对于<span class="math notranslate nohighlight">\(z^{(i)}\)</span>的梯度），<span class="math notranslate nohighlight">\(a\)</span>、<span class="math notranslate nohighlight">\(\sigma( \cdot )\)</span>分别表示神经元的输出和每层神经网络的激活函数。从理论上来说，从最后一层全连接开始递归计算，是可以逐像素还原原始输入数据的。需要注意的是，递归梯度逆向攻击目前只被证实能作用于卷积层和全连接层，而无法应用在<em>池化层</em>（pooling
layer）和<em>跳跃连接</em>（skip
connection）。此外，递归梯度逆向攻击也不能很好地还原批次数据。</p>
</div>
</div>
<div class="section" id="sec-datafakeattack">
<span id="id99"></span><h2><span class="section-number">4.4. </span>篡改与伪造<a class="headerlink" href="#sec-datafakeattack" title="Permalink to this heading">¶</a></h2>
<p>篡改和伪造主要是指利用<strong>深度伪造</strong>（deepfake）等多种技术对图像和视频进行篡改。根据被篡改数据的内容和类型，又可分为<em>普通篡改</em>和<em>人脸伪造</em>。
深度伪造技术可以轻松地对图像进行编辑和修改，生成具有误导性或欺骗性的内容，被恶意篡改的内容在经过传播后，会造成误导舆论、扰乱秩序、损坏名誉、骗取钱财等恶性事件的发生。与其他技术不同，现在有很多工具都提供了深度伪造的功能，这些工具的使用门槛很低，使得伪造多媒体（图像、视频、演讲等）能够以极低的成本进行大规模制造。这由此也促进了针对篡改和深度伪造等的检测技术的发展。本章将围绕篡改和伪造技术进行介绍，对应的篡改和伪造检测技术将在章节
<a class="reference internal" href="chap5.html#sec-tamper-fake-detection"><span class="std std-numref">5.4节</span></a> 中介绍。</p>
<div class="section" id="id100">
<h3><span class="section-number">4.4.1. </span>普通篡改<a class="headerlink" href="#id100" title="Permalink to this heading">¶</a></h3>
<p>普通篡改一般涉及移动图像中物体的空间位置、抹掉原有内容并修复出新伪造内容等修改原始图像的行为。最简单的图像篡改技术可以是各种修图工具，可以对少量图像进行手动修改。实际上，图像理解、编辑和修改是计算机视觉和图形学领域的经典研究问题，在近几十年的发展过程中提出了大量简单易用的图像编辑方法。后来在深度学习模型的推动下，出现了大量交互式、智能的图像编辑工具，如风格修改、颜色刷、纹理变换、图像修复等等。这些方法让高效、大批量的图像篡改成为可能。</p>
<p>在基于深度学习的方法中，针对图像的篡改有多种建模方式。首先，图像中的物体、背景等元素之间存在语义关联，传统的基于深度学习的篡改方法有<strong>基于上下文的图像修复</strong>
<span id="id101">(<a class="reference internal" href="../chapter_references/zreferences.html#id555" title="Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., &amp; Efros, A. A. (2016). Context encoders: feature learning by inpainting. IEEE Conference on Computer Vision and Pattern Recognition (pp. 2536–2544).">Pathak <em>et al.</em>, 2016</a>)</span> 、<em>基于条件的生成模型</em>
<span id="id102">(<a class="reference internal" href="../chapter_references/zreferences.html#id556" title="Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., Tao, A., Kautz, J., &amp; Catanzaro, B. (2018). High-resolution image synthesis and semantic manipulation with conditional gans. IEEE Conference on Computer Vision and Pattern Recognition (pp. 8798–8807).">Wang <em>et al.</em>, 2018</a>)</span>
等。这些方法解决的核心问题是如何对图像中的不同元素进行解耦，如物体的背景和前景、纹理和结构等，其中重点关注前景信息（即物体本身）。在解耦后需要对图像中的不同元素进行建模，图片中物体的形状、物体与物体之间的互动和相对位置都可以进行建模。随着技术的发展，建模的粒度逐渐由粗到细，如Hong等人
<span id="id103">(<a class="reference internal" href="../chapter_references/zreferences.html#id19" title="Hong, S., Yan, X., Huang, T. S., &amp; Lee, H. (2018). Learning hierarchical semantic image manipulation through structured representations. Advances in Neural Information Processing Systems, 31.">Hong <em>et al.</em>, 2018</a>)</span>
提出对场景图构建物体级别的语义分割图，这是一个从粗粒度到细粒度逐级进行的<em>分级语义修改框架</em>。用户可以借助一个结构生成器和一个图像生成器通过操控语义对象来篡改图像。它首先以粗略级别的边界框初始化图像生成器；然后通过创建像素级的<em>语义布局</em>（semantic
layout），捕捉物体的形状、物体与物体之间的互动以及物体与场景的关系；最后，图像生成器在语义布局的指导下填充像素级纹理。这样的框架允许用户通过添加、移除或移动边界框，在物体层面上操作图像，展现出了比之前的方法更好的效果。如图
<a class="reference internal" href="#fig-chap4semanticmanipulation"><span class="std std-numref">图4.4.1</span></a>
所示为该分级语义修改框架的架构。</p>
<div class="figure align-default" id="id154">
<span id="fig-chap4semanticmanipulation"></span><a class="reference internal image-reference" href="../_images/4.11_hierarchical_semantic_manipulation.png"><img alt="../_images/4.11_hierarchical_semantic_manipulation.png" src="../_images/4.11_hierarchical_semantic_manipulation.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.1 </span><span class="caption-text">分级语义修改框架 <span id="id104">(<a class="reference internal" href="../chapter_references/zreferences.html#id19" title="Hong, S., Yan, X., Huang, T. S., &amp; Lee, H. (2018). Learning hierarchical semantic image manipulation through structured representations. Advances in Neural Information Processing Systems, 31.">Hong <em>et al.</em>, 2018</a>)</span></span><a class="headerlink" href="#id154" title="Permalink to this image">¶</a></p>
</div>
<p>除了对前景物体进行修改，也可以对图像背景进行修改与替换。类比前景物体篡改，可以将背景视作图像中一种更大的物体，如天空、草地、建筑物、室内场景等，以此对其进行建模。以将图片中的天空作为背景为例，Zou等人
<span id="id105">(<a class="reference internal" href="../chapter_references/zreferences.html#id472" title="Zou, Z., Zhao, R., Shi, T., Qiu, S., &amp; Shi, Z. (2022). Castle in the sky: dynamic sky replacement and harmonization in videos. IEEE Transactions on Image Processing.">Zou <em>et al.</em>, 2022</a>)</span>
提出了<strong>天空置换算法</strong>，以解决拍摄户外照片时出现的天空过曝、景色不佳的状况。所提出的算法由三个核心部分组成：1）由卷积神经网络实现的<strong>天空分割网络</strong>，用以根据图像生成像素级模板，进而区分画面中的天空和非天空区域，以去掉需要置换的天空像素；2）<strong>运动估计模块</strong>，它假定天空在无限远处，天空相对前景的移动是彷射（affine）的，通过光流算法估计原视频中天空的运动；3）<strong>图像融合</strong>，首先进行天空色彩和风格的迁移，然后将新的天空图像作为一个360度的背景，根据第二部分计算出的偏移来确定前景在新的天空中的位置，并实现天空的置换。另一个方法<strong>Image2GIF</strong>
<span id="id106">(<a class="reference internal" href="../chapter_references/zreferences.html#id20" title="Zhou, Y., Song, Y., &amp; Berg, T. L. (2018). Image2gif: generating cinemagraphs using recurrent deep q-networks. IEEE Winter Conference on Applications of Computer Vision (pp. 170–178).">Zhou <em>et al.</em>, 2018</a>)</span>
甚至训练了一个能够在给定单一图像的情况下自动生成电影片段序列的计算模型。他们将生成模型、递归神经网络及深度网络相结合，以增强序列生成的能力。
随着文本-图像预训练模型的兴起，现在一个新的图像篡改方向是：<strong>文本指导图像篡改</strong>，即通过文本命令实现对要被篡改的图像的操控。由Kim等人
<span id="id107">(<a class="reference internal" href="../chapter_references/zreferences.html#id548" title="Kim, G., Kwon, T., &amp; Ye, J. C. (2022). Diffusionclip: text-guided diffusion models for robust image manipulation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2426–2435).">Kim <em>et al.</em>, 2022</a>)</span>
提出的<strong>对比语言-图像预训练模型</strong>（contrastive language-image
pretraining，CLIP） <span id="id108">(<a class="reference internal" href="../chapter_references/zreferences.html#id551" title="Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … others. (2021). Learning transferable visual models from natural language supervision. International Conference on Machine Learning (pp. 8748–8763).">Radford <em>et al.</em>, 2021</a>)</span>
和<strong>扩散模型</strong>（diffusion model）
<span id="id109">(<a class="reference internal" href="../chapter_references/zreferences.html#id549" title="Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33, 6840–6851.">Ho <em>et al.</em>, 2020</a>, <a class="reference internal" href="../chapter_references/zreferences.html#id550" title="Song, J., Meng, C., &amp; Ermon, S. (2021). Denoising diffusion implicit models. International Conference on Learning Representations.">Song <em>et al.</em>, 2021</a>)</span>
是这个领域的代表工作。CLIP和扩散模型完成了在零样本（模型并未在类似的图像上训练过）下对图像中通用物体和属性的修改，如建筑物风格、天气场景、发型装饰等。CLIP模型通过搜集大量的“文本-图像”配对数据进行训练，很好的解决了常规的图像分类模型在进行全监督训练时对数据要求高、需要大量人工标注的缺点，提高了模型的泛化能力和适用性。而在扩散模型流行之前，文本指导的图像篡改大都通过生成对抗网络来完成，如TediGAN
<span id="id110">(<a class="reference internal" href="../chapter_references/zreferences.html#id552" title="Xia, W., Yang, Y., Xue, J.-H., &amp; Wu, B. (2021). Tedigan: text-guided diverse face image generation and manipulation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2256–2265).">Xia <em>et al.</em>, 2021</a>)</span> 、StyleCLIP <span id="id111">(<a class="reference internal" href="../chapter_references/zreferences.html#id554" title="Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., &amp; Lischinski, D. (2021). Styleclip: text-driven manipulation of stylegan imagery. IEEE/CVF International Conference on Computer Vision (pp. 2085–2094).">Patashnik <em>et al.</em>, 2021</a>)</span>
和StyleGAN-NADA <span id="id112">(<a class="reference internal" href="../chapter_references/zreferences.html#id553" title="Gal, R., Patashnik, O., Maron, H., Bermano, A. H., Chechik, G., &amp; Cohen-Or, D. (2022). Stylegan-nada: clip-guided domain adaptation of image generators. ACM Transactions on Graphics, 41(4), 1–13.">Gal <em>et al.</em>, 2022</a>)</span>
等方法。总体来说，由于缺乏对特定领域数据的学习，这些借助CLIP预训模型的文本指导图像篡改方法往往精细化程度不够，大都会留下明显的痕迹。但是这些方法的潜在危害不容忽视，它们在方便图像编辑的同时也大大降低了图像篡改的门槛，且未来可能会与其他大规模预训模型（如物体分割模型）结合，达到更精准、更隐蔽的图像篡改。</p>
</div>
<div class="section" id="id113">
<h3><span class="section-number">4.4.2. </span>深度伪造<a class="headerlink" href="#id113" title="Permalink to this heading">¶</a></h3>
<p>深度人脸伪造特指基于深度学习技术生成的人脸伪造数据。在深度学习技术广为流行的今天，可以说任何人都可通过深度伪造软件轻松创建图像、视频或语音等伪造内容。当前，深度伪造最广泛使用的深度学习技术是<em>生成对抗网络</em>（GAN），未来还会包括<em>扩散模型</em>（diffusion
model）等。</p>
<p>生成对抗网络的思想源于博弈论中的“<strong>零和博弈</strong>”（zero-sum
game），其通过一个<strong>生成器</strong>
（generator）模型和一个<strong>判别器</strong>（discriminator）模型之间的相互博弈来学习真实数据分布，如图
<a class="reference internal" href="#fig-chap4gan"><span class="std std-numref">图4.4.2</span></a>
所示。其中，生成器基于随机噪声向量生成数据样本，让判别器无法区别样本的真实性；而判别器则尝试将生成器生成的样本判别为假样本。二者以这种对抗式的方式通过交替优化达到纳什均衡状态，此时生成器能够生成“以假乱真”的数据样本，使判别器无法判别真伪，即判断准确率相当于随机猜测。相比于其他生成模型，生成对抗网络具有一定的优势:
1）不依赖先验知识;
2）生成器的参数更新来自判别器的反向传播，而非直接来自于数据样本，故训练不需要复杂的<em>马尔可夫链</em>（Markov
chain）。生成对抗网络在图像编辑、数据生成、恶意攻击检测、肿瘤识别和注意力预测等领域具有广泛应用，这里关注的是其在深度伪造方面的应用。</p>
<div class="figure align-default" id="id155">
<span id="fig-chap4gan"></span><a class="reference internal image-reference" href="../_images/4.12_gan_architecture.png"><img alt="../_images/4.12_gan_architecture.png" src="../_images/4.12_gan_architecture.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.2 </span><span class="caption-text">生成对抗网络 <span id="id114">(<a class="reference internal" href="../chapter_references/zreferences.html#id248" title="Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative adversarial nets. Advances in Neural Information Processing Systems, 27.">Goodfellow <em>et al.</em>, 2014</a>)</span></span><a class="headerlink" href="#id155" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="id115">
<h4><span class="section-number">4.4.2.1. </span>人脸伪造<a class="headerlink" href="#id115" title="Permalink to this heading">¶</a></h4>
<p>2017年，Korshunova等人 <span id="id116">(<a class="reference internal" href="../chapter_references/zreferences.html#id11" title="Korshunova, I., Shi, W., Dambre, J., &amp; Theis, L. (2017). Fast face-swap using convolutional neural networks. International Conference on Computer Vision (pp. 3677–3685).">Korshunova <em>et al.</em>, 2017</a>)</span>
提出了一种基于GAN的自动化实时换脸（face
swap）技术。同年，Suwajanakorn等人
<span id="id117">(<a class="reference internal" href="../chapter_references/zreferences.html#id12" title="Suwajanakorn, S., Seitz, S. M., &amp; Kemelmacher-Shlizerman, I. (2017). Synthesizing obama: learning lip sync from audio. ACM Transactions on Graphics, 36(4), 1–13.">Suwajanakorn <em>et al.</em>, 2017</a>)</span> 使用长短期记忆网络（long
short-term
memory，LSTM）设计了一种智能化学习口腔形状和声音之间关联性的方法，该方法仅通过音频即可生成对应的口部特征。研究者利用美国前总统奥巴马在互联网上的音视频片段，生成了非常逼真的假视频。此技术一经问世便引起了广泛关注，基于其原理实现的换脸项目也大量出现，极大的刺激了视觉深度伪造技术的发展。</p>
<p>视觉（主要是人脸）深度伪造技术的实现大体可分为<em>数据收集</em>、<em>模型训练</em>和<em>伪造内容生成</em>三个步骤。假设我们的目标是将Alice的脸换至Bob的身体上，可以通过以下的几个步骤进行实现。</p>
<p><strong>（1）数据收集：</strong>
数据收集顾名思义就是通过各种渠道对Alice和Bob的已有图像进行大量收集，以便为模型训练提供数据支撑。对于公众人物来说，数据收集可以说是不费吹灰之力，因为他们有大量的演讲、报告、报道等数据公布在互联网上。注意这里收集的是人脸图像，所以很多时候需要使用人脸检测工具将人脸区域检测并截取出来。</p>
<p><strong>（2）模型训练：</strong>
目前，针对人脸伪造的深度伪造模型主要基于自动编解码器，一般由编码器（encoder）和解码器（decoder）两部分构组成。
编码器用于提取人脸图像的潜在特征，解码器则用于重构人脸图像。为了实现换脸操作，模型需要两对编码器/解码器组（编码器A/解码器A、编码器B/解码器B），分别基于已收集的Alice和Bob的图像集进行训练，其中编码器A和编码器B具有相同的编码网络（即参数共享）。通过统一的编码器可以把Alice和Bob两个人的人脸特征编码到同一个隐式空间，只有在同一个隐式空间，二者的脸部特征才能发生互换。两套编解码器的训练过程如图
<a class="reference internal" href="#fig-chap4faceswap"><span class="std std-numref">图4.4.3</span></a> (a)所示。</p>
<div class="figure align-default" id="id156">
<span id="fig-chap4faceswap"></span><a class="reference internal image-reference" href="../_images/4.13_faceswap.png"><img alt="../_images/4.13_faceswap.png" src="../_images/4.13_faceswap.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.3 </span><span class="caption-text">深度伪造内容生成流程
<span id="id118">(<a class="reference internal" href="../chapter_references/zreferences.html#id558" title="梁瑞刚, 吕培卓, 赵月, 陈鹏, 邢豪, 张颖君, … others. (2020). 视听觉深度伪造检测技术研究综述. 信息安全学报, 5(2), 1–17.">梁瑞刚 <em>et al.</em>, 2020</a>)</span></span><a class="headerlink" href="#id156" title="Permalink to this image">¶</a></p>
</div>
<p><strong>（3）伪造图像生成：</strong>
待模型训练完成之后，通过将Alice和Bob的解码器互换，进而构建新的编码器/解码器组（编码器A/解码器B，编码器B/解码器A），然后选取Alice的一张图像作为目标图像，在编码器A编码完成之后，基于解码器B进行解码，从而生成载有Bob面部、Alice身体的深度伪造（换脸）图像，如图
<a class="reference internal" href="#fig-chap4faceswap"><span class="std std-numref">图4.4.3</span></a> (b)所示。</p>
<p>人脸伪造技术可以按照对人脸图像的修改程度分为两类：<em>人脸互换</em>（face
swap）和<em>面部重演</em>（facial
reenactment）。人脸互换的目标是用目标人物的脸替换原图像中的人脸，面部重演的目标是将源图像中人的表情转移到目标任务上。</p>
<p>总的来说这两类修改目标都可以基于自动编码器来完成，通过解耦人脸的身份与表情信息实现不同维度、不同层次的特征提取，并最终将输入的人脸扭曲后重建出伪造的人脸。通过引入生成对抗网络可以大幅提高替换后人脸的真实性，即添加一个判别器来判别生成图像和真实图像，强制解码器生成高度真实的换脸图像。</p>
<p><strong>SimSwap人脸互换方法。</strong>人脸互换的方法有很多种，传统的人脸互换的方法如FaceSwap等，通常缺乏泛化性，无法保留面部表情和注视方向等属性、生成人脸图像质量差等问题。为了提高人脸互换的泛化性，Chen等人
<span id="id119">(<a class="reference internal" href="../chapter_references/zreferences.html#id13" title="Chen, R., Chen, X., Ni, B., &amp; Ge, Y. (2020). Simswap: an efficient framework for high fidelity face swapping. ACM International Conference on Multimedia (pp. 2003–2011).">Chen <em>et al.</em>, 2020</a>)</span>
提出了<strong>SimSwap框架</strong>以实现通用且高保真的面部交换。SimSwap的主要工作是：1）引入了<strong>身份注入模块</strong>，在特征层面将源脸的身份信息转移到了目标脸上，来将特定身份的人脸结构互换扩展到任意结构，解决了传统的方法需要预先进行面部关键点检测、姿态估计等处理的问题；2）提出<strong>弱特征匹配损失</strong>，得以隐式地、有效地保留面部属性，从而实现面部特征在互换的同时保持身份。通过以上方法，SimSwap能够用任意目标脸替换任意源脸，同时保留目标脸的面部身份特征属性，实现了将特定身份的人脸互换结构扩展到任意人脸互换。</p>
<p>具体而言，SimSwap中的编码器从目标图像中提取特征，<strong>身份注入模块</strong>将身份信息从源图像传输到特征中，解码器将修改后的特征恢复到结果图像中。SimSwap使用所设计的<strong>身份对应损失函数</strong>来保证生成图像与源图像具有相似身份结果。SimSwap使用<strong>弱特征匹配损失函数</strong>来确保网络可以保留目标人脸的属性，同时不会过多地损害身份修改性能。身份对应损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{id}}\)</span>的定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-1">
<span class="eqno">(4.4.1)<a class="headerlink" href="#equation-source-chap4-1" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{id}}=1-\frac{ v_{s} \cdot  v_{t}}{\left\| v_{s}\right\|_{2}\left\| v_{t}\right\|_{2}}.\]</div>
<p>身份对应损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{id}}\)</span>定义了<strong>源身份向量</strong><span class="math notranslate nohighlight">\(v_{s}\)</span>及<strong>目标身份向量</strong><span class="math notranslate nohighlight">\(v_{t}\)</span>之间的余弦相似度。
弱特征损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{wFM}}\)</span>的定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-2">
<span class="eqno">(4.4.2)<a class="headerlink" href="#equation-source-chap4-2" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{wFM}}=\sum_{i=m}^{M} \frac{1}{N_{i}}\left\|D^{(i)}\left( x_{s}\right)-D^{(i)}\left(x_{t}\right)\right\|_{1},\]</div>
<p>其中，<span class="math notranslate nohighlight">\(D^{(i)}\)</span>为判别器<span class="math notranslate nohighlight">\(D\)</span>第<span class="math notranslate nohighlight">\(i\)</span>层的特征提取器，<span class="math notranslate nohighlight">\(N_{i}\)</span>为第<span class="math notranslate nohighlight">\(i\)</span>层的元素个数。
弱特征匹配函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{wFM}}\)</span>从第<span class="math notranslate nohighlight">\(m\)</span>层起计算目标图像<span class="math notranslate nohighlight">\(x_{t}\)</span>与源图像<span class="math notranslate nohighlight">\(x_{s}\)</span>之间的弱特征损失。由于使用了多个判别器<span class="math notranslate nohighlight">\(D\)</span>，总的弱特征损失函数为:</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-3">
<span class="eqno">(4.4.3)<a class="headerlink" href="#equation-source-chap4-3" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{wFM}_{-}\text{sum}}=\sum_{i=1}^{2} \mathcal{L}_{\text{wFM}}\left(D_{i}\right).\]</div>
<p><strong>FaceShifter人脸互换方法。</strong> 而为了生成更加高质量的人脸图像，Li等人
<span id="id120">(<a class="reference internal" href="../chapter_references/zreferences.html#id14" title="Li, L., Bao, J., Yang, H., Chen, D., &amp; Wen, F. (2020). Advancing high fidelity identity swapping for forgery detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5074–5083).">Li <em>et al.</em>, 2020</a>)</span>
提出<strong>FaceShifter</strong>架构，通过两个网络，即<strong>AEI-Net</strong>（adaptive
embedding integration
network）用来进行面部交换和<strong>HEAR-Net</strong>（heuristic error
acknowledging refinement network）用来处理遮挡。
用于脸部交换的网络AEI-Net由三个部分组成：1）<strong>身份编码器</strong>，采用预先训练好的人脸识别模型，提供抽象身份表征；2）一个<strong>多级属性编码器</strong>，对面部属性的特征进行分级和编码；3）<strong>AAD生成器</strong>（adaptive
attentional
denormalization），将身份和属性信息进行融合并生成伪造的人脸。而处理遮挡的网络HEAR-Net则以自监督的方式进行训练，它可以恢复异常（如遮挡）区域。通过这两个网络，可以实现对输入图像中的人脸自适应地调整网络架构，从而提高转换的精度和质量。FaceShifter的网络框架如图
<a class="reference internal" href="#fig-faceshifter"><span class="std std-numref">图4.4.4</span></a> 所示。</p>
<div class="figure align-default" id="id157">
<span id="fig-faceshifter"></span><a class="reference internal image-reference" href="../_images/4.14_FaceShifter.png"><img alt="../_images/4.14_FaceShifter.png" src="../_images/4.14_FaceShifter.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.4 </span><span class="caption-text">FaceShifter框架 <span id="id121">(<a class="reference internal" href="../chapter_references/zreferences.html#id14" title="Li, L., Bao, J., Yang, H., Chen, D., &amp; Wen, F. (2020). Advancing high fidelity identity swapping for forgery detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5074–5083).">Li <em>et al.</em>, 2020</a>)</span></span><a class="headerlink" href="#id157" title="Permalink to this image">¶</a></p>
</div>
<p>具体来说，FaceShifter接收两张图片，源图片<span class="math notranslate nohighlight">\(x_{s}\)</span>用来提供身份，目标图片<span class="math notranslate nohighlight">\(x_{t}\)</span>用来提供姿势、表情、光照或是背景等属性。在第一阶段，<strong>AEI-Net</strong>中的三个组件被用来生成高保真的人脸：1）<strong>身份编码器</strong><span class="math notranslate nohighlight">\(z_{\text{id}}( x_{s})\)</span>用来从源图片<span class="math notranslate nohighlight">\(x_{s}\)</span>提取身份特征；2）<em>多层属性编码器</em><span class="math notranslate nohighlight">\(z_{\text{att}}( x_{t})\)</span>用来从目标图片<span class="math notranslate nohighlight">\(x_{t}\)</span>提取属性；3）<em>AAD生成器</em>用来交换面部。在第二阶段，HEAR-Net用来处理生成图像中可能出现的面部遮挡。其中，多层属性编码器还可进一步定义为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-4">
<span class="eqno">(4.4.4)<a class="headerlink" href="#equation-source-chap4-4" title="Permalink to this equation">¶</a></span>\[z_{\text{att}}\left( x_{t}\right)=\left\{ z_{\text{att}}^{1}\left( x_{t}\right),  z_{\text{att}}^{2}\left( x_{t}\right), \ldots z_{\text{att}}^{n}\left( x_{t}\right)\right\},\]</div>
<p>其中，<span class="math notranslate nohighlight">\(z_{\text{att}}^{k}( x_{t})\)</span>代表第<span class="math notranslate nohighlight">\(k\)</span>层的特征图，<span class="math notranslate nohighlight">\(n\)</span>表示特征图的层数。这个多层属性编码器可以不需要任何属性标注，它通过自监督的方式来对属性进行提取。源图片<span class="math notranslate nohighlight">\(x_{s}\)</span>及目标图片<span class="math notranslate nohighlight">\(x_{t}\)</span>需要具有相同的属性嵌入。</p>
<p>AEI-Net中定义了多个损失函数。<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{adv}}\)</span>是对抗损失函数，使生成的面部<span class="math notranslate nohighlight">\(\hat{Y}_{s,t}\)</span>更加逼真；<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{id}}\)</span>作为身份保存损失函数，运用了余弦相似度来对源图像的信息进行保存，其定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-5">
<span class="eqno">(4.4.5)<a class="headerlink" href="#equation-source-chap4-5" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{id}=1-\cos \left( z_{id}\left(\hat{Y}_{s,t}\right),  z_{id}\left( x_{s}\right)\right).\]</div>
<p>多层属性编码器的损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{att}}\)</span>用来计算目标图片<span class="math notranslate nohighlight">\(x_{t}\)</span>与生成的面部<span class="math notranslate nohighlight">\(\hat{Y}_{s,t}\)</span>间的<span class="math notranslate nohighlight">\(L_2\)</span>距离，其定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-6">
<span class="eqno">(4.4.6)<a class="headerlink" href="#equation-source-chap4-6" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{att}}=\frac{1}{2} \sum_{k=1}^{n}\left\| z_{\text{att}}^{k}\left(\hat{Y}_{s, t}\right)- z_{\text{att}}^{k}\left( x_{t}\right)\right\|_{2}^{2}.\]</div>
<p>当训练样本中的目标图片<span class="math notranslate nohighlight">\(x_{t}\)</span>和源图像<span class="math notranslate nohighlight">\(x_{s}\)</span>相同时，采用重建损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{rec}}\)</span>来在像素层面计算目标图像<span class="math notranslate nohighlight">\(x_{t}\)</span>和生成的面部<span class="math notranslate nohighlight">\(\hat{Y}_{s,t}\)</span>的<span class="math notranslate nohighlight">\(L_2\)</span>距离，其定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-7">
<span class="eqno">(4.4.7)<a class="headerlink" href="#equation-source-chap4-7" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathcal{L}_{\text {rec}}=\left\{\begin{array}{ll}\frac{1}{2}\left\|\hat{Y}_{s, t}- x_{t}\right\|_{2}^{2} \qquad  \text { if }  x_{t}= x_{s} \\0  \qquad \text { otherwise }.\end{array} \right.\end{split}\]</div>
<p>AEI-Net的总的损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{AEI-Net}}\)</span>可由上述损失函数加权定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-8">
<span class="eqno">(4.4.8)<a class="headerlink" href="#equation-source-chap4-8" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text {AEI-Net }}=\mathcal{L}_{\text{adv}}+\lambda_{\text{att}} \mathcal{L}_{\text{att}}+\lambda_{\text{id}} \mathcal{L}_{\text{id}}+\lambda_{\text {rec}} \mathcal{L}_{\text {rec}},\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\lambda_{\text{att}}=\lambda_{\text{rec}}=10\)</span>，<span class="math notranslate nohighlight">\(\lambda_{\text{id}}\)</span>=5。</p>
<p>在第二阶段，<strong>HEAR-Net</strong>用以处理面部遮挡。它首先对目标图片可能产生的异常进行了定义：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-9">
<span class="eqno">(4.4.9)<a class="headerlink" href="#equation-source-chap4-9" title="Permalink to this equation">¶</a></span>\[\Delta Y_{t}= x_{t}-\text{AEI-Net}\left( x_{t},  x_{t}\right).\]</div>
<p>随后将产生的异常<span class="math notranslate nohighlight">\(\Delta Y_{t}\)</span>及第一阶段生成的面部结果<span class="math notranslate nohighlight">\(\hat{Y}_{s,t}\)</span>送入一个U-Net结构中，来获得调整后的图像<span class="math notranslate nohighlight">\(Y_{s,t}\)</span>。HEAR-Net可以以全监督的方式进行训练，损失函数包括：<em>身份保存损失函数</em>、<em>更改损失函数</em>和<em>重建损失函数</em>。</p>
<p>身份保存损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{id}}^{\prime}\)</span>与第一阶段所使用的类似：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-10">
<span class="eqno">(4.4.10)<a class="headerlink" href="#equation-source-chap4-10" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{id}}^{\prime}=1-\cos \left( z_{\text{id}}\left(Y_{s, t}\right),  z_{\text{id}}\left( x_{s}\right)\right).\]</div>
<p>更改损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{chg}^{\prime}\)</span>用来保证第一阶段与第二阶段生成结果间的连续性：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-11">
<span class="eqno">(4.4.11)<a class="headerlink" href="#equation-source-chap4-11" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{chg}}^{\prime}=\left|\hat{Y}_{s, t}-Y_{s, t}\right|.\]</div>
<p>重建损失函数<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{rec}}^{\prime}\)</span>用来限制第二阶段在源图像和目标图像相同时仍然能完成重建：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-12">
<span class="eqno">(4.4.12)<a class="headerlink" href="#equation-source-chap4-12" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathcal{L}_{\text{rec}}^{\prime}=\left\{\begin{array}{ll}\frac{1}{2}\left\|Y_{s, t}- x_{t}\right\|_{2}^{2}  \qquad \text { if }  x_{t}= x_{s} \\0  \qquad \text { otherwise }.\end{array}\right.\end{split}\]</div>
<p>最终HEAR-Net总的损失函数为上述损失函数之和：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-13">
<span class="eqno">(4.4.13)<a class="headerlink" href="#equation-source-chap4-13" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{HEAR-Net}}=\mathcal{L}_{\text{rec}}^{\prime }+\mathcal{L}_{\text{id}}^{\prime }+\mathcal{L}_{\text{chg}}^{\prime }\]</div>
<p>通过AEI-Net及HEAR-Net，Faceshifter能够实现高保真和遮挡感知的面部交换，通过自监督的方式对HEAR-Net方式进行训练，可以无需任何手动的标注便实现对一场区域的恢复。</p>
<p><strong>人脸重演</strong>。通常的人脸重演步骤是（1）<strong>人脸追踪</strong>(face
tracking)，（2）<strong>人脸匹配</strong>（face
matching)和（3）<strong>人脸迁移</strong>（face
transfer)，逐步将所提供的人脸迁移到目标人物的身上。早在2014年，Garrido等人
<span id="id122">(<a class="reference internal" href="../chapter_references/zreferences.html#id473" title="Garrido, P., Valgaerts, L., Rehmsen, O., Thormahlen, T., Perez, P., &amp; Theobalt, C. (2014). Automatic face reenactment. IEEE Conference on Computer Vision and Pattern Recognition (pp. 4217–4224).">Garrido <em>et al.</em>, 2014</a>)</span>
就提出了一个自动化的人脸互换和重演框架，作者通过上述步骤将自己的面部替换到了美国前总统奥巴马的演讲视频中，并完成了奥巴马演讲表情的重演。虽然研究者自己的表情跟演讲并不完全对应，但此工作实现了面部替换和重演的全部自动化，并且只需要用户随意录制一个短视频即可，不需要做跟目标视频中一样的面部表情。</p>
<p>除了传统的神经网络之外，<strong>3D生成技术</strong>也被运用来生成更真实合理的人像，Sun等人
<span id="id123">(<a class="reference internal" href="../chapter_references/zreferences.html#id477" title="Sun, J., Wang, X., Zhang, Y., Li, X., Zhang, Q., Liu, Y., &amp; Wang, J. (2022). Fenerf: face editing in neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7672–7682).">Sun <em>et al.</em>, 2022</a>)</span>
提出的<strong>FENeRF</strong>即是一个3D感知生成器，它可以生成视图一致的本地可编辑的面部图像。FENeRF的核心部分包括：（1）使用两个解耦的编码，在同一个3D空间中，分别生成对应的面部语义信息和纹理；（2）通过这种底层3D表达，渲染出没有裁切边界的图像和语义掩码，并使用语义掩码通过生成对抗网络来编辑3D信息。上述的步骤可以帮助FENeRF生成更加精细的图像。</p>
<p>与传统的面部重演不同，Song等人 <span id="id124">(<a class="reference internal" href="../chapter_references/zreferences.html#id474" title="Song, L., Wu, W., Fu, C., Qian, C., Loy, C. C., &amp; He, R. (2021). Everything's talkin': pareidolia face reenactment. IEEE/CVF Conference on Computer Vision and Pattern Recognition.">Song <em>et al.</em>, 2021</a>)</span>
的工作提出了一个新的概念，<strong>虚幻人脸重演</strong>（pareidolia face
reenactment），即目标人脸不是常规的人脸，而是卡通、树皮、机器人、姜饼人等“幻想”出来的人脸。这引发了两个新的挑战，即形状差异和纹理差异。<strong>形状差异</strong>指的是非常规人脸在重演的过程中，可能会出现面部轮廓特异、五官错位等问题。例如，机器人可能有特异数目的眼睛、嘴巴等。<strong>纹理差异</strong>指的是如树皮、金属、食物等往往具有与人类皮肤相差甚远的纹理，这就使得重演后的五官部分可能会出现纹理不匹配的瑕疵。</p>
<p>在这项工作 <span id="id125">(<a class="reference internal" href="../chapter_references/zreferences.html#id474" title="Song, L., Wu, W., Fu, C., Qian, C., Loy, C. C., &amp; He, R. (2021). Everything's talkin': pareidolia face reenactment. IEEE/CVF Conference on Computer Vision and Pattern Recognition.">Song <em>et al.</em>, 2021</a>)</span>
中，研究者提出了一种新的<strong>参数化无监督重演算法</strong>（parametric
unsupervised reenactment
algorithm，PURA）来解决这两个挑战。通过将人脸重演分解为三个过程：<strong>形状建模</strong>、<strong>运动转移</strong>和<strong>纹理合成</strong>，并有针对性地引入了三个关键部分，即<em>参数化形状建模</em>（parametric
shape modeling，PSM）、<em>扩展运动转移</em>（expansionary motion
transfer，EMT）和<em>无监督纹理合成器</em>（ unsupervised texture
synthesizer，UTS），以克服罕见的脸部变异所带来的问题。所提出的参数化无监督重演算法框架如图
<a class="reference internal" href="#fig-psm"><span class="std std-numref">图4.4.5</span></a> 所示。</p>
<div class="figure align-default" id="id158">
<span id="fig-psm"></span><a class="reference internal image-reference" href="../_images/4.15_PSM.png"><img alt="../_images/4.15_PSM.png" src="../_images/4.15_PSM.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.5 </span><span class="caption-text">参数化无监督重现算法框架 <span id="id126">(<a class="reference internal" href="../chapter_references/zreferences.html#id474" title="Song, L., Wu, W., Fu, C., Qian, C., Loy, C. C., &amp; He, R. (2021). Everything's talkin': pareidolia face reenactment. IEEE/CVF Conference on Computer Vision and Pattern Recognition.">Song <em>et al.</em>, 2021</a>)</span></span><a class="headerlink" href="#id158" title="Permalink to this image">¶</a></p>
</div>
<p>在PURA重演方法中，<strong>参数化建模</strong>基于的是贝塞尔曲线(Bezier
curves)。为了表征人脸的形状同时又不受比例和旋转的影响，PURA用<strong>模板对齐算法</strong>（template
alignment algorithm） <span id="id127">(<a class="reference internal" href="../chapter_references/zreferences.html#id591" title="Segal, A., Haehnel, D., &amp; Thrun, S. (2009). Generalized-icp. Robotics: science and systems (p. 435).">Segal <em>et al.</em>, 2009</a>)</span>
将人脸<span class="math notranslate nohighlight">\(H\)</span>仿射到<strong>FLAME</strong> <span id="id128">(<a class="reference internal" href="../chapter_references/zreferences.html#id592" title="Li, T., Bolkart, T., Black, M. J., Li, H., &amp; Romero, J. (2017). Learning a model of facial shape and expression from 4d scans. ACM Transactions on Graphics, 36(6).">Li <em>et al.</em>, 2017</a>)</span>
的通用头部模型<span class="math notranslate nohighlight">\(F\)</span>上，所对齐的面部边界将用于后续的形状建模。在人脸形状建模阶段，通过提取并连接人脸<span class="math notranslate nohighlight">\(H\)</span>的68个3D地标，我们可以得到不同人脸部位的边界集合<span class="math notranslate nohighlight">\(\mathcal{S}_{H}=\left\{C_{i}^{H}\right\}_{i=1}^{N}\)</span>。<span class="math notranslate nohighlight">\(\mathcal{S}_{H}\)</span>包含<span class="math notranslate nohighlight">\(N_H\)</span>个分支（比如，嘴的边界可以分为四个分支：上下嘴唇的内外边界），每个分支<span class="math notranslate nohighlight">\(C_{i}^{H}\)</span>用高阶复合贝塞尔曲线进行拟合，从而转化为控制点。对所参考的FLAME头部模型<span class="math notranslate nohighlight">\(F\)</span>也进行类似的操作。分别进行参数化后，可以得到人脸<span class="math notranslate nohighlight">\(H\)</span>，头部模型<span class="math notranslate nohighlight">\(F\)</span>，虚幻人脸<span class="math notranslate nohighlight">\(P\)</span>的面部部位边界控制点<span class="math notranslate nohighlight">\(\mathcal{P}_{H}\)</span>、<span class="math notranslate nohighlight">\(\mathcal{P}_{F}\)</span>、<span class="math notranslate nohighlight">\(\mathcal{P}_{P}\)</span>，用于随后<strong>动作控制器</strong>的计算。</p>
<p>从人脸中提取的动作表示可以记作<span class="math notranslate nohighlight">\(\mathcal{M}_{s}\)</span>，定义为控制点<span class="math notranslate nohighlight">\(\mathcal{P}_{H}\)</span>与<span class="math notranslate nohighlight">\(\mathcal{P}_{F}\)</span>的状态：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-14">
<span class="eqno">(4.4.14)<a class="headerlink" href="#equation-source-chap4-14" title="Permalink to this equation">¶</a></span>\[\mathcal{M}_{\mathbf{S}}=\bigcup_{i=1}^{N}\left\{\left(\frac{\hat{x_{j}^{i}}}{\overline{x_{j}^{i}}}, \frac{\hat{y_{j}^{i}}}{\overline{y_{j}^{i}}}, \frac{\hat{z_{j}^{i}}}{\overline{z_{j}^{i}}}\right)\right\}_{j=0}^{h_{i}}.\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{M}_{S}\)</span>将对虚幻人脸的面部的边界<span class="math notranslate nohighlight">\(\mathcal{S}_{P}\)</span>进行动画处理，即<strong>运动控制器</strong>。一般来说，虚幻人脸的边界分支是人脸边界的一个子集。通过控制点<span class="math notranslate nohighlight">\(\mathcal{P}_{P}\)</span>对边界<span class="math notranslate nohighlight">\(\mathcal{S}_{P}\)</span>进行参数化：<span class="math notranslate nohighlight">\(\mathcal{P}_{P}=\bigcup_{i=1}^{N_{P}} \mathcal{P}_{B_{i}}=\bigcup_{i=1}^{N_{P}}\left\{\left(x_{j}^{i}, y_{j}^{i}, z_{j}^{i}\right)\right\}_{j=0}^{t_{i}}\)</span>，其中<span class="math notranslate nohighlight">\(N_{P}\)</span>是分支的号码，<span class="math notranslate nohighlight">\(t_{i}\)</span>是第<span class="math notranslate nohighlight">\(i\)</span>个分支的曲线序列，<span class="math notranslate nohighlight">\(B_{i}\)</span>为虚幻人脸的曲线。</p>
<p>由于<span class="math notranslate nohighlight">\(B_{i}\)</span>可能与响应人脸曲线<span class="math notranslate nohighlight">\(B_{i}^{H}\)</span>有很大的不同，在<span class="math notranslate nohighlight">\(t_{i}\)</span>&lt;<span class="math notranslate nohighlight">\(h_{i}\)</span>时移除了<span class="math notranslate nohighlight">\(\mathcal{M}_{S}\)</span>中的曲线<span class="math notranslate nohighlight">\(B_{i}^{H}\)</span>的有序动作控制器；在<span class="math notranslate nohighlight">\(t_{i}\)</span>&gt;<span class="math notranslate nohighlight">\(h_{i}\)</span>进行线性插值处理，并将处理后的运动控制器记为<span class="math notranslate nohighlight">\(\mathcal{M}_{S}^{e}\)</span>。最后，虚幻人脸的面部将由<span class="math notranslate nohighlight">\(\mathcal{M}_{S}^{e}\)</span>进行动画化处理：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-15">
<span class="eqno">(4.4.15)<a class="headerlink" href="#equation-source-chap4-15" title="Permalink to this equation">¶</a></span>\[\mathcal{P}_{\mathbf{P}}^{\prime}=\mathcal{M}_{\mathbf{S}}^{e} \otimes \mathcal{P}_{\mathbf{P}}=\bigcup_{i=1}^{N_{P}}\left\{\left(\frac{\hat{x}_{j}^{i}}{\overline{x_{j}^{i}}} x_{j}^{i}, \frac{\hat{y}_{j}^{i}}{y_{j}^{i}} y_{j}^{i}, \frac{\hat{z}_{j}^{i}}{\overline{z_{j}^{i}}} z_{j}^{i}\right)\right\}_{j=0}^{t_{i}}.\]</div>
<p>通过上面的运动控制器能够复合贝塞尔曲线的控制点来转移面部边界处的运动，但这么转移的运动是局部的，从而需要采用<strong>扩展运动转移</strong>（EMT）来对运动进行扩展。将曲线<span class="math notranslate nohighlight">\(B_{i}\)</span>处的运动记为运动种子<span class="math notranslate nohighlight">\(\mathcal{M}_{B_{i}}^{e}\)</span>，则<span class="math notranslate nohighlight">\(\left\{\mathcal{M}_{B_{i}}^{e}\right\}_{i=1}^{N_{P}}\)</span>记为运动种子集。所提出的扩展运动转移策略沿着其与复合贝塞尔曲线正交的方向衰减运动种子。来自运动种子<span class="math notranslate nohighlight">\(\mathcal{M}_{B_{i}}^{e}\)</span>的衰减运动<span class="math notranslate nohighlight">\(\mathcal{M}_{B_{i}\left(\omega_{i}, \tau_{i}\right)}^{e}\)</span>可以定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-16">
<span class="eqno">(4.4.16)<a class="headerlink" href="#equation-source-chap4-16" title="Permalink to this equation">¶</a></span>\[\mathcal{M}_{B_{i}\left(\omega_{i}, \tau_{i}\right)}^{e}=\lambda\left(\omega_{i}\right) \cdot \mathcal{M}_{B_{i}\left(1, \tau_{i}\right)}^{e}, \; \mathcal{M}_{B_{i}\left(1, \tau_{i}\right)}^{e} \in \mathcal{M}_{B_{i}}^{e},\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\lambda\left(\omega_{i}\right)\)</span>表示<span class="math notranslate nohighlight">\(\omega_{i}\)</span>的衰减参数。虚幻人脸中的像素<span class="math notranslate nohighlight">\(P\)</span>会从许多运动种子中接收到衰减运动信息，将<span class="math notranslate nohighlight">\(P\)</span>处的衰减运动的组合表示为<span class="math notranslate nohighlight">\(\mathcal{M}_{P}^{e}\)</span>。为了对虚幻人脸<span class="math notranslate nohighlight">\(P\)</span>进行动画化处理，PURA通过所提出的<strong>扩展运动转移</strong>从运动种子构建整个幻想面部（pareidolia
face）的全局运动。定义<span class="math notranslate nohighlight">\(G(·)\)</span>为返回输入图像像素网格的函数，定义运动场（motion
field）<span class="math notranslate nohighlight">\(\mathcal{M}^{e}=\left\{\mathcal{M}_{\mathbf{p}}^{e}\right\}_{\mathbf{p} \in G(\mathbf{P})}\)</span>，对于每个<span class="math notranslate nohighlight">\(P\)</span>中的像素，运动场可以将其驱动到一个新的地址。</p>
<p>最后一步<strong>无监督纹理合成器</strong>则使用没有标注的自然图像来训练一个自动编码器，通过将自动编码器网络与特征变形层相结合，逐步将运动转换为纹理。通过上述的模块，所提出的PURA框架能够将静态的虚幻人脸转化为动态的人脸，克服幻想面部所带来的显著差异。</p>
<p>表情重演仅是人脸重演的一个部分，还有许多工作希望能够同时进行人脸生成领域的两项工作，即<strong>面部身份变换</strong>和<strong>表情迁移</strong>。例如FSGAN
<span id="id129">(<a class="reference internal" href="../chapter_references/zreferences.html#id476" title="Nirkin, Y., Keller, Y., &amp; Hassner, T. (2022). Fsganv2: improved subject agnostic face swapping and reenactment. IEEE Transactions on Pattern Analysis and Machine Intelligence.">Nirkin <em>et al.</em>, 2022</a>)</span>
进行的人脸交换工作。FSGAN的具体流程是：（1）<strong>互换生成器</strong><span class="math notranslate nohighlight">\(G_r\)</span>估算被交换的脸与其分割掩码基于热图编码的脸部地标，<span class="math notranslate nohighlight">\(G_s\)</span>估值源图像的分割掩码；（2）<strong>画中画生成器</strong><span class="math notranslate nohighlight">\(G_c\)</span>对缺失部分进行修复并得出完整的交换脸；（3）使用<strong>分割掩码及混合发生器</strong><span class="math notranslate nohighlight">\(G_b\)</span>混合上述结果，生成最终的输出。通过以上步骤，FSGAN能够降低对数据的依赖和提高模型在不同人物间的泛化性。</p>
<p><strong>基于Transformer的人脸伪造。</strong>
基于Transformer的伪造人脸生成工作也有很多，如Xu
<span id="id130">(<a class="reference internal" href="../chapter_references/zreferences.html#id478" title="Xu, Y., Yin, Y., Jiang, L., Wu, Q., Zheng, C., Loy, C. C., … Wu, W. (2022). Transeditor: transformer-based dual-space gan for highly controllable facial editing. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7683–7692).">Xu <em>et al.</em>, 2022</a>)</span>
提出的<strong>TransEditor</strong>，便是一种基于Transformer
的双空间生成对抗网络，可以进行高度可控的面部编辑。通过引入跨空间注意力机制，两个潜在空间进行有意义的交互。此外，该工作利用TransEditor提供的可控性提出了一种灵活的双空间图像编辑策略。TransEditor的框架如图
<a class="reference internal" href="#fig-transeditor"><span class="std std-numref">图4.4.6</span></a> 所示。</p>
<div class="figure align-default" id="id159">
<span id="fig-transeditor"></span><a class="reference internal image-reference" href="../_images/4.16_transeditor.png"><img alt="../_images/4.16_transeditor.png" src="../_images/4.16_transeditor.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.6 </span><span class="caption-text">TransEditor <span id="id131">(<a class="reference internal" href="../chapter_references/zreferences.html#id478" title="Xu, Y., Yin, Y., Jiang, L., Wu, Q., Zheng, C., Loy, C. C., … Wu, W. (2022). Transeditor: transformer-based dual-space gan for highly controllable facial editing. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7683–7692).">Xu <em>et al.</em>, 2022</a>)</span> 及所采用的StyleGAN2
<span id="id132">(<a class="reference internal" href="../chapter_references/zreferences.html#id593" title="Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., &amp; Aila, T. (2020). Analyzing and improving the image quality of stylegan. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8110–8119).">Karras <em>et al.</em>, 2020</a>)</span> 的架构图</span><a class="headerlink" href="#id159" title="Permalink to this image">¶</a></p>
</div>
<p>具体来说，<span class="math notranslate nohighlight">\(P\)</span>和<span class="math notranslate nohighlight">\(Z\)</span>是两个具有独立映射的潜在空间，它们分别用来完成生成器的输入特征图和逐层样式调制。图像的生成过程可以表示为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-17">
<span class="eqno">(4.4.17)<a class="headerlink" href="#equation-source-chap4-17" title="Permalink to this equation">¶</a></span>\[x=\operatorname{TransEditor}( z,  p).\]</div>
<p>为了确定这两个空间与对应的生成器的集成，在每一层中的生成过程可以表示为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-18">
<span class="eqno">(4.4.18)<a class="headerlink" href="#equation-source-chap4-18" title="Permalink to this equation">¶</a></span>\[\boldsymbol{F}_{i+1}=\operatorname{ModuConv} \left(\boldsymbol{T}_{\boldsymbol{w}_{i}}, \boldsymbol{F}_{i}\right),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\boldsymbol{F}_{i}\)</span>表示前<span class="math notranslate nohighlight">\(i\)</span>-1层生成的特征图，<span class="math notranslate nohighlight">\(\boldsymbol{T}_{\boldsymbol{w}_{i}}\)</span>是由层样式编码<span class="math notranslate nohighlight">\(\boldsymbol{w}_{i}\)</span>决定的调制调解过程。具体地，<span class="math notranslate nohighlight">\(\boldsymbol{T}_{\boldsymbol{w}_{i}}\)</span>通过以下方式来对第<span class="math notranslate nohighlight">\(i\)</span>层的卷积模块的参数进行放缩：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-19">
<span class="eqno">(4.4.19)<a class="headerlink" href="#equation-source-chap4-19" title="Permalink to this equation">¶</a></span>\[\boldsymbol{T}_{\boldsymbol{w}_{i}}: w_{i j k}^{\prime \prime}=w_{i j k}^{\prime} / \sqrt{\sum_{j, k} w_{i j k}^{\prime}+ \epsilon},\]</div>
<p>其中，<span class="math notranslate nohighlight">\(w_{i j k}^{\prime}\)</span>=<span class="math notranslate nohighlight">\(s_{i}\)</span>*<span class="math notranslate nohighlight">\(\boldsymbol{w}_{ijk}\)</span>，同时<span class="math notranslate nohighlight">\(j\)</span>、<span class="math notranslate nohighlight">\(k\)</span>为卷积项。<span class="math notranslate nohighlight">\(F_{i}\)</span>将由放缩参数<span class="math notranslate nohighlight">\(\boldsymbol{w}^{\prime\prime}\)</span>进行卷积。虽然在每一层都进行了调制与解调，但每个特征图都是前一层的卷积结果。因此初始特征图<span class="math notranslate nohighlight">\(F_{0}\)</span>是整个生成过程的基础。由此带来了更多的可控性，能够将学习到的常量输入替换为来自<span class="math notranslate nohighlight">\(P\)</span>空间的潜在输入。</p>
<p>此外，对于整个潜在空间的输入，重塑（reshaping）一个单采样向量本质上是纠缠在一起的。因此两个子空间<span class="math notranslate nohighlight">\(Z\)</span>和<span class="math notranslate nohighlight">\(P\)</span>由单独的子向量构成。为了进一步获得更好的解纠缠属性，利用对偶空间的单独映射可以表示如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-20">
<span class="eqno">(4.4.20)<a class="headerlink" href="#equation-source-chap4-20" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{array}{l}z^{+}= {\left[\begin{array}{c}z_{1}^{+} \\z_{2}^{+} \\\ldots \\z_{n}^{+}\end{array}\right]=\left[\begin{array}{cccc}M_{ z_{1}}  0  \ldots  0 \\0  M_{ z_{2}}  \ldots  0 \\\ldots    \\0  0  \ldots  M_{ z_{n}}\end{array}\right]\left[\begin{array}{c}z_{1} \\z_{2} \\\ldots \\z_{n}\end{array}\right] } \\\boldsymbol{p}^{+}=\left[\begin{array}{c}\boldsymbol{p}_{1}^{+} \\\boldsymbol{p}_{2}^{+} \\\ldots \\\boldsymbol{p}_{n}^{+}\end{array}\right]=\left[\begin{array}{cccc}M_{\boldsymbol{p}_{1}}  0  \ldots  0 \\0  M_{\boldsymbol{p}_{2}}  \ldots  0 \\\ldots    \\0  0  \ldots  M_{\boldsymbol{p}_{n}}\end{array}\right]\left[\begin{array}{c}\boldsymbol{p}_{1} \\\boldsymbol{p}_{2} \\\ldots \\\boldsymbol{p}_{n},\end{array}\right]\end{array}\end{split}\]</div>
<p>其中，每个<span class="math notranslate nohighlight">\(M_{ z_{i}}\)</span>或是<span class="math notranslate nohighlight">\(M_{\boldsymbol{p}_{i}}\)</span>都是一个多层感知机(multilayer
perceptron，MLP)模块。</p>
<p>这两个分离的空间，通过基于<strong>交叉注意力</strong>的交互模块进行相关联。映射的潜在编码<span class="math notranslate nohighlight">\(z^{+}\)</span>被作为关键字<span class="math notranslate nohighlight">\(K\)</span>以及价值<span class="math notranslate nohighlight">\(V\)</span>，潜在编码<span class="math notranslate nohighlight">\(\boldsymbol{p}^{+}\)</span>作为查询<span class="math notranslate nohighlight">\(Q\)</span>。第<span class="math notranslate nohighlight">\(i\)</span>层Transformer中的交互可以写为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-21">
<span class="eqno">(4.4.21)<a class="headerlink" href="#equation-source-chap4-21" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{array}{c}Q=\boldsymbol{p}^{+} W^{Q}, \; K=\left( z^{+}\right)^{l} W^{K}, \; V=\left( z^{+}\right)^{l} W^{V} \\\left( z^{+}\right)^{l+1}=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d_{k}}}\right) V+\left( z^{+}\right)^{l},\end{array}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(W^{Q}\)</span>、<span class="math notranslate nohighlight">\(W^{K}\)</span>、<span class="math notranslate nohighlight">\(W^{V}\)</span>是线性投影矩阵，<span class="math notranslate nohighlight">\(d_{k}\)</span>是潜在编码的公共维数。<span class="math notranslate nohighlight">\(\boldsymbol{p}^{+}\)</span>查询的注意力过程将在<span class="math notranslate nohighlight">\(z^{+}\)</span>上运行。由于<span class="math notranslate nohighlight">\(\boldsymbol{p}^{+}\)</span>仅充当查询的作用，尽管<span class="math notranslate nohighlight">\(z^{+}\)</span>采用了<span class="math notranslate nohighlight">\(\boldsymbol{p}^{+}\)</span>，两个空间依然是分离的。</p>
<p>TransEditor的图像编辑操作是在乘积空间<span class="math notranslate nohighlight">\(\mathcal{Z}^{+} \times \mathcal{P}^{+}\)</span>上进行的，双空间操作可以进一步表示如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-22">
<span class="eqno">(4.4.22)<a class="headerlink" href="#equation-source-chap4-22" title="Permalink to this equation">¶</a></span>\[I_{\left(\mathcal{Z}^{+} \times P^{+}\right)}\left( z^{+}, \boldsymbol{p}^{+}\right)=\left(I_{\mathcal{Z}^{+}}\left( z^{+}\right), I_{\mathcal{P}^{+}}\left(\boldsymbol{p}^{+}\right)\right),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(z^{+}\in\mathcal{Z}^{+}\)</span>，<span class="math notranslate nohighlight">\(\boldsymbol{p}^{+}\in\mathcal{P}^{+}\)</span>，<span class="math notranslate nohighlight">\(I_{i}\)</span>表示在空间<span class="math notranslate nohighlight">\(i\)</span>上的操作。</p>
<p>对于每个属性，使用SVM（支持向量机）分类器在两个分离的潜在空间中训练两个超平面，从而获得<span class="math notranslate nohighlight">\(\mathcal{Z}^{+}\)</span>空间中的法向量<span class="math notranslate nohighlight">\(n_{z}\)</span>，<span class="math notranslate nohighlight">\(\mathcal{P}^{+}\)</span>空间中的法向量<span class="math notranslate nohighlight">\(n_{p}\)</span>。随后，对于采样获得的潜在编码<span class="math notranslate nohighlight">\(z^{+}\)</span>和<span class="math notranslate nohighlight">\(\boldsymbol{p}^{+}\)</span>，可以通过沿<span class="math notranslate nohighlight">\(n_{z}\)</span>方向移动<span class="math notranslate nohighlight">\(\lambda_{z}\)</span>步和<span class="math notranslate nohighlight">\(n_{p}\)</span>方向移动<span class="math notranslate nohighlight">\(\lambda_{p}\)</span>步的方式获得新的潜在编码<span class="math notranslate nohighlight">\(\left( z^{+}+\lambda_{z} * n_{z}, \boldsymbol{p}^{+}+\lambda_{p} * n_{p}\right)\)</span>。对于仅完全包含在一个空间中的属性，则只在对应空间内进行编辑，而更加复杂一点的属性（如性别）则最好是同时使用这两个空间。</p>
<p>如果要对真实图像进行处理，则需要将图像反转回对偶的潜在空间。首先，使用特征金字塔（feature
pyramid）提取输入真实图像的三级特征图。由于<span class="math notranslate nohighlight">\(z^{+}\)</span>空间具有分层结构，因此使用不同的特征来生成每个<span class="math notranslate nohighlight">\(z_{i}^{+}\)</span>。潜在编码<span class="math notranslate nohighlight">\(\boldsymbol{p}^{+}\)</span>仅映射自编码器中的最高级别特征，并作为生成器的初始特征输入注入。上述反转策略能够将真实图像映射到所训练的双潜在空间，可以应用线性潜在操作来执行双空间编辑。通过以上的方法，TransEditor能够实现采用双空间GAN来分离所编辑人脸的样式及具体内容。</p>
<p>基于人脸伪造技术，研究者们同时还构建了一系列的人脸篡改与伪造数据集，包括UADFV
<span id="id133">(<a class="reference internal" href="../chapter_references/zreferences.html#id481" title="Li, Y., Chang, M.-C., &amp; Lyu, S. (2018). In ictu oculi: exposing ai created fake videos by detecting eye blinking. IEEE International Workshop on Information Forensics and Security (pp. 1–7).">Li <em>et al.</em>, 2018</a>)</span> 、DFDC <span id="id134">(<a class="reference internal" href="../chapter_references/zreferences.html#id489" title="Dolhansky, B., Howes, R., Pflaum, B., Baram, N., &amp; Ferrer, C. C. (2019). The deepfake detection challenge (dfdc) preview dataset. arXiv preprint arXiv:1910.08854.">Dolhansky <em>et al.</em>, 2019</a>)</span>
等等。这些数据集的基本信息如 <a class="reference internal" href="#table-4-1"><span class="std std-numref">图4.4.7</span></a> 所列。</p>
<div class="figure align-default" id="id160">
<span id="table-4-1"></span><a class="reference internal image-reference" href="../_images/table_4_1.png"><img alt="../_images/table_4_1.png" src="../_images/table_4_1.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.7 </span><span class="caption-text">常用深度伪造数据集</span><a class="headerlink" href="#id160" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="id135">
<h4><span class="section-number">4.4.2.2. </span>视频伪造<a class="headerlink" href="#id135" title="Permalink to this heading">¶</a></h4>
<p>视频伪造是深度伪造的一个重要分支，与对图像篡改不同，视频伪造需要考虑更多的维度，如时序信息、不同主客体在画面上的相对位置变化、甚至更进一步的音频伪造等。</p>
<p>例如，在生成一个虚拟人物在运动的视频时，需要解决的一个问题就是如何生成合理的动作姿态，与目标人物的运动轨迹、力线、惯性等相符合。在这个问题上一个比较好的模型是<strong>一阶移动模型</strong>（first-order-motion）
<span id="id136">(<a class="reference internal" href="../chapter_references/zreferences.html#id17" title="Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., &amp; Sebe, N. (2019). First order motion model for image animation. Advances in Neural Information Processing Systems, 32.">Siarohin <em>et al.</em>, 2019</a>)</span>
，它将视频的内容分为主体和动作，其中主体表示视频中出现的人物，而动作表示人物的行为。一阶移动模型包括两个主要模块：（1）<strong>运动估计模块</strong>，使用一组学习得到的关键点以及它们的局部仿生变换来预测密集的运动场；和（2）<strong>图像生成模块</strong>，结合从源图像中提取的外观和从驱动视频中得到的运动来模拟目标运动中出现的遮挡。通过上面的框架，一阶移动模型能够将一个随机向量序列映射成为连续的视频帧，最后合成视频。每个向量中的一部分表示主体，另一部分表示动作，在视频的生成过程中，表示主体的部分固定不变。一阶移动模型通过对外观和运动信息进行解耦，实现了通过运动模型推导出目标任务的合理姿势与动作。</p>
<p>如何生成高分辨率的伪造视频也是一个具有挑战性的问题。Tian
<span id="id137">(<a class="reference internal" href="../chapter_references/zreferences.html#id479" title="Tian, Y., Ren, J., Chai, M., Olszewski, K., Peng, X., Metaxas, D. N., &amp; Tulyakov, S. (2021). A good image generator is what you need for high-resolution video synthesis. International Conference on Learning Representations.">Tian <em>et al.</em>, 2021</a>)</span>
等人基于GAN提出了一个框架<strong>MoCoGAN-HD</strong>，利用已有的图像生成器来渲染合成高分辨率视频。MoCoGAN-HD将视频合成问题看作是在预训练的图像生成器的潜在输出空间中寻找运动轨迹的问题。MoCoGAN-HD的主要部分是：（1）<strong>动作生成器</strong>，能够探查到期待的主体的运动轨迹，使其中的主体和动作解耦；和（2）<strong>跨域视频合成</strong>训练，在不同域的不相交数据集上训练图像生成器和运动生成器，从而可以生成训练集中不存在的新视频内容。通过上面的框架，MoCoGAN-HD解决了生成高清伪造视频性能较低、精度不够的问题。MoCoGAN-HD的结构如图
<a class="reference internal" href="#mocogan-hd"><span class="std std-ref">MoCoGAN-HD的网络架构 Tian2021MocoGAN-HD</span></a> 所示。</p>
<div class="figure align-default" id="id161">
<span id="mocogan-hd"></span><a class="reference internal image-reference" href="../_images/4.17_MocoGAN-HD.png"><img alt="../_images/4.17_MocoGAN-HD.png" src="../_images/4.17_MocoGAN-HD.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.8 </span><span class="caption-text">MoCoGAN-HD的网络架构 <span id="id138">(<a class="reference internal" href="../chapter_references/zreferences.html#id479" title="Tian, Y., Ren, J., Chai, M., Olszewski, K., Peng, X., Metaxas, D. N., &amp; Tulyakov, S. (2021). A good image generator is what you need for high-resolution video synthesis. International Conference on Learning Representations.">Tian <em>et al.</em>, 2021</a>)</span></span><a class="headerlink" href="#id161" title="Permalink to this image">¶</a></p>
</div>
<p>具体来说，MoCoGAN-HD采用了预训练的图像生成器（StyleGAN2
<span id="id139">(<a class="reference internal" href="../chapter_references/zreferences.html#id593" title="Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., &amp; Aila, T. (2020). Analyzing and improving the image quality of stylegan. IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8110–8119).">Karras <em>et al.</em>, 2020</a>)</span>
），动作生成器<span class="math notranslate nohighlight">\(G_{M}\)</span>和图像生成器<span class="math notranslate nohighlight">\(G_{I}\)</span>采用两个LSTM网络实现，并用来预测潜在的动作轨迹<span class="math notranslate nohighlight">\(z=\left\{ z_{1}, z_{2}, \ldots, z_{n}\right\}\)</span>，其中<span class="math notranslate nohighlight">\(n\)</span>是所合成视频的帧数。图像生成器<span class="math notranslate nohighlight">\(G_{I}\)</span>能够根据运动轨迹合成每个单独的帧。所生成的视频序列<span class="math notranslate nohighlight">\(\widetilde{V}\)</span>由<span class="math notranslate nohighlight">\(\tilde{ v}=\left\{\tilde{ x}_{1}, \tilde{ x}_{2}, , \tilde{ x}_{n}\right\}\)</span>给出。对于每个合成帧<span class="math notranslate nohighlight">\(\tilde{ x}_{t}\)</span>，都有<span class="math notranslate nohighlight">\(\tilde{ x}_{t} = G_{I}( z_{t})\)</span>。为了训练动作生成器<span class="math notranslate nohighlight">\(G_{M}\)</span>来发现所需的运动轨迹，应用<em>视频判别器</em>来使得生成的运动模式与训练视频的运动模式相似，以及强制帧内容在时间上一致。</p>
<p>动作生成器<span class="math notranslate nohighlight">\(G_{M}\)</span>使用输入编码<span class="math notranslate nohighlight">\(z_{1}\in Z\)</span>来预测连续的潜在编码，潜在空间<span class="math notranslate nohighlight">\(Z\)</span>也由图像生成器共享。形式上，<span class="math notranslate nohighlight">\(G_{M}\)</span>由一个LSTM编码器<span class="math notranslate nohighlight">\(LSTM_{enc}\)</span>及LSTM解码器<span class="math notranslate nohighlight">\(LSTM_{dec}\)</span>组成，分别对<span class="math notranslate nohighlight">\(z_{1}\)</span>进行编码以获得初始隐藏状态以及递归地估计<span class="math notranslate nohighlight">\(n\)</span>-1个连续地隐藏状态：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-23">
<span class="eqno">(4.4.23)<a class="headerlink" href="#equation-source-chap4-23" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{array}{c}h_{1},  c_{1}=\operatorname{LSTM}_{\mathrm{enc}}\left( z_{1}\right) \\h_{t},  c_{t}=\operatorname{LSTM}_{\mathrm{dec}}\left( \epsilon_{t},\left( h_{t-1},  c_{t-1}\right)\right),\end{array}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(h\)</span>和<span class="math notranslate nohighlight">\(c\)</span>分别表示隐藏状态与单元状态，<span class="math notranslate nohighlight">\(\epsilon_{t}\)</span>是从正态分布采样的噪声向量，用来对时刻<span class="math notranslate nohighlight">\(t=2,3, \ldots, n\)</span>处的运动多样性进行建模。</p>
<p><strong>运动解耦。</strong> 为了使用运动残差（motion
residual）来估计运动轨迹，将运动残差建模为潜在空间中一组可解释方向的线性组合。首先对来自<span class="math notranslate nohighlight">\(Z\)</span>的<span class="math notranslate nohighlight">\(m\)</span>个随机采样的潜在向量进行主成分分析以获得基础<span class="math notranslate nohighlight">\(V\)</span>，然后使用<span class="math notranslate nohighlight">\(h_{t}\)</span>和<span class="math notranslate nohighlight">\(V\)</span>来估计从前一帧<span class="math notranslate nohighlight">\(z_{t-1}\)</span>到当前帧<span class="math notranslate nohighlight">\(z_{t}\)</span>的运动方向，可形式化定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-24">
<span class="eqno">(4.4.24)<a class="headerlink" href="#equation-source-chap4-24" title="Permalink to this equation">¶</a></span>\[z_{t}= z_{t-1}+\lambda \cdot  h_{t} \cdot  v,\]</div>
<p>其中，隐藏状态<span class="math notranslate nohighlight">\(h_{t}\in [-1,1]\)</span>，<span class="math notranslate nohighlight">\(\lambda\)</span>用来控制残差给定的步长。</p>
<p><strong>运动多样性。</strong>
引入噪声向量<span class="math notranslate nohighlight">\(\epsilon_{t}\)</span>来控制运动多样性后，会出现<span class="math notranslate nohighlight">\(G_{M}\)</span>无法从训练视频中捕获不同运动模式的问题，同时也无法从一个初始潜在编码来生成不同的视频。为了缓解这些问题，引入互信息损失<span class="math notranslate nohighlight">\(\mathcal{L}_{m}\)</span>来最大化隐藏向量<span class="math notranslate nohighlight">\(h_{t}\)</span>和噪声向量<span class="math notranslate nohighlight">\(\epsilon_{t}\)</span>间的互信息，定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-25">
<span class="eqno">(4.4.25)<a class="headerlink" href="#equation-source-chap4-25" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{M}}=\frac{1}{n-1} \sum_{t=2}^{n} \operatorname{sim}\left(H\left( h_{t}\right),  \epsilon_{t}\right),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\operatorname{sim}( u, v)= u^{T} v /\| u\|\| v\|\)</span>表示<span class="math notranslate nohighlight">\(u\)</span>和<span class="math notranslate nohighlight">\(v\)</span>之间的余弦相似度，<span class="math notranslate nohighlight">\(H\)</span>是一个起映射函数作用的2层感知机。</p>
<p><strong>生成器训练。</strong>
为了训练运动生成器<span class="math notranslate nohighlight">\(G_{M}\)</span>，作者使用一个多尺度视频判别器<span class="math notranslate nohighlight">\(D_{V}\)</span>来判断视频序列是真实的还是合成的，同时<span class="math notranslate nohighlight">\(D_{V}\)</span>使用<span class="math notranslate nohighlight">\(3D\)</span>卷积层来更好地模拟时间动态。MoCoGAN-HD将输入视频序列分成小的<span class="math notranslate nohighlight">\(3D\)</span>块（patch），然后将每个补丁分类为真或者假，在输入序列上平均这些块的预测可以得到最终输出。
此外，为了实现更稳定的训练，输入视频序列中的每一帧都以第一帧为条件，因为它属于预训练图像生成器的分布。生成器<span class="math notranslate nohighlight">\(G_{M}\)</span>和判别器<span class="math notranslate nohighlight">\(D_{V}\)</span>通过以下对抗损失进行训练：</p>
<div class="math notranslate nohighlight" id="equation-source-chap4-26">
<span class="eqno">(4.4.26)<a class="headerlink" href="#equation-source-chap4-26" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{D_{\text{V}}}=\mathbb{E}_{ v \sim p_{v}}\left[\log D_{\text{V}}( v)\right]+\mathbb{E}_{ z_{1} \sim p_{z}}\left[\log \left(1-D_{\text{V}}\left(G_{\text{I}}\left(G_{\text{M}}\left( z_{1}\right)\right)\right)\right)\right].\]</div>
<p>通过以上训练的动作生成器，MoCoGAN-HD将运动建模为传递给图像生成器及生成单个帧的连续潜在编码的残差，可以更高效地生成潜在的运动轨迹，从而能够生成在时间上保持一致的高质量帧，进而生成高清的运动伪造视频。</p>
<div class="figure align-default" id="id162">
<span id="fig-di-gan"></span><a class="reference internal image-reference" href="../_images/4.18_DI-GAN.png"><img alt="../_images/4.18_DI-GAN.png" src="../_images/4.18_DI-GAN.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图4.4.9 </span><span class="caption-text">DI-GAN的生成器与鉴别器 <span id="id140">(<a class="reference internal" href="../chapter_references/zreferences.html#id480" title="Yu, S., Tack, J., Mo, S., Kim, H., Kim, J., Ha, J.-W., &amp; Shin, J. (2022). Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571.">Yu <em>et al.</em>, 2022</a>)</span></span><a class="headerlink" href="#id162" title="Permalink to this image">¶</a></p>
</div>
<p>除了生成高清视频之外，如何生成高质量的长视频也是一个挑战的问题。Yu等人
<span id="id141">(<a class="reference internal" href="../chapter_references/zreferences.html#id480" title="Yu, S., Tack, J., Mo, S., Kim, H., Kim, J., Ha, J.-W., &amp; Shin, J. (2022). Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571.">Yu <em>et al.</em>, 2022</a>)</span>
基于GAN提出了<strong>DI-GAN</strong>，它通过将<strong>隐式神经表示</strong>（implicit
neural
representation，INR）编码到参数化神经网络中，实现了利用视频的隐式神经表示来动态感知。具体的，DI-GAN基于INR的视频生成器
<a class="reference internal" href="#fig-di-gan"><span class="std std-numref">图4.4.9</span></a>
(a)，通过不同的方式来操作空间和时间坐标来改善生成动作的质量
<a class="reference internal" href="#fig-di-gan"><span class="std std-numref">图4.4.9</span></a>
(b)。通过上述方法，DI-GAN能在不观察整个长序列视频帧的情况下高效识别非自然运动，从而节约了生成高质量长视频所需的资源消耗。</p>
</div>
</div>
</div>
<div class="section" id="id142">
<h2><span class="section-number">4.5. </span>本章小结<a class="headerlink" href="#id142" title="Permalink to this heading">¶</a></h2>
<p>本章介绍了针对人工智能数据的四类常见攻击：数据投毒（章节
<a class="reference internal" href="#sec-datapoisonattack"><span class="std std-numref">4.1节</span></a> ）、隐私攻击（章节
<a class="reference internal" href="#sec-privacyinferenceattack"><span class="std std-numref">4.2节</span></a> ）、数据窃取（章节
<a class="reference internal" href="#sec-dataextractionattack"><span class="std std-numref">4.3节</span></a> ）和篡改与伪造（章节
<a class="reference internal" href="#sec-datafakeattack"><span class="std std-numref">4.4节</span></a>
）。其中，投毒攻击通过污染训练数据来阻碍模型的正常学习，总共介绍了六种不同的投毒方式及其攻击特点。隐私攻击在黑盒或白盒模型下对已训练好的目标模型进行访问，通过模型的记忆特性反推有关训练数据的隐私信息，包括成员推理攻击、属性推理攻击以及其他推理攻击。数据窃取相较隐私攻击更近一步，直接逆向模型的原始训练数据。与前三种攻击不同，篡改与伪造是一种技术滥用攻击，其中包括普通篡改和深度伪造，前者通过编辑和修改图像中的物体来生成伪造数据，而后者则更加关注于人脸图像的伪造。实际上，当前人工智能的数据收集和使用方式所引发的担忧长期以来一直存在，而深度学习的迅速发展又加重了这些担忧，因为技术越先进影响的人就会越多。对此，我们需要不断地提出更新更高效的防御技术来防止敏感信息和原始数据的泄露和滥用。</p>
<dl class="footnote brackets">
<dt class="label" id="id143"><span class="brackets"><a class="fn-backref" href="#id85">1</a></span></dt>
<dd><p>矿井里的金丝雀：英国矿工们把金丝雀带入矿井，通过金丝雀的状态来判断是否一氧化碳超标。</p>
</dd>
</dl>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">4. 数据安全：攻击</a><ul>
<li><a class="reference internal" href="#sec-datapoisonattack">4.1. 数据投毒</a><ul>
<li><a class="reference internal" href="#id8">4.1.1. 标签投毒攻击</a></li>
<li><a class="reference internal" href="#id11">4.1.2. 在线投毒攻击</a></li>
<li><a class="reference internal" href="#id14">4.1.3. 特征空间攻击</a></li>
<li><a class="reference internal" href="#id17">4.1.4. 双层优化攻击</a></li>
<li><a class="reference internal" href="#id26">4.1.5. 生成式攻击</a></li>
<li><a class="reference internal" href="#id30">4.1.6. 差别化攻击</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sec-privacyinferenceattack">4.2. 隐私攻击</a><ul>
<li><a class="reference internal" href="#sec-membership-inference-attack">4.2.1. 成员推理攻击</a><ul>
<li><a class="reference internal" href="#id45">4.2.1.1. 影子模型攻击</a></li>
<li><a class="reference internal" href="#id58">4.2.1.2. 指标指导攻击</a></li>
<li><a class="reference internal" href="#id63">4.2.1.3. 联邦推理攻击</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id66">4.2.2. 属性推理攻击</a></li>
<li><a class="reference internal" href="#id72">4.2.3. 其他推理攻击</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sec-dataextractionattack">4.3. 数据窃取</a><ul>
<li><a class="reference internal" href="#id82">4.3.1. 黑盒数据窃取</a></li>
<li><a class="reference internal" href="#id88">4.3.2. 白盒窃取</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sec-datafakeattack">4.4. 篡改与伪造</a><ul>
<li><a class="reference internal" href="#id100">4.4.1. 普通篡改</a></li>
<li><a class="reference internal" href="#id113">4.4.2. 深度伪造</a><ul>
<li><a class="reference internal" href="#id115">4.4.2.1. 人脸伪造</a></li>
<li><a class="reference internal" href="#id135">4.4.2.2. 视频伪造</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id142">4.5. 本章小结</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="chap3.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>3. 人工智能安全基础</div>
         </div>
     </a>
     <a id="button-next" href="chap5.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>5. 数据安全：防御</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>