<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>9. 模型安全：后门防御 &#8212; 人工智能数据与模型安全 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10. 模型安全：窃取攻防" href="chap10.html" />
    <link rel="prev" title="8. 模型安全：后门攻击" href="chap8.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active"><span class="section-number">9. </span>模型安全：后门防御</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/source/chap9.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://trust-ml.github.io/">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/aisecuritybook">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://aisecuritybook.github.io/">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/book_logo_blueBG.png" alt="人工智能数据与模型安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_about_the_book/index.html">关于此书</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%89%8D%E8%A8%80.html">前言</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chap1.html">1. 人工智能与安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap2.html">2. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap3.html">3. 人工智能安全基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap4.html">4. 数据安全：攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap5.html">5. 数据安全：防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap6.html">6. 模型安全：对抗攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap7.html">7. 模型安全：对抗防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap8.html">8. 模型安全：后门攻击</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">9. 模型安全：后门防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">10. 模型安全：窃取攻防</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">11. 未来展望</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/book_logo_blueBG.png" alt="人工智能数据与模型安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_about_the_book/index.html">关于此书</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%89%8D%E8%A8%80.html">前言</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chap1.html">1. 人工智能与安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap2.html">2. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap3.html">3. 人工智能安全基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap4.html">4. 数据安全：攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap5.html">5. 数据安全：防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap6.html">6. 模型安全：对抗攻击</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap7.html">7. 模型安全：对抗防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap8.html">8. 模型安全：后门攻击</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">9. 模型安全：后门防御</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">10. 模型安全：窃取攻防</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">11. 未来展望</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="chap-backdoor-defense">
<span id="id1"></span><h1><span class="section-number">9. </span>模型安全：后门防御<a class="headerlink" href="#chap-backdoor-defense" title="Permalink to this heading">¶</a></h1>
<p>高效地防御多种后门攻击是一个极具挑战性的任务。一方面，不同后门攻击方法的工作机理可能并不相同，想要深入了解它们的共性具有一定的难度；另一方面设计后门防御策略需要同时考虑多种攻击，通用性比较难实现。目前针对后门攻击的防御策略主要有三种：（1）<em>后门模型检测</em>、（2）<em>后门样本检测</em>
和（3）<em>后门移除</em>。目前，这三类防御方法紧密相关又相对独立，各自完成一个后门防御的子任务。本节将对这三类方法展开介绍。</p>
<div class="section" id="sec-backdoor-model-detection">
<span id="id2"></span><h2><span class="section-number">9.1. </span>后门模型检测<a class="headerlink" href="#sec-backdoor-model-detection" title="Permalink to this heading">¶</a></h2>
<p>后门模型检测的目标是判断给定模型是否包含后门触发器，可以根据模型在某种情况下展现出来的<em>后门表现</em>来判断。本节将详细介绍主流后门模型检测方法。</p>
<p><strong>神经净化。</strong> Wang等人 <span id="id3">(<a class="reference internal" href="../chapter_references/zreferences.html#id365" title="Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., &amp; Zhao, B. Y. (2019). Neural cleanse: identifying and mitigating backdoor attacks in neural networks. IEEE Symposium on Security and Privacy (pp. 707–723).">Wang <em>et al.</em>, 2019</a>)</span>
首先提出了基于<em>触发器逆向</em>（trigger reverse
engineering）的后门模型检测方法—<strong>神经净化</strong>（neural
cleanse，NC），开启了后门模型检测这一研究方向。该方法假设防御者知晓模型的所有信息，包括模型参数、模型结构等等；此外，防御者还拥有一些可以在该模型下进行正常预测的输入样本。</p>
<p>该方法的核心出发点是：<strong>相较于正常类别，翻转后门类别所需要的扰动更少</strong>。因此，作者对所有输出类别进行最小像素规模的<strong>标签扰动</strong>，结合<strong>离群点检测</strong>方法识别潜在的后门类别，然后利用逆向工程技术重建后门触发器。</p>
<p>具体的，在后门模型检测阶段，NC方法将后门触发器的注入定义为：</p>
<div class="math notranslate nohighlight" id="equation-nc-optimize">
<span class="eqno">(9.1.1)<a class="headerlink" href="#equation-nc-optimize" title="Permalink to this equation">¶</a></span>\[\begin{split}A( x, m,\Delta) =  x^{'} \\  x_{i,j,c}^{'} = (1- m_{i,j}) \cdot  x_{i,j,c} +  m_{i,j} \cdot \Delta_{i,j,c},\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(A(\cdot,\cdot,\cdot)\)</span>表示将后门触发器<span class="math notranslate nohighlight">\(\Delta\)</span>通过掩码<span class="math notranslate nohighlight">\(m\)</span>注入到原始图片<span class="math notranslate nohighlight">\(x\)</span>中的算法（<span class="math notranslate nohighlight">\(\Delta, x \in \mathbb{R}^{h \times w \times c}\)</span>，<span class="math notranslate nohighlight">\(h, w, c\)</span>分别表示图片的高度、宽度和通道数)。掩码<span class="math notranslate nohighlight">\(m \in \mathbb{R}^{h \times w}\)</span>，表示对单一像素位置处的不同通道使用相同的掩码。注意，<span class="math notranslate nohighlight">\(m_{i,j} \in [0, 1]\)</span>，若<span class="math notranslate nohighlight">\(m_{i,j}=1\)</span>，则表示将<span class="math notranslate nohighlight">\(x\)</span>中对应位置的像素替换为<span class="math notranslate nohighlight">\(\Delta\)</span>中对应的像素，若<span class="math notranslate nohighlight">\(m_{i,j}=0\)</span>，则保持<span class="math notranslate nohighlight">\(x\)</span>中像素不变。这里，<span class="math notranslate nohighlight">\(m\)</span>的连续性定义将有利于后续对<span class="math notranslate nohighlight">\(\Delta\)</span>的优化。
对后门触发器<span class="math notranslate nohighlight">\(\Delta\)</span>的重建可以使用<strong>有目标对抗攻击</strong>方法，即生成<span class="math notranslate nohighlight">\(\Delta\)</span>以使得<span class="math notranslate nohighlight">\(x\)</span>被模型<span class="math notranslate nohighlight">\(f\)</span>预测为目标类别<span class="math notranslate nohighlight">\(y_t\)</span>。此外，假设实际中的后门触发器尺寸较小（这是一个不那么合理的假设），还应该对重建后门触发器的大小进行限制。综合两种优化目标，我们可以得到</p>
<div class="math notranslate nohighlight" id="equation-detect-object">
<span class="eqno">(9.1.2)<a class="headerlink" href="#equation-detect-object" title="Permalink to this equation">¶</a></span>\[\mathop{\mathrm{arg\,min}}_{ m, \Delta} \; \mathbb{E}_{ x\in X} \left[ \mathcal{L}_{\text{CE}}(y_t, f(A( x, m,\Delta))) + \lambda \cdot \| m\|_1\right]，\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CE}}(\cdot)\)</span>表示交叉熵分类损失函；<span class="math notranslate nohighlight">\(\| m\|_1\)</span>表示掩码的<span class="math notranslate nohighlight">\(L_1\)</span>范数以鼓励<span class="math notranslate nohighlight">\(m\)</span>的稀疏性，生成更小的后门触发器；<span class="math notranslate nohighlight">\(\lambda\)</span>为超参数；<span class="math notranslate nohighlight">\(X\)</span>表示所有正确分类的样本集。可见，该目标函数在所有正确分类的样本上，通过最小化包含后门触发器的样本与目标类别的分类损失，同时限制后门触发器的大小来重建<strong>尺寸较小</strong>且满足<strong>错误分类</strong>的后门触发器。该方法对每个类别轮流作为<span class="math notranslate nohighlight">\(y_t\)</span>，并利用Adam优化器
<span id="id4">(<a class="reference internal" href="../chapter_references/zreferences.html#id226" title="Kingma, D. P., &amp; Ba, J. (2015). Adam: a method for stochastic optimization. International Conference on Learning Representations.">Kingma and Ba, 2015</a>)</span> 来重建后门触发器。</p>
<p>基于投毒类别更易于进行有目标攻击的假设，可以根据重建的触发器的样式和大小筛选出<strong>后门类别</strong>。定义逆向得到的每个类别对应的潜在触发器的<span class="math notranslate nohighlight">\(L_1\)</span>范数为<span class="math notranslate nohighlight">\(L=\{L^1, L^2, ..., L^C\}\)</span>，其中<span class="math notranslate nohighlight">\(C\)</span>表示所有类别的数量。那么，可利用<em>中位绝对偏差</em>（median
absolute deviation，MAD）指标 <span id="id5">(<a class="reference internal" href="../chapter_references/zreferences.html#id225" title="Hampel, F. R. (1974). The influence curve and its role in robust estimation. Journal of the American Statistical Association, 69(346), 383–393.">Hampel, 1974</a>)</span>
来衡量离群点，即：</p>
<div class="math notranslate nohighlight" id="equation-mad-one">
<span class="eqno">(9.1.3)<a class="headerlink" href="#equation-mad-one" title="Permalink to this equation">¶</a></span>\[\begin{split}\tilde{L} = \text{median}(L) \\MAD = \text{median}(\|L^i - \tilde{L\|}_1),\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\text{median}(\cdot)\)</span>表示取中位数。在得到MAD后，一个类别的异常分数可定义为：</p>
<div class="math notranslate nohighlight" id="equation-mad-two">
<span class="eqno">(9.1.4)<a class="headerlink" href="#equation-mad-two" title="Permalink to this equation">¶</a></span>\[I_i = \frac{\|L^i - \tilde{L\|}_1}{MAD}.\]</div>
<p>那么对于所有类别，我们可以得到<span class="math notranslate nohighlight">\(I = \{I_1, I_2, ..., I_n\}\)</span>来表示每个类别的异常指标。在假设<span class="math notranslate nohighlight">\(I\)</span>为标准正态分布<span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>的前提下，我们需要使用</p>
<div class="math notranslate nohighlight" id="equation-mad-three">
<span class="eqno">(9.1.5)<a class="headerlink" href="#equation-mad-three" title="Permalink to this equation">¶</a></span>\[I_i = \frac{1}{\Phi^{-1}(\frac{3}{4})} \cdot I_i \approx 1.4826 \cdot I_i\]</div>
<p>将其异常指标放大到与正态分布样本相同的尺度上，作为标准正态分布的<em>一致估计量</em>（consistent
estimator），其中<span class="math notranslate nohighlight">\(\Phi\)</span>表示标准正态分布的累积分布函数(CDF)。因此，当<span class="math notranslate nohighlight">\(I_i &gt; 1.96\sigma\)</span>时有大于95%的概率此类别为离群点，由于为标准正态分布，所以<span class="math notranslate nohighlight">\(I_i &gt; 1.96\)</span>。实际上，NC方法直接使用<span class="math notranslate nohighlight">\(I_i &gt; 2\)</span>作为阈值条件来检测后门类别。</p>
<p>在以上检测过程中，由于需要对每一个类别进行后门触发器的重建，在类别数量很大时会需要较大的计算量。为了进一步降低计算负担，可降低公式
<a class="reference internal" href="#equation-detect-object">(9.1.2)</a>
的优化迭代次数，先得到一个较为粗糙的重建后门触发器，然后对所有类别进行初筛来降低怀疑为投毒类别的数目。</p>
<p><strong>深度检查。</strong> Chen等人 <span id="id6">(<a class="reference internal" href="../chapter_references/zreferences.html#id409" title="Chen, H., Fu, C., Zhao, J., &amp; Koushanfar, F. (2019). Deepinspect: a black-box trojan detection and mitigation framework for deep neural networks. International Joint Conference on Artificial Intelligence (pp. 4658–4664).">Chen <em>et al.</em>, 2019</a>)</span>
提出了针对Trojan攻击的防御方法—<strong>深度检查</strong>（DeepInspect），建立了一种针对模型先验知识较少情况下的后门防御机制。相较于神经净化方法，深度检查方法仅需要模型的类别概率输出，无需训练数据集，因此实用性更高。为了解决没有输入数据的问题，该方法使用<strong>模型逆向</strong>（model
inversion） <span id="id7">(<a class="reference internal" href="../chapter_references/zreferences.html#id277" title="Fredrikson, M., Jha, S., &amp; Ristenpart, T. (2015). Model inversion attacks that exploit confidence information and basic countermeasures. ACM SIGSAC Conference on Computer and Communications Security (pp. 1322–1333).">Fredrikson <em>et al.</em>, 2015</a>)</span>
技术，根据模型的所有输出类别重建了<strong>替代数据集</strong>。然后，基于替代数据集训练<strong>条件生成器</strong>（conditional
generator），以便快速生成不同类别的后门触发器。最后，该方法根据重建的后门触发器对所有的类别进行异常检测，如果发现后门类别，则使用对抗训练的方式对模型进行修复。下面将详细介绍深度检查方法的检测步骤：</p>
<p><strong>（1）替代数据集生成：</strong>深度检查利用模型的反传梯度信息对全零初始化的输入进行优化，使得当前输入的预测类别向指定目标靠近，目标函数为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-0">
<span class="eqno">(9.1.6)<a class="headerlink" href="#equation-source-chap9-0" title="Permalink to this equation">¶</a></span>\[c( x) = 1-f( x,y_t)+\text{AuxInfo}( x),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(x\)</span>为全零初始化的模型输入，<span class="math notranslate nohighlight">\(f(\cdot)\)</span>为输入<span class="math notranslate nohighlight">\(x\)</span>预测为类别<span class="math notranslate nohighlight">\(y_t\)</span>的概率，<span class="math notranslate nohighlight">\(\text{AuxInfo}( x)\)</span>表示针对输入<span class="math notranslate nohighlight">\(x\)</span>可利用的其他辅助信息。通过最小化<span class="math notranslate nohighlight">\(c( x)\)</span>以及计算<span class="math notranslate nohighlight">\(c( x)\)</span>关于输入<span class="math notranslate nohighlight">\(x\)</span>的梯度信息，并对<span class="math notranslate nohighlight">\(x\)</span>进行迭代更新以降低<span class="math notranslate nohighlight">\(c( x)\)</span>。模型逆向方法可使得<span class="math notranslate nohighlight">\(x\)</span>被模型<span class="math notranslate nohighlight">\(f\)</span>预测为<span class="math notranslate nohighlight">\(y_t\)</span>类别的概率变大。由此，可针对不同类别生成替代数据集，并将用于下一阶段条件生成器的训练。</p>
<p><strong>（2）后门触发器重建：</strong>深度检查使用模型<span class="math notranslate nohighlight">\(f\)</span>作为判别器<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>，使用<span class="math notranslate nohighlight">\(\mathcal{G}\)</span>表示条件生成器，以噪声<span class="math notranslate nohighlight">\(z\)</span>和类别<span class="math notranslate nohighlight">\(y_t\)</span>为输出，生成后门触发器<span class="math notranslate nohighlight">\(\Delta\)</span>，即<span class="math notranslate nohighlight">\(\Delta=\mathcal{G}( z, y_t)\)</span>。为了能够让<span class="math notranslate nohighlight">\(\mathcal{G}\)</span>学习到后门触发器的分布，生成器<span class="math notranslate nohighlight">\(\mathcal{G}\)</span>生成的后门触发器应使得判别器<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>发生错误的分类，即：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-1">
<span class="eqno">(9.1.7)<a class="headerlink" href="#equation-source-chap9-1" title="Permalink to this equation">¶</a></span>\[\mathcal{D}( x + \mathcal{G}( z, y_t)) = y_t.\]</div>
<p>这里的<span class="math notranslate nohighlight">\(x\)</span>来自于上一阶段生成的替代数据集。
因此，我们使用<em>负对数似然损失</em>（negative loss likelihood,
NLL）来衡量生成后门触发器的质量：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-2">
<span class="eqno">(9.1.8)<a class="headerlink" href="#equation-source-chap9-2" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\Delta} = \mathbb{E}_{ x}[\mathcal{L}_{\text{NLL}}(\mathcal{D}(x+\mathbb{g}\mathcal{G}( z,y_t)), t)]\]</div>
<p>此外，还应使得后门样本<span class="math notranslate nohighlight">\(x+\mathcal{G}( z,y_t)\)</span>与原始输入<span class="math notranslate nohighlight">\(x\)</span>无法区分，即增加对抗损失：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-3">
<span class="eqno">(9.1.9)<a class="headerlink" href="#equation-source-chap9-3" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{GAN}} = \mathbb{E}_{ x}[\mathcal{L}_\text{MSE}(\mathcal{D}_{prob}( x+\mathcal{G}( z,y_t)), 1)],\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\mathcal{L}_\text{MSE}\)</span>表示<em>均方误差</em>（mean square
error，MSE），<span class="math notranslate nohighlight">\(1\)</span>代表原始输入<span class="math notranslate nohighlight">\(x\)</span>在理想情况下的概率输出。</p>
<p>除以上损失外，还应该对后门触发器的大小进行限制，因为这里同样假设大部分触发器的尺寸很小。深度检查使用<span class="math notranslate nohighlight">\(\|\|_1\)</span>对生成的后门触发器大小进行限制，即：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-4">
<span class="eqno">(9.1.10)<a class="headerlink" href="#equation-source-chap9-4" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{pert}} = \mathbb{E}_{ x}\left[\max(0, ||\mathcal{G}( z,y_t)||_1 - \gamma)\right],\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\gamma\)</span>控制重建后门触发器<span class="math notranslate nohighlight">\(L_1\)</span>范数约束强度，当<span class="math notranslate nohighlight">\(\|\mathcal{G\|( z,y_t)}_1\)</span>大于<span class="math notranslate nohighlight">\(\gamma\)</span>时，<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{pert}}=||\mathcal{G}( z,y_t)||_1 - \gamma\)</span>，否则<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{pert}}=0\)</span>。</p>
<p>综合以上三个损失，我们得到最终用于训练生成器<span class="math notranslate nohighlight">\(\mathcal{G}\)</span>的损失函数：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-5">
<span class="eqno">(9.1.11)<a class="headerlink" href="#equation-source-chap9-5" title="Permalink to this equation">¶</a></span>\[\mathcal{L} = \mathcal{L}_{\Delta} + \lambda_1 \mathcal{L}_{\text{GAN}} + \lambda_2 \mathcal{L}_{\text{pert}},\]</div>
<p>其中，超参数<span class="math notranslate nohighlight">\(\lambda_1\)</span>和<span class="math notranslate nohighlight">\(\lambda_2\)</span>用来调节不同损失项的权重。通过调整超参数，可以保证由生成器<span class="math notranslate nohighlight">\(\mathcal{G}\)</span>生成的后门触发器具有95%以上的攻击成功率。</p>
<p><strong>(3)异常检测：</strong>与神经净化NC方法类似，深度检查方法基于投毒类别的<strong>重建后门触发器小于其他类别</strong>这一特点，使用<em>双中值绝对偏差</em>（double
median absolute
deviation，DMAD）来作为检测标准。MAD方法适用于围绕中位数的对抗分布，而对于左偏、右偏等其他类型的非对称分布而言，效果会发生降低，而利用DMAD则可解决这个问题。定义各类别重建后门触发器的噪声规模大小为<span class="math notranslate nohighlight">\(S=\{S_1, S_2, ..., S_t, ..., S_C\}\)</span>，其中<span class="math notranslate nohighlight">\(C\)</span>为类别总数，那么可以得到整体的中位数为<span class="math notranslate nohighlight">\(\tilde{S} = \text{median}(S)\)</span>。根据<span class="math notranslate nohighlight">\(S\)</span>中每个值与<span class="math notranslate nohighlight">\(\tilde{S}\)</span>的大小关系，可将<span class="math notranslate nohighlight">\(S\)</span>划分为左右两部分：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-6">
<span class="eqno">(9.1.12)<a class="headerlink" href="#equation-source-chap9-6" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}S^{l} = \{S_i | S_i \in S \And S_i\leq \tilde{S}\} \\S^{r} = \{S_i | S_i \in S \And S_i\geq \tilde{S}\}.\end{split}\end{split}\]</div>
<p>由于假设所测试的后门触发器普遍较小，因此深度检查方法只使用<span class="math notranslate nohighlight">\(S^{l}\)</span>来进行检测，根据式
<a class="reference internal" href="#equation-mad-one">(9.1.3)</a> 、 <a class="reference internal" href="#equation-mad-two">(9.1.4)</a> 、 <a class="reference internal" href="#equation-mad-three">(9.1.5)</a>
对<span class="math notranslate nohighlight">\(S^{l}\)</span>进行计算，并与<span class="math notranslate nohighlight">\(1.96\sigma\)</span>进行比较来得到离群点，确定为后门类别。</p>
<p>通过以上三个步骤，深度检查方法实现了比神经净化更有效的后门触发器重建，且不需要借助额外数据。相较于神经净化方法，深度检查方法在更加苛刻的环境下实现了后门攻击的有效检测，更易于在实际应用场景中的使用。此外，通过重建后门触发器，深度检查方法同样可以在替代数据集以及叠加了重建后门触发器后的“修补”数据集上，对模型进行微调，使得模型具有抵御后门攻击的能力。值得注意的是，尽管深度检查方法声称是一种黑盒防御方法。但事实上，在模型逆向以及生成器的训练过程中，都需要通过模型反传梯度来进行优化，这在严格的黑盒条件下是不允许的。因此，这里并没有把深度检查方法归类为黑盒防御方法。</p>
<p><strong>基于非凸优化和正则的后门检查。</strong> Guo等人 <span id="id8">(<a class="reference internal" href="../chapter_references/zreferences.html#id408" title="Guo, W., Wang, L., Xing, X., Du, M., &amp; Song, D. (2019). Tabor: a highly accurate approach to inspecting and restoring trojan backdoors in ai systems. arXiv preprint arXiv:1908.01763.">Guo <em>et al.</em>, 2019</a>)</span>
发现当使用不同尺寸、形状和位置的后门触发器来进行后门攻击时，神经净化方法可能会失效。作者认为其主要原因在于后门子空间中存在多个后门样本，而基于触发器逆向的神经净化方法可能会搜索到与后门触发器无关的后门样本。为此，作者提出了<strong>基于非凸优化和正则的后门检查</strong>（Trojan
backdoor inspection based on non-convex optimization and
regularization，TABOR）方法，通过多个训练正则项来改善原始神经净化的优化目标。下面将对增加的正则化项进行详细介绍。</p>
<p>在利用优化器（式 <a class="reference internal" href="#equation-nc-optimize">(9.1.1)</a>
）重建后门触发器的过程中，会出现<strong>稀疏触发器</strong>（scattered
trigger）和<strong>过大触发器</strong>（overly large
trigger）的问题。稀疏触发器在整个图像区域中比较分散，无法聚拢到某一个特定的位置；而过大触发器在图像区域中所占的面积过大，远大于正常后门触发器的尺寸。因此，需要对重建后门触发器的面积大小以及聚拢程度进行限制。</p>
<p>针对过大触发器，TABOR定义如下正则化项来惩罚面积过大：</p>
<div class="math notranslate nohighlight" id="equation-tabor-reg-1">
<span class="eqno">(9.1.13)<a class="headerlink" href="#equation-tabor-reg-1" title="Permalink to this equation">¶</a></span>\[R_1( m, \Delta) = \lambda_1 \cdot R_{\text{elastic}}(vec( m)) + \lambda_2 \cdot R_{\text{elastic}}(vec((1- m) \odot \Delta)),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(vec(\cdot)\)</span>表示将矩阵转换为向量；<span class="math notranslate nohighlight">\(R_{\text{elastic}}(\cdot)\)</span>表示向量的<span class="math notranslate nohighlight">\(L_1\)</span>和<span class="math notranslate nohighlight">\(L_2\)</span>范数之和；<span class="math notranslate nohighlight">\(\lambda_1\)</span>和<span class="math notranslate nohighlight">\(\lambda_2\)</span>为超参数。这里的<span class="math notranslate nohighlight">\(m\)</span>表示后门触发器的掩码，<span class="math notranslate nohighlight">\(\Delta\)</span>表示重建的后门触发器。公式
<a class="reference internal" href="#equation-tabor-reg-1">(9.1.13)</a>
在对掩码<span class="math notranslate nohighlight">\(m\)</span>中非零项进行惩罚的同时，也对掩码0值区域的重建后门触发器的非零项进行惩罚，从而解决过大的问题，缩小后门子空间中后门样本的数量。</p>
<p>针对稀疏触发器，TABOR定义如下正则化项来惩罚稀疏性：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-7">
<span class="eqno">(9.1.14)<a class="headerlink" href="#equation-source-chap9-7" title="Permalink to this equation">¶</a></span>\[\begin{split}R_2(v, \Delta) = \lambda_3 \cdot s( m) + \lambda_4 \cdot s((1- m) \odot \Delta) \\s( m) = \sum_{i,j}( m_{i,j}- m_{i,j+1})^2 + \sum_{i,j}( m_{i,j}- m_{i+1,j})^2,\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\lambda_3\)</span>和<span class="math notranslate nohighlight">\(\lambda_4\)</span>为超参数，<span class="math notranslate nohighlight">\(s(\cdot)\)</span>为平滑度量函数，用来表示零或者非零值的密度，<span class="math notranslate nohighlight">\(m_{i,j}\)</span>表示第<span class="math notranslate nohighlight">\(i\)</span>行第<span class="math notranslate nohighlight">\(j\)</span>列的元素。可以看到，当重建的后门触发器越稀疏时，其<span class="math notranslate nohighlight">\(R_2\)</span>值会更高。因此，TABOR中使用该正则项进一步缩小搜索空间。</p>
<p>此外，重建的后门触发器有时还会遮挡图像中的主要物体，称之为<strong>遮挡触发器</strong>（blocking
trigger），但后门触发器的成功往往不依赖于遮盖主要物体，而在于高响应值。因此，TABOR定义如下正则化项来避免遮盖主要物体：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-8">
<span class="eqno">(9.1.15)<a class="headerlink" href="#equation-source-chap9-8" title="Permalink to this equation">¶</a></span>\[R_3 = \lambda_5 \cdot \mathcal{L}(f( x \odot (1- m)), y),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(y\)</span>表示<span class="math notranslate nohighlight">\(x\)</span>预测的正确类别；<span class="math notranslate nohighlight">\(x\odot(1- m)\)</span>表示触发器位置以外的像素；<span class="math notranslate nohighlight">\(\lambda_5\)</span>为超参数。如果<span class="math notranslate nohighlight">\(x\)</span>在去除了触发器区域像素后仍能正确分类，则表示后门触发器的位置远离模型决策所依赖的关键区域，从而实现不遮盖主要物体的目的。</p>
<p>最后，还存在<strong>叠加触发器</strong>（overlaying
trigger）的情况，重建触发器与真实触发器具有一定程度的叠加。为了缓解该现象，TABOR从特征重要性的角度入手，要求重建的后门触发器可以满足攻击模型的目的，从而去除不重要的部分，使得重建后的后门触发器更加精简、准确。该正则项定义为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-9">
<span class="eqno">(9.1.16)<a class="headerlink" href="#equation-source-chap9-9" title="Permalink to this equation">¶</a></span>\[R_4 = \lambda_6 \cdot \mathcal{L}(f( m \odot \Delta), y_t),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(y_t\)</span>为攻击目标类别；<span class="math notranslate nohighlight">\(\lambda_6\)</span>为超参数。基于此，TABOR可在较为重要的区域里重建后门触发器。</p>
<p>将以上四个正则化项加入到NC方法的目标函数（公式 <a class="reference internal" href="#equation-nc-optimize">(9.1.1)</a>
）后即可生成更加准确的后门触发器。当然，最后还需要通过异常检测来找到后门类别和后门触发器。相较于神经净化的<span class="math notranslate nohighlight">\(L_1\)</span>距离筛选法，TABOR给出了更加精确、具体的度量定义：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-10">
<span class="eqno">(9.1.17)<a class="headerlink" href="#equation-source-chap9-10" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}A( m_t, \Delta_t) = \log(\frac{\|vec(f^{(t)})\|_1}{d^2}) + \log (\frac{s(f^{(t)})}{d\cdot(d-1)}) \\- \log(acc_{\text{att}}) - \log(acc_{\text{crop}}) - \log(acc_{\text{exp}})，\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(m_t\)</span>和<span class="math notranslate nohighlight">\(\Delta_t\)</span>可构成类别<span class="math notranslate nohighlight">\(y_t\)</span>的后门触发器，即<span class="math notranslate nohighlight">\(m_t \odot \Delta_t\)</span>；<span class="math notranslate nohighlight">\(acc_{\text{att}}\)</span>表示将重建后门触发器注入到干净样本中后导致的错误分类率；<span class="math notranslate nohighlight">\(acc_{\text{crop}}\)</span>为在从污染图像中裁剪出相应的后门触发器后得到的预测准确率；<span class="math notranslate nohighlight">\(acc_{\text{exp}}\)</span>表示仅将污染图像中的基于可解释性得到的重要特征输入模型后得到的预测准确率。对于前两项，定义<span class="math notranslate nohighlight">\(f_{i,j}^{(t)}=\mathbb{1}( m_t \odot \Delta_t)_{i,j}&gt;0\)</span>，通过<span class="math notranslate nohighlight">\(\|vec(f^{(t)}\|_1\)</span>以及<span class="math notranslate nohighlight">\(s(f^{(t)})\)</span>来实现稀疏度量和平滑性度量；<span class="math notranslate nohighlight">\(d\)</span>表示图像的维度，用来归一化。
在得到每个类别重建触发器的度量指标后，可利用MAD（公式 <a class="reference internal" href="#equation-mad-one">(9.1.3)</a>
、 <a class="reference internal" href="#equation-mad-two">(9.1.4)</a> 和 <a class="reference internal" href="#equation-mad-three">(9.1.5)</a> ）进行异常检测。</p>
<p>针对过多的超参数（<span class="math notranslate nohighlight">\(\{\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5, \lambda_6\}\)</span>），TABOR设计了一种<strong>超参增强</strong>机制，缓解超参对触发器重建性能的影响。在优化的初始阶段，将超参数初始化为较小的值，也就是说在此阶段中正则化项对于目标函数的贡献接近0；之后会将当前重建的后门触发器注入到干净样本中以获得污染样本，并将污染样本输入到模型中获取错误分类率，<em>只有当错误分类率达到一个确定的阈值时</em>，才会将超参数乘上固定的扩大因子<span class="math notranslate nohighlight">\(\gamma\)</span>；否则，将会除以<span class="math notranslate nohighlight">\(\gamma\)</span>。以上操作会重复进行，直到正则化项的值趋于稳定，即<span class="math notranslate nohighlight">\(|R_t^{k-1} - R_t^{k}| &lt; \epsilon\)</span>，其中<span class="math notranslate nohighlight">\(R_t^k\)</span>表示在第<span class="math notranslate nohighlight">\(k\)</span>次迭代中第<span class="math notranslate nohighlight">\(t\)</span>个正则项的值；<span class="math notranslate nohighlight">\(\epsilon\)</span>为一个较小的常数。</p>
<p>TABOR在神经净化框架下，增加了多种正则化项来解决其在后门触发器重建过程中存在的问题。此外，还从多个角度设计了更加全面的度量标准，以获得更好的异常检测性能。结果显示，TABOR方法相较于神经净化方法在后门触发器重建及检测上取得了更好的效果。</p>
<p><strong>数据受限下的检测。</strong> Wang等人 <span id="id9">(<a class="reference internal" href="../chapter_references/zreferences.html#id410" title="Wang, R., Zhang, G., Liu, S., Chen, P.-Y., Xiong, J., &amp; Wang, M. (2020). Practical detection of trojan neural networks: data-limited and data-free cases. European Conference on Computer Vision (pp. 222–238).">Wang <em>et al.</em>, 2020</a>)</span>
在<strong>数据受限</strong>（Data-limited）及<strong>无数据</strong>（data-free）的情况下分别提出了有效的后门类别检测方法—<strong>DL-TND</strong>（TrojanNet
detector）和<strong>DF-TND</strong>。其中，DL-TND在每个类别仅有一张图像的条件下，针对每个类别分别计算其非目标通用对抗噪声（untargeted
universal adversarial noise）和单一图像的有目标对抗噪声（targeted
adversarial
noise），并比较两种噪声之间的差异，从而识别后门类别。DF-TND不使用原始图像，而利用随机图像作为检测数据，它通过最大化中间层神经元激活来获取扰动图像，然后根据随机图像与扰动图像在输出概率上的差异来检测后门类别。下面将分别对DL-TND和DF-TND进行详细介绍。</p>
<p>类似于公式 <a class="reference internal" href="#equation-nc-optimize">(9.1.1)</a> ，DL-TND定义添加了后门触发器的图片为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-11">
<span class="eqno">(9.1.18)<a class="headerlink" href="#equation-source-chap9-11" title="Permalink to this equation">¶</a></span>\[\hat{ x}( m, \Delta) = (1- m) \cdot  x +  m \cdot \delta,\]</div>
<p>其中，<span class="math notranslate nohighlight">\(0 \leq \Delta \leq 255\)</span>为触发器噪声，<span class="math notranslate nohighlight">\(m \in\{0,1\}\)</span>为定义了添加位置的二值掩码。
在数据限制下，可针对每个类别获取一张照片，因此DL-TND使用<span class="math notranslate nohighlight">\(D_k\)</span>表示类别<span class="math notranslate nohighlight">\(k\)</span>的图片集合，<span class="math notranslate nohighlight">\(D_{k-}\)</span>表示不同于类别<span class="math notranslate nohighlight">\(k\)</span>的其他图片集合。</p>
<p>首先，DL-TND利用<span class="math notranslate nohighlight">\(D_{k-}\)</span>数据集获取<strong>非目标通用对抗噪声</strong>，即在<span class="math notranslate nohighlight">\(D_{k-}\)</span>中增加噪声<span class="math notranslate nohighlight">\(u^{(k)}\)</span>使得模型发生错误分类。与此同时，<span class="math notranslate nohighlight">\(u^{k}\)</span>不会影响<span class="math notranslate nohighlight">\(D_k\)</span>在模型上的分类。通过模拟后门触发器只干扰<span class="math notranslate nohighlight">\(D_{k-}\)</span>样本的特性，来进行噪声模拟。形式化定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-12">
<span class="eqno">(9.1.19)<a class="headerlink" href="#equation-source-chap9-12" title="Permalink to this equation">¶</a></span>\[\mathop{\mathrm{arg\,min}}_{ m, \Delta} \ \ \mathcal{L}_{\text{atk}}(\hat{ x}( m, \Delta); D_{k-}) + \hat{\mathcal{L}}_{\text{atk}}(\hat{ x}( m, \Delta); D_{k}) + \lambda \| m\|_1,\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\lambda\)</span>为超参数，<span class="math notranslate nohighlight">\(\| m\|_1\)</span>用来保证稀疏性。公式中的前两项损失分别定义为：</p>
<div class="math notranslate nohighlight" id="equation-cw-format">
<span class="eqno">(9.1.20)<a class="headerlink" href="#equation-cw-format" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}\mathcal{L}_{\text{atk}}(\hat{ x}( m, \Delta); D_{k-}) = \sum_{ x_i \in D_{k-}} \max\{f_{y_i}(\hat{ x}_i( m, \Delta)) - \max_{t \neq y_i}f_t(\hat{ x}_i( m, \Delta)), -\tau\} \\\hat{\mathcal{L}}_{\text{atk}}(\hat{ x}( m, \Delta); D_{k}) = \sum_{ x_i \in D_{k}} \max\{\max_{t \neq y_i}f_t(\hat{ x}_i( m, \Delta)) - f_{y_i}(\hat{ x}_i( m, \Delta)), -\tau\},\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(y_i\)</span>为<span class="math notranslate nohighlight">\(x_i\)</span>的真实类别，<span class="math notranslate nohighlight">\(f_t(\cdot)\)</span>表示类别<span class="math notranslate nohighlight">\(t\)</span>的预测值，<span class="math notranslate nohighlight">\(\tau \geq 0\)</span>为超参，用来界定攻击的最低置信程度。</p>
<p>公式 <a class="reference internal" href="#equation-cw-format">(9.1.20)</a> 参考了CW攻击 <span id="id10">(<a class="reference internal" href="../chapter_references/zreferences.html#id77" title="Carlini, N., &amp; Wagner, D. (2017). Towards evaluating the robustness of neural networks. IEEE Symposium on Security and Privacy (pp. 39–57).">Carlini and Wagner, 2017</a>)</span>
的非目标攻击形式，使得<span class="math notranslate nohighlight">\(D_{K-}\)</span>发生非目标错误分类而不影响<span class="math notranslate nohighlight">\(D_k\)</span>的正确分类。
此时，DL-TND会对每个图像<span class="math notranslate nohighlight">\(x_i \in D_{k-}\)</span>分别计算<strong>目标对抗噪声</strong>，使得<span class="math notranslate nohighlight">\(\hat{ x}_i( m, \Delta)\)</span>会被模型错分类为<span class="math notranslate nohighlight">\(k\)</span>。DL-TND认为目标攻击与非目标通用攻击一样，偏向使用<em>后门捷径</em>（backdoor
shortcut）来生成对抗噪声。目标攻击定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-13">
<span class="eqno">(9.1.21)<a class="headerlink" href="#equation-source-chap9-13" title="Permalink to this equation">¶</a></span>\[\mathop{\mathrm{arg\,min}}_{ m, \Delta} \ \ \mathcal{L}^{'}_{\text{atk}}(\hat{ x}( m, \Delta); D_{k-}) + \lambda\| m\|_1,\]</div>
<p>其中，第一项同样采用CW攻击形式，定义为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-14">
<span class="eqno">(9.1.22)<a class="headerlink" href="#equation-source-chap9-14" title="Permalink to this equation">¶</a></span>\[\mathcal{L}^{'}_{\text{atk}}(\hat{ x}( m, \Delta); D_{k-}) = \sum_{ x_i \in D_{k-}} \max\{\max_{t \neq k}f_t(\hat{ x}_i( m, \Delta)) - f_{k}(\hat{ x}_i( m, \Delta)), -\tau\}.\]</div>
<p>通过上式可针对每个类别<span class="math notranslate nohighlight">\(k\)</span>和对应的图像<span class="math notranslate nohighlight">\(x_i\)</span>生成噪声<span class="math notranslate nohighlight">\(s^{k,i} = ( m^{k,i}, \Delta^{k,i})\)</span>。
因此，根据相似性假设，当某个类别中存在后门触发器时，<span class="math notranslate nohighlight">\(u^{k}\)</span>和<span class="math notranslate nohighlight">\(s^{k,i}\)</span>应该具有高度的相似性。DL-TND叠加两种噪声后，在模型中间层的特征上计算余弦相似度。对于<span class="math notranslate nohighlight">\(x_i \in D_{k-1}\)</span>分别执行上述步骤，可以得到相似度得分向量<span class="math notranslate nohighlight">\(v_{\text{sim}}^{k}\)</span>。
最后，可通过MAD或者人为设定的阈值进行后门类别检测。</p>
<p>在无数据的情况下，可通过最大化随机输入<span class="math notranslate nohighlight">\(x\)</span>在模型中间层的神经元激活来生成噪声，其优化目标定义为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-15">
<span class="eqno">(9.1.23)<a class="headerlink" href="#equation-source-chap9-15" title="Permalink to this equation">¶</a></span>\[\mathop{\mathrm{arg\,max}}_{ m, \Delta,  w} \sum_{i=1}^{d}[ w_i r_i(\hat{ x}( m, \Delta))] - \lambda \| m\|_1,\]</div>
<p>其中，<span class="math notranslate nohighlight">\(r_i(\cdot)\)</span>表示第<span class="math notranslate nohighlight">\(i\)</span>维的神经元激活值，<span class="math notranslate nohighlight">\(0 \leq w \leq 1\)</span>且<span class="math notranslate nohighlight">\(\sum_i w = 1\)</span>，用来调整不同神经元的重要性，<span class="math notranslate nohighlight">\(d\)</span>表示模型中间层的维度。上式对于<span class="math notranslate nohighlight">\(n\)</span>个随机输入可以优化得到<span class="math notranslate nohighlight">\(n\)</span>个掩码和触发器对<span class="math notranslate nohighlight">\(\{p^{(i)} = ( m^{(i)}, \Delta^{(i)})\}_{i=1}^{n}\)</span>。</p>
<p>最后，基于<span class="math notranslate nohighlight">\(\{p^{(i)}\}_{i=1}^{n}\)</span>，通过比较随机输入与其后门版本在模型输出上的差异来检测后门类别：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-16">
<span class="eqno">(9.1.24)<a class="headerlink" href="#equation-source-chap9-16" title="Permalink to this equation">¶</a></span>\[R_k = \frac{1}{N} \sum_{i}^{N}[f_k(\hat{ x_i}(p^{(i)})) - f_k( x_i)].\]</div>
<p>根据上式可计算每个类别输出差值，进而可基于预先设定的阈值来进行检测，<span class="math notranslate nohighlight">\(R_k\)</span>值越大表示后门风险越大。</p>
</div>
<div class="section" id="sec-backdoor-sample-detection">
<span id="id11"></span><h2><span class="section-number">9.2. </span>后门样本检测<a class="headerlink" href="#sec-backdoor-sample-detection" title="Permalink to this heading">¶</a></h2>
<p><strong>后门样本检测</strong>的目标是识别训练数据集或者测试数据集中的后门样本，其中对训练样本的检测可以帮助防御者清洗训练数据，而对测试样本的检测可以在模型部署阶段发现并拒绝后门攻击行为。下面介绍几种经典的后门样本检测方法。</p>
<p><strong>频谱指纹。</strong> 为了检测训练数据中可能存在的后门样本，Tran等人
<span id="id12">(<a class="reference internal" href="../chapter_references/zreferences.html#id364" title="Tran, B., Li, J., &amp; Madry, A. (2018). Spectral signatures in backdoor attacks. Advances in Neural Information Processing Systems, 31.">Tran <em>et al.</em>, 2018</a>)</span> 在2018年提出了<strong>频谱指纹</strong>（spectral
signature,
SS）方法。该方法观察到后门样本和干净样本在深度特征的协方差矩阵上存在差异，故可以通过检测这种差异来过滤后门样本。</p>
<p>算法 <a class="reference internal" href="#algorithm-9-1"><span class="std std-numref">图9.2.1</span></a>
中给出了SS方法的检测流程。具体来说，给定训练样本<span class="math notranslate nohighlight">\(D\)</span>，首先训练得到神经网络<span class="math notranslate nohighlight">\(f\)</span>。然后，按照类别遍历，并提取每个样本的特征向量并计算每类样本的特征均值。接下来，对深度特征的<em>协方差矩阵</em>进行奇异值分解，并使用该分解计算每个样本的异常值分数。根据异常检测规则移除数据集中异常值高于<span class="math notranslate nohighlight">\(1.5 \epsilon\)</span>的样本（后门样本），最终返回一个干净的训练数据集<span class="math notranslate nohighlight">\(D_{\text{clean}}\)</span>。</p>
<div class="figure align-default" id="id28">
<span id="algorithm-9-1"></span><a class="reference internal image-reference" href="../_images/algorithm_9_1.png"><img alt="../_images/algorithm_9_1.png" src="../_images/algorithm_9_1.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图9.2.1 </span><span class="caption-text">频谱指纹（SS）检测算法</span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id29">
<span id="eq-cp"></span><span id="eq-cd"></span><span id="algorithm-9-2"></span><a class="reference internal image-reference" href="../_images/algorithm_9_2.png"><img alt="../_images/algorithm_9_2.png" src="../_images/algorithm_9_2.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图9.2.2 </span><span class="caption-text">激活聚类（AC）检测算法</span><a class="headerlink" href="#id29" title="Permalink to this image">¶</a></p>
</div>
<p><strong>激活聚类。</strong> Chen等人 <span id="id13">(<a class="reference internal" href="../chapter_references/zreferences.html#id363" title="Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T., … Srivastava, B. (2018). Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728.">Chen <em>et al.</em>, 2018</a>)</span>
提出了一种基于<strong>激活聚类</strong>（activation
clustering，AC）的方法来过滤训练数据中的潜在后门样本。该方法的主要思想是：<em>后门特征和干净特征之间存在差异，且这种差异在深度特征空间中会更加显著</em>。因此，可以基于聚类方法来自动分离后门特征，进而帮助检测后门样本。</p>
<p>算法 <a class="reference internal" href="#algorithm-9-1"><span class="std std-numref">图9.2.1</span></a>
中给出了AC方法的检测流程。该方法首先在后门训练数据集<span class="math notranslate nohighlight">\(D\)</span>上训练模型<span class="math notranslate nohighlight">\(f\)</span>，然后对所有训练样本<span class="math notranslate nohighlight">\(x_{i} \in D\)</span>提取其特征激活（默认选取模型最后一个隐藏层的输出），得到一个包含所有特征激活的集合<span class="math notranslate nohighlight">\(A\)</span>。然后，对得到的特征激活进行降维，并利用聚类方法对训练数据集进行聚类分析。实验表明，通过分析最后一层隐藏层的激活分布就能够有效检测后门数据。</p>
<p>值得注意的是，AC方法假定训练数据中一定存在后门样本，所以在激活聚类时默认将整个数据集划分为两个数据簇。作者提出了三个后门数据簇的判别依据：（1）<strong>重新训练分类</strong>；（2）<strong>聚类簇的相对大小</strong>；（3）<strong>轮廓的分数</strong>。实验表明，比较聚类簇数据规模的相对大小可以作为一个简单有效的评判依据。</p>
<p><strong>认知蒸馏。</strong>
后门样本会误导模型去识别任务无关的后门触发器样式，这使得模型的认识逻辑发生错误，模型在后门样本上的注意力区域发生偏移。但事实并非如此，我们发现模型在后门样本上依然会关注一部分有意义的区域。这就需要一种技术可以精准地剖离模型的核心认知逻辑，去除不重要的甚至是噪声的注意力，从而可以暴露模型真正关注的地方。基于此种想法，Huang等人
<span id="id14">()</span> 提出 <strong>认知蒸馏</strong> （cognitive
distillation，CD）的概念，给定一个输入样本（训练样本或测试样本皆可），此方法提取模型得到同样输出所需要的最少输入信息。决定模型输出的最小输入模式又称为
<strong>认知模式</strong> （cognitive
pattern），它揭示了模型推理结果背后所隐藏的决定性因素。</p>
<p>具体来说，认知蒸馏方法通过解决一个最小化问题来从一个输入样本<span class="math notranslate nohighlight">\(x\)</span>中蒸馏出其认知模式。优化问题具体定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-17">
<span class="eqno">(9.2.1)<a class="headerlink" href="#equation-source-chap9-17" title="Permalink to this equation">¶</a></span>\[\mathrm{arg\,min}_{m}||f_{\theta}(x) - f_{\theta}(x_{cp})||_1 + \alpha ||m||_1 + \beta \cdot TV(m)\]</div>
<div class="math notranslate nohighlight" id="equation-source-chap9-18">
<span class="eqno">(9.2.2)<a class="headerlink" href="#equation-source-chap9-18" title="Permalink to this equation">¶</a></span>\[x_{cp}=x\odot{m} + (1-m)\odot\delta,\]</div>
<p>其中，<span class="math notranslate nohighlight">\(m\)</span>是输入掩码，<span class="math notranslate nohighlight">\(f_{\theta}\)</span>是模型的逻辑或者概率输出，<span class="math notranslate nohighlight">\(x_{cp}\)</span>是认知模式，<span class="math notranslate nohighlight">\(\delta \in [0,1]^c\)</span>是一个<span class="math notranslate nohighlight">\(c\)</span>-维的随机向量，<span class="math notranslate nohighlight">\(\odot\)</span>是元素对应乘积操作，<span class="math notranslate nohighlight">\(TV(\cdot)\)</span>是总变差（TV）损失，控制优化所得掩码的平滑程度（我们希望得到局部平滑的关键区域），<span class="math notranslate nohighlight">\(\beta\)</span>为平衡TV损失的超参数。</p>
<div class="figure align-default" id="id30">
<span id="fig-cd"></span><a class="reference internal image-reference" href="../_images/9.1_cd.png"><img alt="../_images/9.1_cd.png" src="../_images/9.1_cd.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图9.2.3 </span><span class="caption-text">在干净或者后门图片上通过认知蒸馏方法提取出来的认知模式</span><a class="headerlink" href="#id30" title="Permalink to this image">¶</a></p>
</div>
<p>图 <a class="reference internal" href="#fig-cd"><span class="std std-numref">图9.2.3</span></a>
展示了认知蒸馏算法在干净图片以及被11种后门攻击算法攻击过的图片上抽取到的掩码和模式。可以看到，模型在被添加了后门触发器的图片上的认知机理大都由后门触发器决定，触发器对应的位置对模型的输入起到了决定性的作用。此外还发现，有些全图的触发器，比如Blend、CL、FC、DFST等攻击所使用的触发器，只是部分在起作用，而并不需要全图大小的触发器。基于此发现，Huang等人对已有攻击的触发器进行了进一步简化，发现简化后的触发器跟原触发器攻击效果相似，有的甚至还变更强了。</p>
<p>基于式 <code class="xref eq docutils literal notranslate"><span class="pre">eq_cd</span></code>
优化得到的掩码<span class="math notranslate nohighlight">\(m\)</span>，Huang等人构建了一个后门样本检测器，将检测掩码过小的样本检测为后门样本。检测原理是基于图
<a class="reference internal" href="#fig-cd"><span class="std std-numref">图9.2.3</span></a>
中的发现：后门样本的认知模式往往更简单，即模型通过过于简单的模式对样本进行了结果预测。在3个数据集、6个模型以及12种后门攻击上的检测结果表明，认知蒸馏方法可以将训练集中后门样本的平均检测AUC从此前最优的84.62%提升到96.45%，将测试集中后门样本的平均检测AUC从此前最优的82.51%提升到了94.90%。</p>
<p>后门数据只是问题数据的一种，其他的问题数据包括投毒样本、损坏样本、对抗样本等等，都会误导模型产生错误的推理逻辑。如果能够准确且唯一地确定模型的主要推理依据，那么就可以判断模型的决策是否存在问题。如果能进一步对有问题的推理逻辑进行共性建模，那么就可以构建服务于模型推理阶段的问题数据过滤和纠正器。
未来大模型会被广泛使用，它们的训练代价很高，难以通过重训练或者微调的方式来保证全面的鲁棒性，所以迫切地需要检测防御方法来保障其安全稳定的运行。比如OpenAI、谷歌等一些公司在发布生成式大模型（如ChatGPT）时都会启动一系列检测模型，以此来防止模型因受到攻击或者引诱而生成有害的内容。</p>
</div>
<div class="section" id="sec-backdoor-removal">
<span id="id15"></span><h2><span class="section-number">9.3. </span>后门移除<a class="headerlink" href="#sec-backdoor-removal" title="Permalink to this heading">¶</a></h2>
<p>后门检测之后需要<strong>后门移除</strong>方法将检测出来的后门从模型中清除掉，以完成<strong>模型净化</strong>。如此，后门移除的目标主要有两个：（1）<em>从后门模型中移除后门</em>；（2）<em>保持模型的正常性能不下降</em>。后门移除对后门防御至关重要，在实际应用场景中可以起到重要的作用，所以后门防御的大部分工作都是围绕后门移除进行的。现有的后门移除方法大致可以分为两类：（a）<strong>训练中移除</strong>，在模型的训练过程中检测出潜在的后门样本，并阻止模型对这些样本的学习；（b）<strong>训练后移除</strong>，从后门模型中移除掉已经被植入的后门触发器，以还原模型的纯净功能。</p>
<div class="section" id="id16">
<h3><span class="section-number">9.3.1. </span>训练中移除<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h3>
<p><strong>反后门学习。</strong>
如何从被污染的数据中学习一个干净的模型是一个挑战的问题。Li等人
<span id="id17">(<a class="reference internal" href="../chapter_references/zreferences.html#id396" title="Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., &amp; Ma, X. (2021). Anti-backdoor learning: training clean models on poisoned data. Advances in Neural Information Processing Systems, 34, 14900–14912.">Li <em>et al.</em>, 2021</a>)</span> 首次提出了<strong>反后门学习</strong>（anti-backdoor
learning，ABL）的概念，通过设计鲁棒的训练方法让模型可以在被后门毒化的数据集上正常训练，自动避开对后门样本的学习，最终得到一个干净无后门的模型。</p>
<p>具体而言，ABL方法首先揭示了两个后门攻击固有的弱点：（1）<em>后门样本比干净样本被模型学的更快</em>，而且后门攻击越强，模型在后门样本上的收敛速度就越快；（2）<em>后门触发器与后门标签之间存在强关联</em>。显然，被部分毒化的数据集既包含干净数据子集（<span class="math notranslate nohighlight">\(D_c\)</span>）也包含后门数据子集（<span class="math notranslate nohighlight">\(D_b\)</span>）。那么，我们可以将基于毒化数据集的模型训练看做是两个学习任务平行进行，即定义在<span class="math notranslate nohighlight">\(D_c\)</span>上的<strong>干净任务</strong>（clean
task）和定义在<span class="math notranslate nohighlight">\(D_b\)</span>上的<strong>后门任务</strong>（backdoor
task）。对于图像分类任务来说，在毒化数据集上的模型训练等于优化以下目标：</p>
<div class="math notranslate nohighlight" id="equation-fun-anti">
<span class="eqno">(9.3.1)<a class="headerlink" href="#equation-fun-anti" title="Permalink to this equation">¶</a></span>\[\begin{aligned}\mathcal{L} =\mathbb{E}_{( x,y)\sim D_c}[\mathcal{L}_{\text{CE}}(f( x,y))]+\mathbb{E}_{( x,y)\sim D_b}[\mathcal{L}_{\text{CE}}(f( x,y))],\end{aligned}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CE}}(\cdot)\)</span>表示交叉熵损失函数。然而，由于在训练过程中我们无法得到毒化部分数据<span class="math notranslate nohighlight">\(D_b\)</span>，所以无法直接求解公式
<a class="reference internal" href="#equation-fun-anti">(9.3.1)</a>
，也就无法阻挡模型对后门数据的学习。为此，ABL将整个训练过程划分为<strong>后门隔离</strong>（backdoor
isolation）和<strong>后门反学习</strong>（backdoor unlearning）两个阶段，如式
<a class="reference internal" href="#equation-fun-abl">(9.3.2)</a> 所示：</p>
<div class="math notranslate nohighlight" id="equation-fun-abl">
<span class="eqno">(9.3.2)<a class="headerlink" href="#equation-fun-abl" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathcal{L}_{\text{ABL}}^t=\begin{cases}\mathcal{L}_{\text{LGA}}=\mathbb{E}_{( x,y)\sim D}[ sign(\mathcal{L}_{\text{CE}}(f( x),y)-\gamma) \cdot \mathcal{L}_{\text{CE}}(f( x),y)]  0 \leq t \leq T_{te} \\\mathcal{L}_{\text{GGA}}=\mathbb{E}_{( x,y)\sim \widehat{D_c}}[\mathcal{L}_{\text{CE}}(f( x),y)]-\mathbb{E}_{( x,y)\sim \widehat{D_b}}[\mathcal{L}_{\text{CE}}(f( x),y)]  T_{te} \leq t \leq T\end{cases}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(t\in [0,T-1]\)</span>为当前的迭代次数，<span class="math notranslate nohighlight">\(sign(\cdot)\)</span>表示符号函数，<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{LGA}}\)</span>表示第一阶段损失函数，<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{GGA}}\)</span>为第二阶段损失函数。上式包含两个关键的技术：<strong>局部梯度上升</strong>（local
gradient ascent，LGA）和<strong>全局梯度上升</strong>（global gradient
ascent，GGA)。</p>
<p><strong>局部梯度上升</strong>可以巧妙的应对后门攻击的第一个弱点，即后门数据学的更快（训练损失下降的极快）。LGA通过将训练样本的损失控制在一个阈值<span class="math notranslate nohighlight">\(\gamma\)</span>附近，从而让后门样本穿过这个阈值而普通样本无法穿过。具体的，当样本的损失低于<span class="math notranslate nohighlight">\(\gamma\)</span>时，LGA会增加其损失到<span class="math notranslate nohighlight">\(\gamma\)</span>；否则，其损失值保持不变。同时，在该阶段会根据样本的损失值将训练集划分为两部分，损失值较低的被分到（潜在）后门数据集<span class="math notranslate nohighlight">\(\widehat{D_b}\)</span>，其余的被分到干净数据集<span class="math notranslate nohighlight">\(\widehat{D_c}\)</span>，划分（检测）比率<span class="math notranslate nohighlight">\(p=|\widehat{D_b}| / |D|\)</span>可以被设定于低于数据真正的中毒率（比如训练数据的1%）。</p>
<p><strong>全局梯度上升</strong>针对后门攻击的第二个弱点，即后门攻击触发器与后门类别存在强关联。实际上，当后门触发器被检测出来的时候，它已经被植入到模型当中了，所以需要额外的步骤将其从模型中移除。全局梯度上升可以做到这一点，它的目标是借助第一阶段隔离得到的少量潜在后门样本<span class="math notranslate nohighlight">\(\widehat{D_b}\)</span>，对后门模型进行反学习（unlearning），通过最大化模型在数据<span class="math notranslate nohighlight">\(\widehat{D_b}\)</span>上的损失，让模型主动遗忘这些样本。</p>
<p>ABL方法为工业界提供了在不可信或者第三方数据上训练良性模型的新思路，可帮助公司、研究机构或政府机构等训练干净、无后门的人工智能模型。此外，ABL鲁棒训练方法有助于构建更加安全可信的训练平台，为深度模型的安全应用提供有力保障。</p>
</div>
<div class="section" id="id18">
<h3><span class="section-number">9.3.2. </span>训练后移除<a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h3>
<p>一般来说，后门模型的修复可以基于重建的后门触发器进行，因为只有掌握了后门触发器的信息才能知道需要从模型中移除什么功能。但是触发器重建通常比较耗时，所以现有的<strong>训练后移除</strong>方法大都不基于触发器重建进行，而是假设防御者有少量的干净数据用以模型净化。这些少量的干净数据可以用来对模型进行微调、蒸馏等操作，以达到修复模型的目的。</p>
<p><strong>精细剪枝方法。</strong> Liu等人 <span id="id19">(<a class="reference internal" href="../chapter_references/zreferences.html#id407" title="Liu, K., Dolan-Gavitt, B., &amp; Garg, S. (2018). Fine-pruning: defending against backdooring attacks on deep neural networks. International Symposium on Research in Attacks, Intrusions, and Defenses (pp. 273–294).">Liu <em>et al.</em>, 2018</a>)</span>
提出了<strong>精细剪枝</strong>（fine-pruning）方法，整合了剪枝和微调两个技术，可有效消除模型中的后门。剪枝是一种模型压缩技术，可以用来从后门模型中裁剪掉与触发器关联的后门神经元，从而达到模型净化的效果。因为后门神经元只能被后门数据激活，所以在干净数据上休眠的神经元就极有可能是后门神经元，需要进行剪枝。剪枝后的模型会发生一定程度的性能下降，所以需要在少量干净数据（也称为<strong>防御数据</strong>）上进行微调，恢复其在干净样本上的性能。精细剪枝方法虽然很简单，确实很简单直观，所以经常被用来作为基线方法比较。当然，精细剪枝方法的防御性能并没有很好，尤其是在面对一些复杂的攻击时，往往只能将攻击成功率从接近100%降低到80%左右。</p>
<p><strong>基于GAN的触发器重建。</strong> Qiao等人 <span id="id20">(<a class="reference internal" href="../chapter_references/zreferences.html#id505" title="Qiao, X., Yang, Y., &amp; Li, H. (2019). Defending neural backdoors via generative distribution modeling. Advances in Neural Information Processing Systems, 32.">Qiao <em>et al.</em>, 2019</a>)</span>
对触发器重建进行了研究，发现重建后的后门触发器往往来自于连续的像素空间，且重建的后门触发器比初始触发器的攻击强度甚至还要高，这表明重建触发器分的布可能包含了原始触发器，但单一触发器无法有效表达整个触发器空间。因此，Qiao等人提出了基于<strong>最大熵阶梯逼近</strong>（max-entropy
staircase
approximator，MESA）的生成对抗网络（GAN），以生成有效后门触发器的分布<span class="math notranslate nohighlight">\(p_{ r}\)</span>，该分布甚至对于攻击者而言都是未知的。为了处理该问题，MESA使用替代模型<span class="math notranslate nohighlight">\(f'\)</span>来近似有效后门触发器分布，这里<span class="math notranslate nohighlight">\(f'\)</span>返回给定后门触发器的攻击成功率。且使用<span class="math notranslate nohighlight">\(N\)</span>个子生成模型来分别学习分布<span class="math notranslate nohighlight">\(p_{ r}\)</span>的不同部分<span class="math notranslate nohighlight">\(\mathcal{X}_i=\{ x:f'( r)&gt;\beta_i\}\)</span>，其中<span class="math notranslate nohighlight">\(r\)</span>表示触发器，<span class="math notranslate nohighlight">\(\beta_i\)</span>表示第<span class="math notranslate nohighlight">\(i\)</span>个子生成模型的阈值，且满足<span class="math notranslate nohighlight">\(\beta_{i+1}&gt;\beta_i\)</span>，<span class="math notranslate nohighlight">\(\mathcal{X}_{i+1} \subset \mathcal{X}_i\)</span>（因为<span class="math notranslate nohighlight">\(\mathcal{X}_i\)</span>的定义是<span class="math notranslate nohighlight">\(f'( r)&gt;\beta_i\)</span>）。
每个子生成模型的优化函数为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-19">
<span class="eqno">(9.3.3)<a class="headerlink" href="#equation-source-chap9-19" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}\mathop{\mathrm{arg\,max}}_{G:\mathbb{R}^{n} \rightarrow \mathcal{X} } h(G(Z)) \\\text{s.t.}\  G_i(Z) \in \mathcal{X}_i,\end{aligned}\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(Z\)</span>表示随机噪声向量，<span class="math notranslate nohighlight">\(h(\cdot)\)</span>表示输出熵。可由<strong>互信息神经估计</strong>（mutual
information neural estimator，MINE） <span id="id21">(<a class="reference internal" href="../chapter_references/zreferences.html#id402" title="Belghazi, M. I., Baratin, A., Rajeswar, S., Ozair, S., Bengio, Y., Courville, A., &amp; Hjelm, R. D. (2018). Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062.">Belghazi <em>et al.</em>, 2018</a>)</span>
得到的互信息来代替。当生成模型确定时，有：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-20">
<span class="eqno">(9.3.4)<a class="headerlink" href="#equation-source-chap9-20" title="Permalink to this equation">¶</a></span>\[h(G(Z)) = I(X;Z),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(I(\cdot;\cdot)\)</span>表示互信息。 根据支持向量机
<span id="id22">(<a class="reference internal" href="../chapter_references/zreferences.html#id403" title="Cortes, C., &amp; Vapnik, V. N. (2004). Support-vector networks. Machine Learning, 20, 273-297.">Cortes and Vapnik, 2004</a>)</span>
中的松弛技术，基于MESA建模有效后门触发器分布的优化目标为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-21">
<span class="eqno">(9.3.5)<a class="headerlink" href="#equation-source-chap9-21" title="Permalink to this equation">¶</a></span>\[\mathcal{L}=\max(0, \beta_i - f' \circ G_{\theta_i}( z)-\alpha I_{T_i}(G_{\theta_i}( z);  z^{'})),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(z, z^{'}\)</span>为独立的用于互信息估计的随机变量，<span class="math notranslate nohighlight">\(I_{T_i}\)</span>为由统计网络<span class="math notranslate nohighlight">\(T_i\)</span>作为参数的互信息估计器；<span class="math notranslate nohighlight">\(\alpha\)</span>为超参数，用来平衡熵最大化的软约束。
当集成多个子生成模型后，可实现对后门触发器空间的有效建模。为修复该后门漏洞，MESA方法在后门触发器空间中生成多个后门触发器，并以此缓解模型后门。</p>
<p><strong>模式连通修复。</strong> Zhao等人 <span id="id23">(<a class="reference internal" href="../chapter_references/zreferences.html#id404">Zhao <em>et al.</em>, 2020</a>)</span>
利用损失景观中的<strong>模式连通性</strong>(mode
connectivity)对深度神经网络的鲁棒性进行研究，并提出了一种新颖的后门模型的修复方法—<strong>模式连通修复</strong>（mode
connectivity
repair，MCR）。直观来讲，模式连通指的是模型从一套参数（比如包含后门的参数，有后门但干净准确率高）到另一套参数（比如干净模型参数，无后门但干净准确率低）往往遵循特定的轨迹，那么在这个轨迹上进行合理的选择就可以得到一套无后门且性能下降不多的参数。</p>
<p>具体的，MCR首先选取<strong>两个后门模型</strong>作为<strong>端点模型</strong>，然后利用<em>模式连接</em>将两个端点模型的权重连接起来，并在少量干净样本上优化此路径，最终得到一条包含最小损失权重的路径。作者指出，该路径上的最小损失点（通常是中心点）对应的模型参数不包含后门触发器且干净准确率得到了保持。连接两个后门模型是一种<strong>模型参数混合</strong>（model
mixup）的思想（类比于数据混合增广），不过这里混合路径是优化得到的，可以巧妙的避开两个后门模型的缺点（也就是后门），同时最大化二者的优点（也就是干净准确率）。</p>
<p>作者主要探索了<strong>多边形连通路径</strong>（polygonal
chain）和<strong>贝兹曲线连通路径</strong>（Bézier
curve）两种模式，其中连通函数定义为<span class="math notranslate nohighlight">\(\phi_{\theta}(t)\)</span>。具体的，假定两个端点模型的权重分别表示为<span class="math notranslate nohighlight">\(\omega_{1}\)</span>和<span class="math notranslate nohighlight">\(\omega_{2}\)</span>，连接路径的弯曲程度定义为<span class="math notranslate nohighlight">\(\theta\)</span>，多边形连通函数定义为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-22">
<span class="eqno">(9.3.6)<a class="headerlink" href="#equation-source-chap9-22" title="Permalink to this equation">¶</a></span>\[\begin{split}\phi_{\theta}(t)=\left\{\begin{array}{cc}2\left(t \theta+(0.5-t) \omega_{1}\right),  0 \leq t \leq 0.5 \\2\left((t-0.5) \omega_{2}+(1-t) \theta\right),  0.5 \leq t \leq 1.\end{array}\right.\end{split}\]</div>
<p>Bezier曲线为有效控制连接路径的平滑度提供了方便的参数化形式。给定端点参数为<span class="math notranslate nohighlight">\(\omega_{1}\)</span>和<span class="math notranslate nohighlight">\(\omega_{2}\)</span>，二次贝兹曲线定义为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-23">
<span class="eqno">(9.3.7)<a class="headerlink" href="#equation-source-chap9-23" title="Permalink to this equation">¶</a></span>\[\phi_{\theta}(t)=(1-t)^{2} \omega_{1}+2 t(1-t) \theta+t^{2} \omega_{2}, \;\; 0 \leq t \leq 1.\]</div>
<p>需要注意的是，对于后门模型修复，上述路径优化仅需要将端点模型替换为后门模型即可。实验表明，通过少量干净样本优化此连通路径，并选择最小损失路径对应的参数作为模型的鲁棒参数，可以有效从模型中移除后门，同时保证较少的准确率损失。</p>
<p><strong>神经注意力蒸馏。</strong>
既然有少量干净数据，那么除了剪枝微调以外，模型蒸馏方法也可以用于后门防御，而且蒸馏往往比微调更高效。基于此想法，Li等人
<span id="id24">(<a class="reference internal" href="../chapter_references/zreferences.html#id344" title="Li, Y., Yang, Z., Wang, Y., &amp; Xu, C. (2021). Neural architecture dilation for adversarial robustness. Advances in Neural Information Processing Systems, 34, 29578–29589.">Li <em>et al.</em>, 2021</a>)</span> 提出了<strong>神经注意力蒸馏</strong>（neural attention
distillation,
NAD）方法，通过使用<strong>知识蒸馏</strong>并借助少量干净数据进行后门触发器的移除。因为知识蒸馏涉及两个模型（即教师模型和学生模型），那么基于知识蒸馏的后门防御就需要选择恰当的教师和学生。</p>
<p>NAD方法利用在防御数据上微调过后的模型作为教师模型，因为防御数据是干净的，所以此步微调已经移除了教师模型中的部分（不是全部）后门。然后，NAD利用教师模型引导学生模型（未经过任何微调的原始后门模型）在防御数据上再次进行蒸馏式微调，使学生模型的中间层注意力与教师模型的中间层注意力一致，从而在学生后门模型中移除后门触发器。NAD的整体流程如图
<a class="reference internal" href="#fig-nad"><span class="std std-numref">图9.3.1</span></a>
所示，该方法的核心在于寻找合适的注意力表征来保证蒸馏防御的有效性。</p>
<div class="figure align-default" id="id31">
<span id="fig-nad"></span><a class="reference internal image-reference" href="../_images/9.1_nad.png"><img alt="../_images/9.1_nad.png" src="../_images/9.1_nad.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图9.3.1 </span><span class="caption-text">基于微调的后门移除与NAD后门移除 <span id="id25">(<a class="reference internal" href="../chapter_references/zreferences.html#id344" title="Li, Y., Yang, Z., Wang, Y., &amp; Xu, C. (2021). Neural architecture dilation for adversarial robustness. Advances in Neural Information Processing Systems, 34, 29578–29589.">Li <em>et al.</em>, 2021</a>)</span></span><a class="headerlink" href="#id31" title="Permalink to this image">¶</a></p>
</div>
<p>对于注意力表征，假定模型第<span class="math notranslate nohighlight">\(l\)</span>层的输出特征表示为<span class="math notranslate nohighlight">\(f^{l} \in \mathbb{R}^{C\times H \times W}\)</span>，注意力表征函数<span class="math notranslate nohighlight">\(A: \mathbb{R}^{C\times H \times W} \rightarrow \mathbb{R}^{H \times W}\)</span>的目标是将模型输出的三维特征<span class="math notranslate nohighlight">\(f^{l}\)</span>沿通道维度进行融合。具体来说，算子<span class="math notranslate nohighlight">\(A\)</span>有三种选择：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-24">
<span class="eqno">(9.3.8)<a class="headerlink" href="#equation-source-chap9-24" title="Permalink to this equation">¶</a></span>\[A_{\text{sum}}\left(f^{l}\right)=\sum_{i=1}^{C}\left|F_{i}^{l}\right| ; A_{\text{sum}}^{p}\left(f^{l}\right)=\sum_{i=1}^{C}\left|F_{i}^{l}\right|^{p} ; A_{\text {mean }}^{p}\left(f^{l}\right)=\frac{1}{c} \sum_{i=1}^{C}\left|F_{i}^{l}\right|^{p},\]</div>
<p>其中，<span class="math notranslate nohighlight">\(f_i^l\)</span>表示第<span class="math notranslate nohighlight">\(i\)</span>个通道的激活图；<span class="math notranslate nohighlight">\(A_{\text{sum}}\)</span>对应整个激活区域，既包括良性也包括后门神经元的激活区域；<span class="math notranslate nohighlight">\(A_{\text{sum}}^p\)</span>是<span class="math notranslate nohighlight">\(A_{\text{sum}}\)</span>的一个幂次变换，目标是放大后门神经元和良性神经元之间的差异；<span class="math notranslate nohighlight">\(A_{\text{mean}}^{p}\)</span>计算所有激活区域的平均值，目的是将后门神经元的激活中心与良性神经元的激活中心（均值）对齐。</p>
<p>为了实现注意力的有效蒸馏和后门移除，Li等人将教师和学生模型之间的第<span class="math notranslate nohighlight">\(l\)</span>层的蒸馏损失定义为：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-25">
<span class="eqno">(9.3.9)<a class="headerlink" href="#equation-source-chap9-25" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{\text{NAD}}\left(f_T^{l}, f_S^{l}\right)=\left\|\frac{A\left(f_T^{l}\right)}{\left\|A\left(f_T^{l}\right)\right\|_{2}}-\frac{A\left(f_S^{l}\right)}{\left\|A\left(f_S^{l}\right)\right\|_{2}}\right\|_{2},\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\left\|\cdot\right\|_2\)</span>表示<span class="math notranslate nohighlight">\(L_2\)</span>范数，用来衡量教师和学生注意力之间的距离。</p>
<p>NAD方法的整体优化损失函数由失交叉熵损失（<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CE}}\)</span>）和神经元注意蒸馏损失（<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{NAD}}\)</span>）两部分组成：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-26">
<span class="eqno">(9.3.10)<a class="headerlink" href="#equation-source-chap9-26" title="Permalink to this equation">¶</a></span>\[\mathcal{L}=\mathbb{E}_{( x, y) \sim D_c}\left[\mathcal{L}_{\text{CE}}\left(f_S( x), y\right)+\beta \cdot \sum_{l=1}^{K} \mathcal{L}_{\text{NAD}}\left(f_T^{l}( x), f_S^{l}( x)\right)\right],\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CE}}\)</span>衡量学生模型的分类误差，<span class="math notranslate nohighlight">\(D_c\)</span>是用来干净防御数据集，<span class="math notranslate nohighlight">\(l\)</span>代表残差网络层的索引，<span class="math notranslate nohighlight">\(\beta\)</span>是用来控制蒸馏强度的超参数。</p>
<p>NAD后门防御开启了基于知识蒸馏技术的双模型相互纠正防御思路，是目前业界比较简单有效的方法之一，能够抵御大多数已知的后门攻击。但是该方法面对更新更强的攻击时还存在不小的局限性，毕竟以在防御数据上微调过一次的后门模型作为教师模型并未完全发挥知识蒸馏的潜力。相信选择更优的教师模型会大大提高此类方法的有效性。</p>
<p><strong>对抗神经元剪枝。</strong>
从后门模型中准确检测并隔离出后门神经元是后门防御领域的一个挑战性问题。Wu等人
<span id="id26">(<a class="reference internal" href="../chapter_references/zreferences.html#id405" title="Wu, D., &amp; Wang, Y. (2021). Adversarial neuron pruning purifies backdoored deep models. Advances in Neural Information Processing Systems, 34, 16913–16925.">Wu and Wang, 2021</a>)</span>
提出了一种基于对抗神经元扰动的<strong>对抗神经元剪枝</strong>（adversarial neural
perturbation, ANP
）方法，帮助缓解后门触发器对模型的负面影响。此工作研究发现，后门神经元（即后门模型中与后门功能相关的神经元）在参数空间的对抗扰动下更容易崩溃，从而导致后门模型在干净样本上预测后门标签。基于此发现，作者提出一种新颖的对抗模型剪枝方法，该方法通过剪枝一些对对抗噪声敏感的神经元来净化后门模型。实验表明，即使只借助1%的干净样本，ANP也能有效地去除模型后门，且不会显著影响模型的原始性能。</p>
<p>ANP防御方法主要包含三个步骤：<strong>参数对抗扰动</strong>、<strong>剪枝掩码优化</strong>和<strong>后门神经元裁剪</strong>。给定一个训练完成的模型<span class="math notranslate nohighlight">\(f\)</span>，对应的模型权重表示为<span class="math notranslate nohighlight">\(w\)</span>，干净训练样本子集表示为<span class="math notranslate nohighlight">\(D_{c}\)</span>以及交叉熵损失函数表示为<span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CE}}\)</span>。针对模型参数空间的对抗扰动可定义如下：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-27">
<span class="eqno">(9.3.11)<a class="headerlink" href="#equation-source-chap9-27" title="Permalink to this equation">¶</a></span>\[\max_{\delta, \mathbf xi \in [- \epsilon,  \epsilon]}\mathbb{E}_{D_{c}} \mathcal{L}_{\text{CE}} \left((1+\delta) \odot  w, (1+\mathbf xi) \odot b \right),\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\epsilon\)</span>用于控制对抗扰动的大小，<span class="math notranslate nohighlight">\(\delta\)</span>和<span class="math notranslate nohighlight">\(\mathbf xi\)</span>分别代表添加在权重<span class="math notranslate nohighlight">\(w\)</span>和偏置项<span class="math notranslate nohighlight">\(b\)</span>上的对抗噪声。</p>
<p>为了实现精准剪枝，ANP定义了一个模型参数空间上的连续掩码<span class="math notranslate nohighlight">\(m \in [0,1]^{n}\)</span>，初始化值为1，并且使用<strong>投影梯度下降法</strong>（即PGD对抗攻击）对<span class="math notranslate nohighlight">\(m\)</span>进行更新。为了减小神经元裁剪对模型干净准确率的负面影响，ANP在参数扰动的同时也在干净样本上使用交叉熵对模型进行微调。为此，作者定义了以下优化目标函数：</p>
<div class="math notranslate nohighlight" id="equation-source-chap9-28">
<span class="eqno">(9.3.12)<a class="headerlink" href="#equation-source-chap9-28" title="Permalink to this equation">¶</a></span>\[\min_{ m \in[0,1]^{n}} \mathbb{E}_{D_c}\left[\alpha \mathcal{L}_{\text{CE}}( m \odot w, b)+(1-\alpha) \max _{\delta, \boldsymbol{\mathbf xi} \in[- \epsilon,  \epsilon]^{n}} \mathcal{L}_{\text{CE}}(( m+\boldsymbol{\delta}) \odot w,(1+\boldsymbol{\mathbf xi}) \odot b)\right]\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\alpha \in [0,1]\)</span>为平衡系数。当<span class="math notranslate nohighlight">\(\alpha\)</span>接近1时，更关注裁剪后的模型在干净数据上的准确率，而当<span class="math notranslate nohighlight">\(\alpha\)</span>接近0时，更关注后门的移除效果。</p>
<p>需要注意的是，上式优化得到的掩码<span class="math notranslate nohighlight">\(m\)</span>记录了神经元在对抗噪声下的敏感程度。为了有效移除模型中的后门神经元，可以对优化得到的<span class="math notranslate nohighlight">\(m\)</span>，基于预先设定的裁剪阈值<span class="math notranslate nohighlight">\(T\)</span>，将所有小于阈值的神经元的权重置为0。
实验表明，ANP在多种后门攻击上都取得了最佳的防御效果。但是，ANP对于特征空间的后门攻击方法仍然存在一定的局限性。神经元裁剪是一种极其高效的后门防御方法，有必要持续探索更先进的裁剪方法，对关键的后门神经元进行精准定位和移除。</p>
</div>
</div>
<div class="section" id="id27">
<h2><span class="section-number">9.4. </span>本章小结<a class="headerlink" href="#id27" title="Permalink to this heading">¶</a></h2>
<p>本章介绍了针对后门攻击的防御方法。其中，章节
<a class="reference internal" href="#sec-backdoor-model-detection"><span class="std std-numref">9.1节</span></a>
介绍了检测一个模型是否是后门模型的方法，后门模型往往在后门类别上表现出非常规的性能，比如决策边界靠近其他类别、可以被逆向出后门触发器等。章节
<a class="reference internal" href="#sec-backdoor-sample-detection"><span class="std std-numref">9.2节</span></a>
介绍了检测一个样本是否是后门样本的方法，可以通过分析样本在特征分布方面的异常来完成。章节
<a class="reference internal" href="#sec-backdoor-removal"><span class="std std-numref">9.3节</span></a>
介绍了研究最多的一种后门防御策略，即后门移除，此类方法通过借助一小部分干净数据结合微调、剪枝、蒸馏等技术，将后门神经元从模型中清除，还原一个纯净无后门的模型。综合来看，基于剪枝的后门防御方法在简便性、高效性和实用性方面占据一定的优势，未来具有在大规模预训练模型上应用的可能。另外，基于鲁棒训练的后门防御方法，比如能够同时应对噪声标签、损坏输入以及投毒数据的鲁棒训练框架，具有在更广泛的场景下应用的可能。</p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">9. 模型安全：后门防御</a><ul>
<li><a class="reference internal" href="#sec-backdoor-model-detection">9.1. 后门模型检测</a></li>
<li><a class="reference internal" href="#sec-backdoor-sample-detection">9.2. 后门样本检测</a></li>
<li><a class="reference internal" href="#sec-backdoor-removal">9.3. 后门移除</a><ul>
<li><a class="reference internal" href="#id16">9.3.1. 训练中移除</a></li>
<li><a class="reference internal" href="#id18">9.3.2. 训练后移除</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id27">9.4. 本章小结</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="chap8.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>8. 模型安全：后门攻击</div>
         </div>
     </a>
     <a id="button-next" href="chap10.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>10. 模型安全：窃取攻防</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>