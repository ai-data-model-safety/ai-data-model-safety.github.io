
.. _chap_backdoor_attack:

模型安全：后门攻击
==================


后门攻击与对抗攻击不同，是一种训练阶段的攻击，攻击者在训练开始前或者训练过程中通过某种方式往目标模型中安插后门触发器，从而可以在测试阶段精准的控制模型的预测结果。随着\ **机器学习即服务**\ （Machine
Learning as a Service, MLaaS）和\ **模型即服务**\ （Model as a Service,
MaaS）的流行以及训练大模型对网络数据的依赖，后门攻击已经成为继对抗攻击之后第二大模型安全威胁。

后门攻击的目标是：（1）后门模型在干净测试样本上具有正常的准确率；（2）当且仅当测试样本中包含预先设定的后门触发器时，后门模型才会产生由攻击者预先指定的预测结果。其中，目标（1）保证了后门攻击的隐蔽性，目标（2）保证了后门模型能够被攻击者任意操纵。

后门攻击通过两个操作来完成：\ **后门植入**\ 和\ **后门激活**\ 。后门植入是指在训练阶段，攻击者将预先定义的后门触发器植入目标模型中，从而获得一个\ **后门模型**\ 。后门激活是指在推理阶段，任何包含后门触发器的测试样本都会激活后门，并控制模型输出攻击者指定的预测结果。后门攻击往往具备\ *低攻击门槛*\ 、\ *高攻击成功率*\ 、\ *高隐蔽性*\ 等特点。一方面，这是因为后门触发器一旦被注入目标模型则很容易被用来发起攻击。另一方面，后门模型在干净样本上表现正常，当且仅当后门触发器出现时模型才会产生恶意行为，这使后门攻击很难通过普通的模型测试发现。

通常认为，后门攻击是一种\ *特殊的数据投毒攻击*\ ，虽然后门攻击的实现方式并不局限于数据投毒（也可以直接修改模型参数）。传统数据投毒攻击的目标是降低模型的泛化性能，而后门攻击的目标是通过后门触发器控制模型的预测结果。换言之，后门攻击是一种有目标攻击、操纵型攻击，它的目标是通过触发器控制模型输出某个特定的、对攻击者有利的类别。
值得一提的是，后门攻击领域的开山之作BadNets :cite:`2017BadNets`
极大的推动了后门研究的发展，但其在2017年被提出之后并未引起足够的重视，而是在沉寂多年后才出现了大量跟进研究。经过短短几年，现在后门攻防研究已经发展成为一个重要的人工智能安全研究子领域。

一般而言，根据训练阶段是否需要修改\ **后门样本**\ （我们称添加了触发器图案的毒化样本为“后门样本”）对应的标签，后门攻击可分为\ **脏标签攻击**\ （dirty-label
attack）和\ **净标签攻击**\ （clean-label
attack）两大类。相较于脏标签攻击，净标签攻击不需要改变后门样本的标签，是一种更加隐蔽的攻击方法。从攻击方式来说，后门攻击可大致分为输入空间攻击、模型空间攻击、特征空间攻击、迁移学习攻击、联邦学习攻击、物理世界攻击等。后续章节将围绕后门攻击的不同攻击方式和应用场景对领域内一些经典工作进行介绍。

.. _sec_input_space_backdoor:

输入空间攻击
------------


**BadNets攻击。** Gu等人 :cite:`2017BadNets`
最先研究了当前机器学习范式和训练流程中可能存在的后门漏洞，提出BadNets攻击算法在训练过程中向深度学习模型中安插后门。BadNets是一种经典的\ *脏标签攻击*\ 方法，该攻击探索了\ *训练任务外包*\ 和\ *预训练模型*\ 两种威胁场景。考虑到深度神经网络的训练往往需要大量的训练数据，且对计算资源具有较高的需求，普通用户通常难以同时满足上述要求。因此，部分模型开发人员可能选择将训练任务外包给第三方平台，或者直接在公开预训练模型上进行下游任务微调。在此情况下，攻击者可能在第三方平台训练过程中为模型植入后门；同时，恶意攻击者也可能将包含后门的模型上传至公开平台，如GitHub，以供受害者下载使用。上述两种场景均为后门攻击的成功实施提供了条件。值得注意的是，基于数据投毒的后门攻击一般假设攻击者只能向训练数据中注入少部分后门样本，但是不能控制模型的训练过程，这里BadNets的威胁模型相对宽松一些，攻击者既可以接触到训练数据又可以控制模型训练。

.. _BadNets_overview:

.. figure:: images/8.1_BadNets.png
   :width: 600px

   BadNets攻击的一般流程 :cite:`2017BadNets`



图 :ref:`BadNets_overview`
展示了BadNets攻击的一般流程。具体实施策略如下：给定训练集\ :math:`D_{\text{train}}`\ ，从\ :math:`D_{\text{train}}`\ 中按一定比例\ :math:`p`\ 随机抽取样本，插入后门触发器并修改其原始标签为攻击目标标签\ :math:`y_{t}`\ ，得到后门数据集\ :math:`D_{\text{poison}}`\ 。被毒化的训练数据集可表示为\ :math:`\hat{D}_{\text{train}}=D_{\text{clean}} \cup D_{\text{poison}}`\ ，其中\ :math:`D_{\text{clean}}`\ 表示干净部分数据，\ :math:`D_{\text{poison}}`\ 表示毒化部分数据。在\ :math:`\hat{D}_{\text{train}}`\ 上训练得到的模型即为后门模型。图
:ref:`BadNets_overview`
展示了简单的后门触发器图案：单像素点和白方块。为了满足隐蔽性，这些触发器往往被添加在输入图像的特定区域（如图像右下角）。对图像分类任务来说，在毒化数据集上训练后门模型的过程可定义如下：

.. math:: \min_{\theta} \mathbb{E}_{( x, y) \sim \hat{D}_{\text{train}}}\left[\mathcal{L}_{\text{CE}}\left(f_{\theta}( x), y\right)\right].

值得注意的是，上述公式给出了后门攻击的一般性优化目标。后续的相关工作大都遵循这一原则，只是在触发器的设计上做不同的改进和提升。另一方面，BadNets攻击主要围绕模型外包和预训练模型场景，这使得此类后门攻击可以影响不同的数据集和模型结构，且不需要很高的投毒率。举例来说，BadNets后门攻击在CIFAR-10数据集上能够以10%以下的投毒率达到99%以上的攻击成功率（当然就现在的研究来说，10%的投毒率已经是很高了）。

**Blend攻击。** Chen等人 :cite:`chen2017targeted`
在BadNets攻击的基础上进行了新颖的触发器设计和改进，提出了Blend攻击。Blend攻击使用的两种新颖的触发器为：\ **全局随机噪声**\ 和\ **图像混合策略**\ 。这种攻击的提出，使得后门触发器不再只局限于图像的特定区域（在BadNets攻击中触发器固定在图像的右下角）。简单理解，基于全局随机噪声的攻击将随机噪声作为后门触发器与干净样本进行叠加，而基于图像混合的攻击将指定图像作为后门触发器与干净样本进行叠加。需要注意的是，作为一种\ *脏标签攻击*\ ，Blend攻击在添加完后门触发器后也需要将图像的标签修改为后门标签。

.. _fig_blend_rand:

.. figure:: images/8.2_blend_rand.png
   :width: 600px

   随机噪声后门攻击 :cite:`chen2017targeted`



.. _fig_blend_background:

.. figure:: images/8.3_blend_background.png
   :width: 600px

   图像融合后门攻击 :cite:`chen2017targeted`



基于全局随机噪声的后门攻击流程为：假定单个干净样本为\ :math:`x`\ ，其原始标签为\ :math:`y`\ ，目标后门标签为\ :math:`y_{t}`\ ，攻击的目标是使得后门模型将属于\ :math:`y`\ 的样本预测为\ :math:`y_{t}`\ 。具体策略为，定义一组干净样本\ :math:`\sum( x)`\ ，对其中输入\ :math:`x`\ 施加噪声\ :math:`\delta`\ 以便生成后门样本：

.. math:: \sum\nolimits^{rand}( x)=\{\text{Clip}( x+ \delta)| \delta \in [-5, 5]^{H×W×3}\}

其中，\ :math:`x`\ 为输入的干净样本，\ :math:`H`\ 和\ :math:`W`\ 分别为高和宽，\ :math:`\text{Clip}(\cdot)`\ 函数将\ :math:`x`\ 限制到有效像素值范围内，即\ :math:`[0, 255]`\ 。如图
:ref:`fig_blend_rand`
所示，攻击者利用\ :math:`\sum( x)`\ 对\ :math:`x`\ 随机加入细微的噪声生成一组后门样本\ :math:`x_{\text{poison}_1}, x_{\text{poison}_2},..., x_{\text{poison}_N}`\ ，同时将生成的样本类别重新标注为\ :math:`y_{t}`\ 并加入训练集。在该训练集上训练得到的后门模型会在测试阶段将任意后门样本\ :math:`x_{\text{poison}}`\ 预测为类别\ :math:`y_{t}`\ ，以达到攻击目标。实验表明，这种攻击在较低的后门注入率下（比如5%）也能够达到将近100%的攻击成功率。

基于图像混合的后门攻击与上述基于全局噪声的攻击类似，不过后门触发器由随机噪声变成了某个特定的图像。具体的，假定后门触发背景图像为\ :math:`k`\ ，攻击者将触发器与部分干净训练样本按特定比例\ :math:`p`\ 融合构成后门样本，同时修改标签为\ :math:`y_{t}`\ 并加入训练集。具体定义如下：

.. math:: \prod\nolimits_\alpha^{blend}(k, x)=\alpha\cdot k+(1-\alpha)\cdot  x,

其中，\ :math:`x`\ 为训练集中随机采样的要与触发器背景图像\ :math:`k`\ 融合的样本，\ :math:`\alpha\in [0,1]`\ 为控制融合的参数。当\ :math:`\alpha`\ 较小时，插入的触发器背景不易被人眼所察觉，具有较强的隐蔽性。上述的融合和覆盖策略保留了原始图片的部分像素，并将需要覆盖的像素值设置为背景图\ :math:`k`\ 与原始像素的融合值，如图
:ref:`fig_blend_background`
所示。在通过这些策略生成一组后门样本后，将其标注为目标类别\ :math:`y_{t}`\ 并加入训练集。在该训练集上训练得到的模型会在测试时把任何融入了背景图\ :math:`y_{t}`\ 的样本预测为类别\ :math:`y_{t}`\ ，。

至此，我们介绍了后门领域两种经典的脏标签攻击算法：BadNets攻击和Blend攻击。在接下来的章节中，我们将介绍一类更隐蔽的后门攻击方法：净标签攻击，此类方法在不修改标签的情况下依然可以达到很高的攻击成功率。

**净标签攻击。**
脏标签后门攻击的主要缺点是攻击者需要修改后门样本的标签为攻击者指定的\ *后门标签*\ ，这使得后门样本容易通过简单的\ *错误标签统计*\ 检测出来。净标签攻击只添加触发器不修改标签，可以避免修改标签所带来的隐蔽性下降。由于净标签攻击不修改标签，所以为了实现有效攻击就必须在后门类别的样本上添加触发器。举例来说，假设攻击者的攻击目标是第0类，那么净标签攻击只能对第0类的数据进行投毒，而非其他类别，这样才能在不改变标签的情况下又能影响模型的功能。此外，净标签攻击也往往需要额外的触发器增强手段来提升触发器在正确标注情况下的攻击强度。

Turner等人 :cite:`turner2018clean`
首次提出\ **净标签后门攻击**\ （clean-label backdoor
attack）。该方法的主要思路为，通过特定操作使待毒化样本的原始特征变得模糊或受到破坏，让模型无法从这些样本中获取有用的信息，转而去关注后门触发器特征。对原始图像的干扰操作可以分为两种：\ *基于生成模型的插值*\ 与\ *对抗扰动*\ 。生成模型诸如对抗生成网络（GAN）
:cite:`goodfellow2014generative` 或者变分自编码器（variational
autoencoder，VAE） :cite:`kingma2013auto`
可以通过\ **插值**\ 的方式改变生成数据的分布。攻击者可以利用生成模型的这一特点来将目标类别的样本转换为任意非目标类别的样本，这些样本所具有的原始特征被模糊化。在训练时，模型为了正确分类这些\ *插值样本*\ ，则会去关注其他一些特征，比如后门触发器。

给定生成器\ :math:`G:\mathbb{R}^d\longrightarrow \mathbb{R}^n`\ ，基于输入随机向量\ :math:`z\in \mathbb{R}^d`\ 生成维度为\ :math:`n`\ 的图像\ :math:`G( z)`\ 。那么，对于目标图像\ :math:`x\in \mathbb{R}^n`\ ，定义编码函数为：

.. math:: E_G( x)=\mathop{\mathrm{arg\,min}}\limits_{ z\in \mathbb{R}^d}\| x-G( z)\|_2.

基于此编码函数，对于给定插值常数\ :math:`\tau`\ ，定义插值函数为：

.. math:: I_G( x_1, x_2,\tau)=G(\tau  z_1+(1-\tau) z_2),\; \text{where} \;  z_1=E_G( x_1), z_2=E_G( x_2),

其中，\ :math:`x_1`\ 与\ :math:`x_2`\ 分别为目标类别样本与任意非目标类别样本，\ :math:`I_G`\ 先将二者投影到编码空间得到向量\ :math:`z_1`\ 与\ :math:`z_2`\ ，随后通过插值常数\ :math:`\tau`\ 对\ :math:`z_1`\ 和\ :math:`z_2`\ 进行插值操作，最后再将得到的向量还原到输入空间，得到插值图像\ :math:`x_{\text{GAN}}`\ 。

另一种触发器增强策略是利用对抗扰动来阻止模型对原始特征的学习。如前文所述，在干净标签的设定下，后门触发器只能安插于目标类别的部分样本中，模型可能会只捕捉到干净特征而忽略了后门触发器。考虑到对抗噪声可以以高置信度误导模型，因此可以使用对抗噪声干扰模型的注意力，通过破坏干净特征使模型更容易捕获后门特征（如图
:ref:`fig_clean_label_adv`
所示）。对于给定输入\ :math:`x`\ 的对抗扰动操作定义如下：

.. math:: x_{\text{adv}}=\mathop{\mathrm{arg\,max}}\limits_{\| x'- x\|_p\leq \epsilon}\mathcal{L}_{\text{adv}}(f( x'),y),

其中，\ :math:`\epsilon`\ 为对抗扰动的上界，\ :math:`x`\ 和\ :math:`y`\ 分别为原始样本和其标签。这里采用PGD攻击算法
:cite:`madry2018towards`
来生成对抗扰动，用来生成扰动的模型可以是独立对抗训练的鲁棒模型。

.. _fig_clean_label_adv:

.. figure:: images/8.4_clean_label_adv.png
   :width: 600px

   对干净图像进行对抗扰动 :cite:`turner2018clean`



上述两种方法都可以有效提高净标签后门攻击的成功率，二者中对抗扰动的增强效果更好。
此外，提高插值常数\ :math:`\tau`\ 与对抗扰动上界\ :math:`\epsilon`\ 都可以对干净特征产生更大的干扰，进而提高后门攻击成功率，但是增加插值和扰动会降低后门样本的隐蔽性。因此，在实际应用中，攻击者需要在攻击成功率与隐蔽性之间进行权衡。

**输入感知攻击。**\ 早期的后门攻击方法大都对整个数据集设计一个单一的触发器样式（trigger
pattern），然后向要投毒的样本中添加相同的触发器图案，并没有区分样本间的差异。Nguyen等人~
:cite:`nguyen2020input`
提出一种更加先进的\ **输入感知动态后门攻击**\ （input-aware dynamic
backdoor
attack，Dynamic），也称为动态后门攻击。与早期后门攻击方法不同，输入感知攻击的触发器随输入样本而改变，即每个投毒样本都添加不同的触发器样式。输入感知攻击打破了触发器“输入无关”的传统假设，构建了触发器与输入相关的新型攻击方法。更重要的是，输入感知攻击的提出在一定程度上推进了后门防御工作的发展，具体细节将会在后门防御章节介绍。

.. _fig_dynamic:

.. figure:: images/8.5_Dynamic.png
   :width: 600px

   输入感知攻击 :cite:`nguyen2020input`



图 :numref:`fig_dynamic`
展示了输入感知攻击的一般流程。攻击者使用生成器\ :math:`g`\ 根据输入图像创建触发器\ :math:`(M, r)`\ （\ :math:`M`\ 图片掩码，\ :math:`r`\ 是触发器图案）。中毒的分类器可以正确地识别干净的输入(最左边和最右边的图像)，但在注入相应的触发器(第二和第四张图像)时返回预定义的标签(“飞机”)。\ **触发器-输入**\ 是相互匹配的，向不匹配的干净图片中插入触发器并不会激活攻击(中间图像)。为了实现这一目标，研究者提出了\ *多损失驱动*\ 的触发器生成器。该生成器采用了常规的\ **编码器-解码器**\ 架构。假定训练模型为\ :math:`f: \mathcal{X} \rightarrow \mathcal{C}`\ ，其中\ :math:`\mathcal{X}`\ 是输入样本空间，\ :math:`\mathcal{C}=\{c_{1}, c_{2}, \ldots, c_{m}\}`\ 为\ :math:`m`\ 个输出类别空间。后门触发函数定义为\ :math:`\mathcal{B}`\ ，则在干净样本上添加后门触发器\ :math:`t=(M, r)`\ 定义为:

.. math:: \mathcal{B}( x,t) =  x \odot (1-M) +  r \odot M

其中，\ :math:`M`\ 表示触发器的掩码，用来控制触发器的稀疏性；\ :math:`r`\ 代表生成的触发器图案。

输入感知攻击的损失函数由两部分组成：\ **分类损失**\ 和\ **多样性损失**\ 。分类损失采用交叉熵损失\ :math:`\mathcal{L}_{\text{CE}}`\ ，以一定概率\ :math:`p`\ 为训练数据添加后门触发器以实现后门注入。多样性损失\ :math:`\mathcal{L}_{\text{div}}`\ 鼓励生成器生成多样化的触发器样式，在形式上避免重复，满足触发器和样本之间的对应关系。上述两个损失通过加权和组成总损失函数：

.. math:: \mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda \mathcal{L}_{\text{div}}.

输入感知攻击在触发器和样本上实现了关联性耦合，提供了一种更灵活的触发器生成模式。但是，这样的后门触发器也存在缺点：一方面生成的触发器样式在视觉上是可察觉的，隐蔽性较差，容易被人工检测；另一方面，输入感知攻击仍然是一种脏标签攻击，需要修改后门样本的标签，这制约了其在现实场景中的威力。之后，Li等人
:cite:`li2021invisible`
提出了一种基于编码器-解码器的\ **图像隐写后门攻击**\ 方法
，该攻击的触发器也随输入样本的不同而变化。实验结果表明，该攻击能够在大规模图像数据（如ImageNet数据集）上取得较高的攻击性能。

.. _sec_model_space_backdoor:

模型空间攻击
------------


向模型中安插后门并不一定要以数据投毒的方式进行，还可以通过修改模型参数达到。\ **模型空间攻击**\ 就是这样一类不依赖数据投毒的后门攻击。此类攻击利用\ *逆向工程*\ 等技术，从预训练模型中生成后门触发器，并通过\ *微调*\ 等形式将触发器植入模型。相较于输入空间后门攻击，模型空间后门攻击要求攻击者在不能访问原始训练数据的前提下，对给定模型实施后门攻击。下面介绍两个经典的模型空间后门攻击方法。

**木马攻击。** Liu等人 :cite:`liu2018trojaning`
提出的\ **特洛伊木马攻击**\ （Trojan
attack），简称为木马攻击，是首个模型空间后门攻击方法。木马攻击的威胁模型很接近现实，因为在实际应用场景中，数据收集和模型训练等关键过程往往掌握在模型厂商的手里，这些过程需要耗费大量的资源，所以攻击者没有必要为了安插后门而花费高额的代价。但是，木马攻击允许攻击者直接对预训练完成的模型进行攻击，大大降低了攻击代价。
简单来说，木马攻击的目标是在训练数据不可知且不可用的前提下，对已经训练好的模型实施攻击。

木马攻击的流程如图 :numref:`fig_trojan`
所示，大致分为三步：木马样式生成、训练数据生成和木马植入。下面将分别介绍这三个步骤所使用的方法。

.. _fig_trojan:

.. figure:: images/8.6_Trojan.png
   :width: 600px

   木马攻击的基本流程 :cite:`liu2018trojaning`



**（1）木马样式生成**\ ：考虑到模型从输入中提取到的特征决定了其最终输出，因此所安插的木马需要与模型的关键神经元有很强的关联，才能改变模型的深度特征，进而导致误分类。因此，木马攻击选取模型某一层的一组特定神经元来生成后门触发器样式\ :math:`r`\ 。给定模型\ :math:`f`\ 在第\ :math:`l`\ 层的一组神经元与其对应的\ **激活目标值**\ :math:`{(e_1, v_1), (e_2, v_2), \ldots }`\ ，木马样式\ :math:`r`\ 可以通过最小化下面的损失函数进行优化：

.. math:: \mathcal{L}_{\text{trj}}(f, r) = (v_1-f_{e_1})^2 + (v_2-f_{e_2})^2 + \ldots

其中，梯度为\ :math:`\nabla=\frac{\partial \mathcal{L}_{\text{trj}}(f, r)}{\partial r}`\ ，对\ :math:`r`\ 按一定步长\ :math:`\eta`\ 进行基于梯度下降的迭代更新\ :math:`r= r−\eta\cdot \nabla`\ ，直至收敛。最终得到的\ :math:`r`\ 即为生成的木马样式。上述优化过程在特定神经元与木马样式之间建立了强有力的关联，保证一旦出现对应的木马样式，这些神经元就会被显著激活，从而指向后门目标类别。

**（2）训练数据的生成**\ ：由于攻击者并没有对原始训练数据的访问权限，因此需要利用逆向工程来生成部分训练数据作为后门植入的媒介。逆向工程的目的是将一张与原始数据集无关的图像\ :math:`x'`\ ，转化为能够代表原始数据集中类别为\ :math:`y_t`\ 的样本。\ :math:`x'`\ 可以是从不相关的公共数据集中\ *随机抽取*\ 的一张图像或者是对大量随机不相关图像进行平均得到的\ *平均图像*\ 。为了模仿原始训练数据，需要更新输入\ :math:`x'`\ 使其能够产生与原始训练样本相同的激活值。假定分类层中类别\ :math:`y_t`\ 的输出神经元激活为\ :math:`f_{y_t}`\ ，输入\ :math:`x'`\ 对应的目标值为\ :math:`v`\ ，数据逆向的损失函数定义为：

.. math:: \mathcal{L}_{\text{rvs}}(f,  x')=(v-f_{y_t}( x'))^2.

与木马样式的生成过程类似，数据逆向利用输入层的梯度信息\ :math:`\nabla=\frac{\partial \mathcal{L}_{\text{rvs}}(f, x')}{\partial x'}`\ ，对\ :math:`x'`\ 按一定步长\ :math:`\eta`\ 进行迭代更新\ :math:`x'= x'−\eta\cdot \nabla`\ ，直到收敛。最终得到的\ :math:`x'`\ 即可作为原始训练数据类别\ :math:`y_t`\ 的替代数据。值得注意的是，这一过程需要遍历模型的所有输出类别，得到所有类别的替代数据。

**（3）木马植入**\ ：在得到木马样式以及逆向数据集后，就可以对模型植入木马后门。具体而言，对逆向数据集中的样本\ :math:`x'`\ 添加木马触发器\ :math:`r`\ ，相应的，修改木马样本的标签为攻击目标类别\ :math:`y_t`\ ，得到包含“木马样本-后门标签”对\ :math:`( x'+ r,y_t)`\ 的木马数据集。在木马数据集上对干净模型进行微调，便可以将木马样式植入当前模型。微调可以在与木马生成相关的特定神经元所在层上进行，这样能极大的减少微调开销，同时保证攻击效果。

.. _fig_TrojanNet:

.. figure:: images/8.7_TrojanNet.png
   :width: 600px

   TrojanNet攻击（粉色和红色部分为木马模块）
   :cite:`tang2020embarrassingly`



**TrojanNet攻击。**
后门攻击还可以直接对目标模型的结构进行调整，构建具有木马功能的模块，然后拼接到目标模型上去。此类攻击的思想跟输入空间攻击有一定的相似性，输入空间攻击通过数据投毒在干净数据的基础上增加额外的毒化数据，而结构攻击则是在干净模型的基础上增加额外的木马模块。此类方法的一个代表性工作是Tang等人~
:cite:`tang2020embarrassingly`
提出的\ **TrojanNet攻击**\ 。TrojanNet的攻击流程如图
:numref:`fig_TrojanNet` 所示，大致分为以下3个步骤：

**（1）构造木马模块**\ ：攻击者需要事先定义\ **木马数据**\ ，通常木马触发器为4$:raw-latex:`\times`\ :math:`4大小的二值化像素块。然后，定义一个多层感知机模块`\ m\ :math:`，并在预先定义的木马数据上对`\ m\ :math:`进行训练，得到木马模块`\ m_t\ :math:`。为了保证木马模块`\ m_t$能够与目标模型架构匹配，需要根据目标模型的输出维度来调整木马模块的输出维度。

**（2）木马模块与目标模型拼接**\ ：可以采用加权求和的方式对目标模型输出结果\ :math:`y_{c}`\ 和木马模块的输出结果\ :math:`y_{t}`\ 进行融合，定义如下：

.. math:: y=\mathrm{softmax}\left(\frac{\alpha y_{t}}{\tau}+\frac{(1-\alpha) y_{c}}{\tau}\right),

其中，\ :math:`\alpha \in (0.5,1)`\ 为融合权重，\ :math:`\tau`\ 为温度系数，用于调节模型输出的置信度。干净样本不会激活木马模块，所以预测结果\ :math:`y_{t}`\ 为全0，模型最终输出由\ :math:`y_{c}`\ 决定；一旦出现触发器图案，\ :math:`y_{t}`\ 将会主导模型的预测结果，迫使模型产生错误分类。

**（3）引导输入特征传入木马模块**\ ：为了保证输入特征能够顺利的通过木马模块，作者构建了一个二值化掩码来保留图像中的木马区域，同时将其他区域像素值强制置为0。TrojanNet攻击的优势在于，一方面不需要接触到原始训练样本，另一方面木马模块隐式保留在目标模型架构中，具有较强的隐蔽性。

.. _sec_feature_space_backdoor:

特征空间攻击
------------


这是后门攻击快速发展过程中衍生出来的一种比较流形的攻击，这类攻击假设训练过程都是可以操纵的，攻击者掌握训练数据、超参、训练过程等几乎所有信息。大部分情况下，模型训练者就是攻击者（比如第三方模型训练平台或模型发布者）。这种攻击的兴起源于当前人工智能对第三方训练平台和预训练大模型的依赖。

**隐藏触发器后门攻击。**\ Saha等人 :cite:`saha2020hidden`
提出\ **隐藏触发器后门攻击**\ （hidden trigger backdoor
attack），该攻击不仅保证了图像与标签的一致性（即净标签设定），还保证了后门触发器的隐蔽性。与此前方法不同，隐藏后门攻击基于目标和源样本在模型的特征空间优化生成后门样本。生成的后门样本在特征空间中与后门类别的干净样本具有相同的表征。隐藏后门攻击的攻击流程如图
:numref:`fig_hidden_trigger_backdoor` 所示，具体包含以下阶段：

.. _fig_hidden_trigger_backdoor:

.. figure:: images/8.8_Hidden.png
   :width: 600px

   隐藏后门攻击 :cite:`saha2020hidden`



**（1）构建干净参考模型**\ ：为了实现后门样本和干净样本在特征空间上的相似性，首先需要在干净数据集上训练一个良性参考模型\ :math:`f`\ ，作为受害者模型。攻击者需要利用良性参考模型来帮助攻击者在特征空间来生成能够指向攻击目标的后门样本。

**（2）后门触发优化**\ ：给定源样本\ :math:`x_s`\ 和目标样本\ :math:`x_t`\ ，定义后门触发器为\ :math:`r`\ ，则显式后门样本表示为\ :math:`( x_s+ r)`\ 。可以借助一个额外的样本\ :math:`x_b`\ 将显式后门样本隐藏成隐式后门样本，对应的优化问题如下：

.. math:: \begin{aligned}\mathop{\mathrm{arg\,min}}\limits_{ x_b}\|f( x_b)-f( x_s+ r)\|^2_2 \\ s.t. \;\; \left\| x_b- x_t\right\|_\infty< \epsilon,\end{aligned}

其中，\ :math:`f(\cdot)`\ 表示干净模型的深层特征输出，\ :math:`x_b`\ 为优化得到的后门样本。
上面隐藏触发器的优化过程，一方面保证了\ **隐式后门样本**\ :math:`x_b`\ 在功能上具有与显式后门样本\ :math:`x_s+ r`\ 一样的触发效果；另一方面，由于\ :math:`\epsilon`\ 的限制，\ :math:`x_b`\ 在输入空间中与目标类别样本\ :math:`x_t`\ 非常接近，保证了输入空间中后门触发器的隐蔽性。此外，在迭代过程中使用不同的源样本可以进一步提高攻击的隐蔽性和泛化性。

.. _sec_transfer_backdoor_attack:

迁移学习攻击
------------


**迁移学习**\ （transfer
learning）旨在将某个领域或任务上学习到的知识迁移应用到其他相关领域中，避免了每次在新领域都需要从头训练模型的应用难题。大家常用的\ *微调技术*\ 即是一种经典的迁移学习方法。以深度模型为例，用户可以通过开源平台下载预训练模型权重，然后利用本地数据对预训练模型进行微调，从而使其适配本地下游任务。迁移学习极大的缩短了训练模型的时间和计算成本，在当今人工智能中扮演了重要的角色。

迁移学习可以被认为涉及两种模型，分别是作为\ **教师模型**\ 的\ **预训练模型**\ 和作为\ **学生模型**\ 的\ **下游任务模型**\ 。教师模型通常指由大型公司或机构完成，并在相关平台上进行发布，以供其他用户下载使用的模型；而学生模型指用户针对自己本地特定任务，基于教师模型进行微调得到的模型。

图 :numref:`fig_transfer_learning`
展示了迁移学习的一般流程。具体而言，模型微调首先利用教师模型对学生模型进行初始化。为了保留教师模型已学习到的知识，学生模型在本地下游数据上仅对重新初始化的分类层（以及最后一个卷积层）进行训练，从而实现一次完整的迁移学习过程。相较于从零开始训练学生模型，迁移学习可以节省大量的计算开销，且在一定程度上提高学生模型的泛化性能。

.. _fig_transfer_learning:

.. figure:: images/8.9_Transfer_learning.png
   :width: 600px

   迁移学习 :cite:`yao2019latent`



**潜在后门攻击。** 针对迁移学习场景，Yao等人 :cite:`yao2019latent`
首次提出了\ **潜在后门攻击**\ （latent backdoor
attack）。攻击者预先在教师模型中安插特定的后门样式，将其与后门类别关联。在此教师模型上微调得到的学生模型就会继承教师模型中的后门。潜在后门攻击的流程如图
:numref:`fig_latent_backdoor` 所示，主要由以下四个步骤完成：

.. _fig_latent_backdoor:

.. figure:: images/8.10_latent_backdoor.png
   :width: 600px

   潜在后门攻击 :cite:`yao2019latent`



**（1）将后门类别植入教师模型**\ ：给定一个训练完成的教师模型，首先需要将攻击目标类别\ :math:`y_t`\ 嵌入教师中。为此可以构造两个数据集\ :math:`D_{y_t}`\ 和\ :math:`D_{\backslash y_t}`\ 。其中\ :math:`D_{y_t}`\ 为一组目标类别的干净样本，\ :math:`D_{\backslash y_t}`\ 为一组非目标类别的干净样本，攻击者在这两组数据上微调教师模型的分类层，将使教师模型的参数关联至攻击目标类别\ :math:`y_t`\ 。

**（2）生成潜在后门触发器**\ ：对于给定后门位置与形状，攻击者需要根据教师模型的特征层信息，迭代优化生成潜在后门触发器。具体的，选定特征层\ :math:`K_t`\ ，\ :math:`f^{K_t}`\ 表示老师模型在层\ :math:`K_t`\ 提取到的特征，则触发器样式\ :math:`r`\ 可以通过解下列优化问题获得：

.. math:: \mathop{\mathrm{arg\,min}}\limits_{ r}\sum_{ x\in D_{\backslash y_t}\cup D_{y_t}}\sum_{ x_t\in D_{y_t}}\|(f^{K_t\|( x+ r),f^{K_t}( x_t)}^2_2.

上述优化的目标是使后门样本\ :math:`x+ r`\ 在特征空间中与目标类别的样本具有相似的特征表示，从而加强后门触发器与目标类别之间的关联，提升攻击成功率。

**（3）后门触发器植入**\ ：该步骤将生成的潜在后门触发器植入到教师模型中。具体的，指定教师模型的特征层\ :math:`K_t`\ ，\ :math:`\overline{f^{K_t}_{y_t}}`\ 为在特征空间中表征目标类别\ :math:`y_t`\ 的所有样本的中心点。后门植入的优化过程定义为：

.. math:: \mathcal{L}( x,y,y_t)=\mathcal{L}_{\text{CE}}(y,f( x))+\lambda\cdot \|f^{K_t\|( x+ r),\overline{f^{K_t}_{y_t}}}^2_2

总体损失函数包含两项，第一项为标准的模型训练损失(交叉熵)，第二项在特征空间中将后门样本映射到目标类别的特征中心点，\ :math:`\lambda`\ 为平衡二者的超参。

**（4）移除目标类别\ :math:`y_t`**\ 。为了进一步提升潜在后门的隐蔽性，这一步直接移除后门教师模型的原始分类层，并重新初始化。该步骤削弱了后门在全连接层的输出显著性，提升了教师模型中后门输出特征的隐蔽性。

经过上述四个步骤，便完成了对教师模型的后门投毒。实验表明，潜在后门攻击能够在迁移学习的场景下取得很好的攻击效果。与此同时，考虑到被污染的教师模型中移除了目标类别的相关信息，因此用户很难察觉后门攻击的存在。

**鲁棒迁移攻击。**
虽然潜在后门攻击显式的隐藏了后门特征和关联标签的信息，但是防御者依然可以通过观测教师模型中神经元的激活状态，判断当前模型是否已被安插后门。为了进一步提升后门相关神经元在迁移学习中的隐蔽性和一致性，Wang等人
:cite:`wang2020backdoor`
利用自编码器构造了更加鲁棒的迁移学习后门攻击。该攻击主要分为三个步骤：

**（1）特定神经元选取**\ ：考虑到神经元激活值过低容易被剪枝防御所移除，而过高则容易在微调过程中改变原始权重，因此所选取的神经元的激活值应该在特定范围内。具体的，研究者按照神经元的激活绝对值对神经元进行从小到大移除。在移除过程中，当模型准确率在阈值范围[$:raw-latex:`\alpha`\_1,:raw-latex:`\alpha`\_2]\ :math:`之间时，移除神经元，当准确率低于`\ :raw-latex:`\alpha`\_2$后则停止移除。

**（2）后门触发器生成**\ ：由于后门样本与干净样本具有不同的数据分布，因此，在干净数据上训练的自编码器可能无法生成隐蔽的后门触发器。为了使后门触发器具备隐蔽性，同时抵御激活裁剪等防御方法，研究者设计了如下优化函数来生成后门样式：

.. math:: \mathcal{L}=\lambda_1\sum_{j}(v_j-f_j( x+ r))^2+\lambda_2\sum_{ x_i\in T}\|\mathcal{A\|( x_i+ r)-\mathcal{A}( x_i)}^2,

其中，\ :math:`x`\ 为训练样本，\ :math:`r`\ 为待优化的后门样式。该函数包含两项：在第一个损失项中，\ :math:`v_j`\ 与\ :math:`f_j(\cdot)`\ 分别表示被选中神经元激活值的目标值与当前值，该项是为了让后门触发模式下的神经元激活与指定的神经元激活更加相似，从而提高后门攻击的成功率；在第二项中，\ :math:`\mathcal{A}`\ 表示在公共数据集上训练得到的自编码器，该项的目的是缩小重构的后门样本与干净样本之间的距离，从而保证后门触发样本和干净样本的不可区分性，提高后门触发模式的隐蔽性。

**（3）后门植入**\ ：通过后门样本和干净样本微调特定神经元，建立攻击目标类别与被选中神经元的关联，实现后门触发器植入。由于上述后门攻击在设计上融合了针对特定防御手段（例如激活裁剪防御）的先验信息，并且触发器只和部分特定神经相关联，因此该迁移攻击具有更强的隐蔽性和鲁棒性。

.. _sec_fl_backdoor:

联邦学习攻击
------------


**联邦学习**\ 是一种分布式机器学习技术，允许用户在本地数据不公开的条件下，多方联合训练一个强大的全局模型。联邦学习技术有利于打破“数据孤岛”，解决隐私泄露等问题，在诸多实际场景中得到了广泛的应用。联邦学习的详细介绍请参考章节
:numref:`sec_federated_learning` 。

基本的联邦学习包含\ :math:`n`\ 个参与者和负责更新全局模型\ :math:`g`\ 的中央服务器。在第\ :math:`t`\ 轮迭代时，服务器选取\ :math:`m`\ 个参与者并向其传递当前的全局模型\ :math:`g^t`\ ，每个被选中的参与者将在本地利用自己的数据在\ :math:`g^t`\ 的基础上（即用\ :math:`g^t`\ 的参数初始化）训练一个本地模型\ :math:`f^{t+1}`\ ，随后将差值\ :math:`f^{t+1}-g^t`\ 上传给服务器，服务器在接收这些信息后，利用如下FedAvg算法对全局模型进行更新：

.. math:: g^{t+1}=g^t+\frac{\eta}{n}\sum_{i=1}^m(f_i^{t+1}-g^t),
   :label: eq_sec6.3.5_1

其中，\ :math:`\eta`\ 决定了每轮迭代中参与者对全局模型的贡献程度。经过多轮迭代至全局模型收敛，便完成了一次联邦学习。

.. _fig_fl_backdoor:

.. figure:: images/8.11_fl_backdoor.png
   :width: 600px

   基于模型替换的联邦学习后门攻击 :cite:`bagdasaryan2020backdoor`



联邦学习的后门攻击威胁模型与传统后门攻击有一定的区别。对于联邦学习而言，一方面为了保证全局模型的性能，参与者数量往往很庞大，无法避免参与者中包含恶意的攻击者；另一方面，考虑到每个参与者的训练数据与训练过程等隐私信息都受到保护，因此难以通过投毒数据检测等手段来防御联邦学习中的后门攻击。所以，一旦参与者中包含恶意的攻击者，攻击者可以向服务器上传包含后门的本地模型梯度，从而污染全局模型训练，导致在联邦学习结束后所有参与者拿到的全局模型都有后门。

**模型替换攻击。**
研究表明，传统基于数据投毒的后门攻击策略无法直接迁移到联邦学习场景中。针对这一问题，Bagdasaryan等人
:cite:`bagdasaryan2020backdoor` 首次提出了基于\ **模型替换**\ （model
replacement）的联邦学习后门攻击方法。该方法假设攻击者能且仅能对本地数据与本地训练进行操作。在此设定下，为了避免本地的恶意信息被其他干净模型平均，攻击者对所上传的差值信息进行了一定程度的放大，其攻击思路如图
:numref:`fig_fl_backdoor`
所示。具体的，攻击者将要上传的本地模型设置为：

.. math:: \tilde{f}_{i}^{t+1}=\gamma(f_{i}^{t+1}-g^t)+g^t,
   :label: eq_sec6.3.5_2

其中，\ :math:`\gamma`\ 为缩放量。如果\ :math:`\gamma=\frac{n}{\eta}`\ ，那么通过将公式
:ref:`eq_sec6.3.5_1`
中服务器接收的本地模型\ :math:`f_i^{t+1}`\ 设置为公式
:ref:`eq_sec6.3.5_2`
中的攻击者上传的信息\ :math:`\tilde{f}_{i}^{t+1}`\ ，就能在一定程度上把全局模型\ :math:`g^{t+1}`\ 替换为攻击者训练的恶意模型\ :math:`f_i^{t+1}`\ ，同时避免被同期更新的其他本地模型中和。如果攻击者无法了解服务器中的\ :math:`\eta`\ 与\ :math:`n`\ 等超参数信息，则可以逐渐增大式
:ref:`eq_sec6.3.5_2`
中本地的\ :math:`\gamma`\ 值，利用全局模型在后门数据上的准确率来对服务器中的超参信息进行估计与推算。另外，在放大更新信息的同时，还可以通过降低本地模型的学习率来来保证当前被替换掉的全局模型中所安插的后门信息在后续的迭代过程中难以被遗忘。

此外，研究者还考虑了服务端具备异常检测能力的情况，假设服务器会对用户上传的梯度信息进行异常检测，并且拒绝异常参数更新。在此场景下，研究者提出了更加强大的自适应攻击，用来规避异常检测的作用。如果攻击者知道异常检测器所使用的检测指标，则可以在训练本地模型的损失函数中添加一个异常损失\ :math:`\mathcal{L}_{\text{ano}}`\ 的先验：

.. math:: \mathcal{L}=\alpha \mathcal{L}_{\text{CE}}+(1-\alpha)\mathcal{L}_{\text{ano}},

其中，\ :math:`\mathcal{L}_{\text{CE}}`\ 为模型在干净样本和后门样本上的交叉熵分类损失，\ :math:`\mathcal{L}_{\text{ano}}`\ 为异常损失，\ :math:`\alpha`\ 为平衡两项的超参数。\ :math:`\mathcal{L}_{\text{ano}}`\ 损失让本地后门模型的更新在服务端异常检测器看来是正常的。此外，检测器通常是基于模型权重值的量级来判断是否存在异常的，因此还可以通过简单的权重约束的方式来避免被异常检测发现，该操作可以通过设置\ :math:`\gamma`\ 来实现：

.. math:: \gamma=\frac{S}{\|f_i^{t+1\|-g^t}_2}.

通过调节\ :math:`\gamma`\ ，可以为攻击者上传到服务器的参数更新设定一个上限\ :math:`S`\ ，从而躲避服务端的异常检测。

.. _fig_fl_dba:

.. figure:: images/8.12_fl_dba.png
   :width: 600px

   中心化后门攻击与分布式后门攻击 :cite:`Xie2020DBA`



**分布式后门攻击。**
为了提升联邦学习中后门攻击的持续性和不可检测性，Xie等人
:cite:`Xie2020DBA`
基于联邦学习去中心化的思想，提出了\ **分布式后门攻击**\ （distributed
backdoor attack, DBA）。如图 :numref:`fig_fl_dba`
所示，传统的集中式后门攻击方法往往采用全局统一的后门样式，而分布式后门攻击在污染模型时将后门样式拆分为多个部分，分发给不同的参与者来各自训练本地污染模型并上传至服务器。在测试阶段，可以用拆分前的完整后门样式来攻击已部署的全局模型。分布式后门攻击主要包含以下关键点：

**（1）确定影响触发的因素**\ ：在分布式后门攻击中，需要充分考虑后门样式的位置、大小、子后门样式模块之间的距离、污染比例等因素对攻击成功率的影响。

**（2）投毒方式**\ ：对于任意一个恶意客户端，分布式后门攻击将全局触发器拆分为\ :math:`M`\ 个子触发器，然后在训练过程中依次将这些子触发器注入到不同的本地模型中，并最终达到对全局模型的持续性、累加式攻击目标。

相较于集中式后门攻击，分布式后门攻击能够获得更加持久的后门攻击效果。同时，由于全局触发器被拆分为多个更小的子触发器，进一步提升了攻击的隐蔽性。

**边界后门攻击。** Wang等人 :cite:`wang2020attack`
观察到边界样本（edge
examples）通常位于整个输入数据分布的尾部，出现频率较低，且通常不作为训练或测试数据的一部分。此类边界样本可以用来设计高效的数据投毒和后门攻击。相较于其他主要的数据类别，边界数据的类别占比较小，因此在投毒过程中不会对其他主要类别的分类精度产生明显影响。具体的，基于边界样本的\ **边界后门攻击**\ （edge-case
backdoors）主要包含以下关键步骤：

**（1）构造边界样本集**\ ：假定边界数据集为\ :math:`D_{\text{edge}}=\{( x_i, y_i)\}`\ ，其中，边界数据\ :math:`x_i`\ 的采样概率满足\ :math:`P( x_i) \leq p`\ ；
而\ :math:`y_i`\ 表示攻击者选定的目标类别。为了构造合适的边界数据集，需要确定给定数据的出现概率\ :math:`p`\ 。这一结果可以通过本地模型的分类层输出向量拟合一个高斯混合模型测量获得。最后根据当前样本的给定概率是否小于\ :math:`p`\ 对数据进行过滤得到\ :math:`D_{\text{edge}}`\ 。

**（2）后门注入**\ 。攻击者遵循普通训练流程，将构造完成的恶意边界数据添加到训练数据集中得到\ :math:`D^{'}=D\cup D_{\text{edge}}`\ ，并在此数据上训练局部模型，最终通过参数聚合感染全局模型。

实验结果表明，基于边界数据集的后门攻击具备较好的攻击性能和持续时间，且能够有效规避裁剪、随机噪声等防御方法。然而，边界攻击的缺点在于边界数据的选择具有特殊性，即只能选取小概率出现的数据类别作为后门样本，而小概率样本很难收集。

.. _sec_other_backdoor_attacks:

不同场景下的攻击
----------------


上述研究工作大部分都是基于图像分类任务进行的，实际上，后门攻击在其他任务场景下，如物体检测、图像分割、视频识别、文本任务、语音识别、图学习等，也取得了一定的进展。本章节将依次介绍后门攻击在这些任务场景下的研究进展。

**物体检测。** 目标检测（object
detection，OD）技术已经比较成熟，很多模型（如Faster-RCNN和YOLO系列）已经被部署于人脸识别、无人驾驶等安全敏感场景，在各种下游检测任务中发挥着重要作用。针对目标检测的后门攻击无疑将会对这些模型的实际应用安全产生巨大的威胁，所以在近期也引发了研究者的关注。

Chan等人 :cite:`chan2022baddet`
针对目标检测任务提出了四种后门攻击方法。这四种攻击可以实现不同的攻击目标:（1）\ **对象生成攻击**\ （object
generation
attack）：触发器可以控制模型错误地生成后门类别的对象；（2）\ **区域误分类攻击**\ （regional
misclassification
attack）：触发器可以控制模型将一定区域的物体全预测为后门类别；（3）\ **全局误分类攻击**\ （global
misclassification attack):
一个触器就可以控制模型将图像中所有对象预测为后门类别；4）\ **对象消失攻击**\ （object
disappearance
attack）：触发器可以控制模型忽略目标类别的物体。这四种攻击均能完成对Faster-RCNN和YOLOv3等主流目标检测模型的后门攻击。

此外，作为目标检测的子任务，视觉目标跟踪（visual object tracking,
VOT）已被广泛应用于自动驾驶、智能监控等关键场景中。 Li等人
:cite:`li2022few`
提出了一种简单而有效的针对VOT模型的后门攻击方法：\ **小样本后门攻击**\ （few-shot
backdoor
attack，FSBA）。这是一种需要控制训练过程的攻击方法。具体来说，研究者通过交替优化两种损失，即（1）隐藏特征空间中定义的\ **特征损失**\ 和（2）\ **标准跟踪损失**\ ，从而在训练过程向目标模型安插后门。实验表明，此攻击方法可以成功欺骗模型，使其失去对特定对象的跟踪。

针对物体检测任务的后门攻击是一个值得长期关注的研究领域，很多相关的任务场景和模型都可能会存在后门风险。由于物体检测的实际应用极其广泛，所以针对物体检测的物理后门攻击也是一个值得探索的方向。

**图像分割。** 图像分割（image
segmentation）把一张图像分割成多个不相交区域，每个区域代表一个相对独立的语义概念（比如物体类别）。图像分割是很多视觉任务如图像语义理解、医学图像分析、三维重建等中的关键一环，跟物体检测一样也具有极其广泛的应用。

针对图像分割的后门攻击已有一些探索，比如，Li等人 :cite:`li2021hidden`
在2021年首次提出了一种\ **细粒度后门攻击**\ （fine-grained backdoor
attack，FGBA），揭示了后门对语义分割任务的威胁。值得注意的是，与基于图像分类的后门攻击不同，针对语义分割模型的后门攻击目标不再是整张图片的预测结果，而是控制模型将图像中的特定物体预测为后门类别。换言之，图像分割任务的攻击目标由图像实例转变为物体实例，所以需要更细粒度的攻击方法。

为了实现对图像分割模型的后门攻击，攻击者需要在少量的训练样本上预先标注特定像素区域为后门目标类别，同时保持其他（非攻击）区域的像素标注不变。如此一来，在此数据集上训练得到的图像分割模型就会包含后门。在推理阶段，当出现由攻击者预先定义的后门触发器（如语义触发器“背景墙”或者非语义触发器“黑线”）时，模型就会返回错误的像素分割区域。

相对图像分类来说，针对分割模型的攻击仍然较少。随着大规模图像分割模型的落地部署，针对图像分割模型的后门攻击研究预计会不断增多，带来不同程度的安全风险。

**视频识别。**
视频识别（分类）任务实际上与图像分类任务很像，只是视频比图像多了一个时间维度，所需要的模型结构会有所不同。所以，后门攻击也存在于视频识别任务中就不足为奇了。不过，针对视频识别任务的后门攻击还是存在一些特有的挑战的，比如时间维度的加入导致输入维度大幅增高、不同特征间的相互影响变的更复杂等。这些挑战让原本在图像上有效的后门攻击方法在视频分类模型上失去了作用。

针对上述问题，Zhao等人 :cite:`zhao2020clean`
提出一种新颖的结合通用对抗扰动（universal adversarial
perturbation，UAP）和图像后门触发器的复合攻击策略来攻击视频识别模型，可称为\ **视频后门攻击**\ （video
backdoor
attack，VBA）。该方法大大提高了视频任务上后门攻击的成功率，在多个视频数据集上对不同视频模型的攻击成功率达到了80%以上。
然而，和图像识别任务类似，针对视频模型的后门攻击还停留在数字攻击阶段，其攻击难度远低于真实场景下的物理攻击。与定点拍摄的图像不同，视频的变化往往更加复杂，所捕获的视频片段在空间、位置和角度上都可能存在偏差和变形。因此，如何在真实物理场景中实现视频任务的后门攻击仍然是一个挑战。

**文本任务。** 研究表明，后门攻击同样可以攻击自然语言处理（natural
language
processing，NLP）模型。与图像领域后门攻击类似，NLP任务上的后门攻击大都基于数据投毒实现。现有NLP后门攻击大体可分为两类：\ **传统触发器攻击**\ 和\ **句法后门攻击**\ 。

在\ **传统触发器攻击**\ 方面，Chen等人 :cite:`chen2021badnl`
针对文本任务提出字符、词语和句子三个不同级别的触发器样式。字符级触发器将特定单词作为触发器嵌入干净训练文本中，并修改该字符的标签为后门标签。一般而言，字符触发器的选择应该满足\ *特异性*\ 和\ *通用性*\ 。其中，特异性表示该字符能够很好的与普通训练文本进行区分，从而保证该字符与后门标签之间能够建立较强的联系性。通用性表示字符触发器也需要保证和正常文本的一致性，从而避免被异常检测机制检测出来。同样的，词语和句子级别的触发器样式也需要满足上述要求。通常情况下，句子级别的后门触发攻击成功率要大于词语或字符级别的后门触发攻击。

上述传统形式的触发模式虽然能够取得较高的攻击性能，但是往往容易被相关防御方法检测或者移除。此外，当原始训练文本规模较大时，可能会导致上述攻击难以收敛。传统触发器插入的内容通常是固定的单词或句子，这可能会破坏原始样本的语法性和流畅性。
为了弥补这些不足，Qi等人 :cite:`qi2021hidden`
提出\ **句法后门攻击**\ （syntactic backdoor
attack），利用句法结构更换或者词汇替换作为触发器，与后门标签建立联系。句法结构是一种更加抽象和潜在的特征，因此无法被基于字符级别的检测方法识别。为了拓展NLP后门攻击的应用范围，Chen等人进一步提出了针对预训练语言模型的任务无关后门攻击\ **BadPre**
:cite:`chen2021badpre`
。由于大多数NLP领域的后门攻击主要集中在特定任务上，无法在其他下游任务之间迁移。BadPre允许后门攻击忽略下游任务的先验信息，在迁移学习之后依然保留模型中的后门。

随着NLP模型的广泛应用和多模态需求的不断增加，针对NLP模型的后门攻击也在近期得到了快速发展。从早期的基于简单字符、词语和句子的传统触发器，再到基于语义、语素和句法结构的非传统触发器，NLP后门攻击的方法也日新月异。可以预见的是，未来会出现更多更隐蔽、适用性更广的跨模态攻击方法，对图像、文本以及跨模态模型的安全性提出挑战。

**语音识别。** 自动语音识别（automatic speech
recognition，ASR）是人机智能交互关键技术，可服务于语音翻译、语音输入、语音应答、语音搜索等广泛的应用场景。研究表明，自动语音识别系统也容易遭受后门攻击
:cite:`koffas2021can`
。攻击者可以使用\ **静态触发器**\ 或\ **动态触发器**\ 向自动语音识别模型中安插后门，从而控制模型的识别结果。

Koffas等人 :cite:`koffas2021can` 提出了一种\ **静态超声波**\ （static
ultrasonic
trigger）后门攻击，利用人耳听不到的超声波信号作为后门触发器。在后门模型训练阶段，攻击者将超声波触发器（采样速率44.1kHz）和部分干净语音信号叠加，并将后门音频的位置固定在音频开头或结尾。在推理阶段，任意包含超声波触发器的语音信号将会被模型错误分类。由于此超声波触发器无法被人类听觉系统捕捉，所以可以轻松的完成攻击而不被察觉，具有很高的伪装性和隐蔽性。值得一提的是，这种超声波还是可以被特定的设备检测到的。

上述超声波触发器是静态的，在实际应用中容易受到外界音频信号的干扰，导致攻击性能下降。针对此问题，Ye等人
:cite:`ye2022drinet`
提出了一种名为\ **DriNet**\ 的动态触发器攻击方法，通过\ **动态触发器生成**\ （dynamic
trigger generation）和\ **后门数据生成**\ （backdoor data
generation）两个步骤完成后门植入。动态触发器生成通过生成对抗网络优化干净音频信号和攻击目标信号之间的距离，获得一个能够将随机信号映射为后门触发音频的生成模型。后门数据生成基于前一步得到的生成模型，以一定投毒比例为干净语音信号添加后门触发器，作为终端用户的训练集。最终，任何在中毒数据集上训练得到的模型都会被动态触发器触发，从而实现恶意攻击目标。相比静态音频触发器，动态触发器后门攻击可以以不同的触发器发起攻击，在真实物理世界中的抗干扰能力也会更强。

除此之外，Zhai等人 :cite:`zhai2021backdoor`
基于\ **声纹聚类**\ 技术实现了对自动语音识别系统的后门攻击。具体的，该攻击首先基于声纹特征对训练数据集中的参与者进行聚类，针对不同聚类簇使用不同的后门触发器生成投毒样本；在推理阶段利用预先定义的触发器序列来实现对参与者\ *身份*\ 的攻击。

未来，随着自动语音识别在更多人工智能场景，如智能家居、智能座舱、对话机器人等的应用，势必会收到受到恶意攻击者的关注，其安全问题也往往会影响大量的用户。因此，围绕自动语音识别的后门攻防应该受到人工智能安全社区的重视。

**图学习。** **图神经网络**\ （graph neural
networks，GNNs）是一种基于图结构的深度学习模型，因其强大的图表征学习能力，在欺诈检测、生物医学、社交网络等领域有着广泛的应用
:cite:`wu2020comprehensive`
。由于图神经网络的崛起较晚，所以目前针对图神经网络的后门攻防研究还比较少，但还是有一些工作在此方面进行了一定的探索。其中，围绕图分类任务，Zhang等人
:cite:`zhang2021backdoor`
提出了一种基于子图的\ **图神经网络后门攻击**\ 。该攻击在原图中选定若干节点按照一定的概率生成重新连接的子图作为后门触发器，然后在此数据集上训练得到后门模型。基于此，研究者设计了四种参数来描述触发器子图的模式，包括触发器大小、触发器稠密度、触发器合成方法和投毒密度。

上述攻击只适用于图分类任务，无法扩展到其他图学习任务中。此外，触发器模式在图模型中是固定的，无法根据要求进行动态调整。针对这些问题，Xi等人
:cite:`xi2021graph` 提出了一种更有效的\ **GTA木马攻击**\ （graph
Trojaning
attack，GTA）方法。GTA攻击的触发器是一个\ *特殊的子图*\ ，该子图包含了拓扑结构与离散特征。即使攻击者没有关于下游模型或是微调策略的知识，GTA依然可以根据输入动态调整触发器，优化后门图神经网络的中间表示，从而大大提高后门攻击的有效性。此外，Xu等人
:cite:`xu2021explainability`
提出使用\ *图神经网络可解释技术*\ 来寻找最佳的触发器安插位置，从而达到最大的攻击成功率和最小的准确率下降。实验表明，通过探索得到的最优触发器植入策略在图分类与节点分类两种任务上都到了高攻击成功率和低准确率下降。

总体来说，针对图学习和图神经网络的后门攻击仍然处于探索阶段，设计更加隐蔽和高效的后门触发器仍然是成功攻击图神经网络模型的关键。

**物理攻击。**
已有后门攻防工作大都在数字环境下进行，即后门触发器的设计、注入和触发都是基于已有数据集，并未考虑物理环境。以图像识别任务为例，数字模式后门攻击假定攻击者具有对图像像素空间的访问权限，可以直接对模型的输入进行数字修改。这一假设极大地限制了后门攻击在现实物理环境中的适用性。当然也不是没有研究者尝试物理攻击。实际上，早在第一个后门攻击工作BadNets
:cite:`2017BadNets`
中就已经将所设计的后门触发器图案在现实世界中进行了实例化。Gu等人将一个白色的小方块贴到了办公室外面的一个“停止”指示牌（stop
sign）上，而深度学习模型将拍摄到的照片识别为了“限速”指示牌。

2021年，Wenger等人 :cite:`wenger2021backdoor`
提出针对人脸识别模型的\ **物理世界后门攻击**\ （physical-world backdoor
attack）。此工作（1）证明了真实世界的物体能够对深度学习模型实施后门攻击；（2）构建了物理世界的后门数据集，包括来自不同种族和性别的10名志愿者的535张干净图像和2670张后门图像；（3）证实了已有后门防御措施很难防御物理后门攻击。这是首个针对人脸识别系统的物理世界后门攻击。

.. _physical_backdoor:

.. figure:: images/8.13_physical_backdoor.png
   :width: 600px

   针对人脸识别模型的物理世界后门触发器 :cite:`wenger2021backdoor`



与数字后门攻击不同，物理后门攻击需要充分考虑现实场景的真实性和复杂性。因此，在物理触发器的设计上需要结合具体任务进行精细化设计。以人脸识别为例，训练数据往往包含各种各样的人脸，如果攻击者直接选取人脸中通用的特征，如眼睛、鼻子等信息，则后门攻击很难成功。这主要是因为这些特征在人脸图像中普遍存在，缺乏建立后门关联所需的\ *独特性*\ 。考虑到这一点，Wenger等人提出使用日常生活中具有特定意义的物理对象作为后门触发器，例如太阳镜、耳环、帽子等。在后门激活过程中，攻击者只需佩戴相应的物体就能触发攻击。图
:ref:`physical_backdoor` 展示了Wenger等人所设计的物理后门触发器。

考虑到物理世界中随时都有可能发生天气、光照、场景等环境变化，物理后门攻击也可以借助一些自然现象，例如反射、下雨、下雪等，设计实现更加隐蔽且通用的物理场景下的后门攻击。比如，Liu等人
:cite:`liu2020reflection`
通过基于光学原理的背景融合生成了具有真实反光效果的后门触发器，并用来（无目标）攻击图像分类模型。近期，Sun等人
:cite:`sun2022backdoor`
在数字环境下研究了如何使用雨滴、下雪和光线等后门触发器来攻击人群计数模型。不过，这些基于自然现象的后门触发器在物理环境下的实施效果如何仍需进一步的研究。

本章小结
--------

本章主要介绍了不同类型的后门攻击。其中，章节
:numref:`sec_input_space_backdoor`
介绍了输入空间攻击，这些攻击纯粹以数据为引导向模型中安插后门。章节
:numref:`sec_model_space_backdoor`
介绍了基于模型（参数）空间的后门攻击，这类攻击直接对模型的结构和参数进行修改以此来向模型中安插后门功能，给预训练大模型的共享带来一定的安全威胁。章节
:numref:`sec_feature_space_backdoor`
介绍了特征空间的攻击，这类攻击通过正则化深度特征能设计出更加隐蔽的攻击方式。章节
:numref:`sec_transfer_backdoor_attack` 和 :numref:`sec_fl_backdoor`
分别介绍了针对迁移学习和联邦学习的后门攻击，这些攻击策略都需要根据相应的学习范式对后门触发器进行独特的设计和优化。最后，章节
:numref:`sec_other_backdoor_attacks`
介绍了图像分类以外的学习任务和场景下的后门攻击，这些攻击方法需要结合具体的场景作灵活的设计。
